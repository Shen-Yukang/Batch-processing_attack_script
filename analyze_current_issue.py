#!/usr/bin/env python3
"""
åˆ†æå½“å‰æ‰¹å¤„ç†é—®é¢˜çš„ä¸“ç”¨è„šæœ¬
é’ˆå¯¹"è¿”å›ç 0ä½†æ²¡æœ‰ç»“æœæ–‡ä»¶"çš„é—®é¢˜è¿›è¡Œæ·±åº¦åˆ†æ
"""

import os
import glob
import json
import re
import argparse
import logging
from datetime import datetime
from typing import List, Dict, Optional

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def analyze_recent_batch_runs(output_base_dir: str = "output") -> Dict:
    """åˆ†ææœ€è¿‘çš„æ‰¹å¤„ç†è¿è¡Œæƒ…å†µ"""
    logger.info(f"ğŸ” åˆ†ææœ€è¿‘çš„æ‰¹å¤„ç†è¿è¡Œ: {output_base_dir}")
    
    analysis = {
        'timestamp': datetime.now().isoformat(),
        'output_directories': [],
        'problematic_batches': [],
        'successful_batches': [],
        'recommendations': []
    }
    
    # æŸ¥æ‰¾æ‰€æœ‰æ‰¹å¤„ç†è¾“å‡ºç›®å½•
    if os.path.exists(output_base_dir):
        pattern = os.path.join(output_base_dir, "batch_results_*")
        output_dirs = glob.glob(pattern)
        analysis['output_directories'] = output_dirs
        
        logger.info(f"ğŸ“ æ‰¾åˆ° {len(output_dirs)} ä¸ªè¾“å‡ºç›®å½•")
        
        for output_dir in output_dirs:
            dir_analysis = analyze_single_output_directory(output_dir)
            
            if dir_analysis['has_issues']:
                analysis['problematic_batches'].append(dir_analysis)
            else:
                analysis['successful_batches'].append(dir_analysis)
    
    # ç”Ÿæˆæ€»ä½“å»ºè®®
    analysis['recommendations'] = generate_overall_recommendations(analysis)
    
    return analysis

def analyze_single_output_directory(output_dir: str) -> Dict:
    """åˆ†æå•ä¸ªè¾“å‡ºç›®å½•"""
    logger.info(f"ğŸ“‚ åˆ†æç›®å½•: {os.path.basename(output_dir)}")
    
    dir_analysis = {
        'directory': output_dir,
        'directory_name': os.path.basename(output_dir),
        'has_issues': False,
        'issues': [],
        'batch_files': [],
        'result_files': [],
        'status_info': None,
        'log_files': [],
        'batch_ids_found': []
    }
    
    # æŸ¥æ‰¾æ‰¹å¤„ç†è¾“å…¥æ–‡ä»¶
    batch_pattern = os.path.join(output_dir, "batch_*.jsonl")
    batch_files = glob.glob(batch_pattern)
    dir_analysis['batch_files'] = batch_files
    
    # æŸ¥æ‰¾ç»“æœæ–‡ä»¶
    result_pattern = os.path.join(output_dir, "batch_results_*.jsonl")
    result_files = glob.glob(result_pattern)
    dir_analysis['result_files'] = result_files
    
    # æŸ¥æ‰¾æ—¥å¿—æ–‡ä»¶
    log_pattern = os.path.join(output_dir, "logs", "*.log")
    log_files = glob.glob(log_pattern)
    dir_analysis['log_files'] = log_files
    
    # è¯»å–çŠ¶æ€æ–‡ä»¶
    status_file = os.path.join(output_dir, "batch_status.json")
    if os.path.exists(status_file):
        try:
            with open(status_file, 'r', encoding='utf-8') as f:
                status_data = json.load(f)
                dir_analysis['status_info'] = status_data
        except Exception as e:
            logger.warning(f"âš ï¸  è¯»å–çŠ¶æ€æ–‡ä»¶å¤±è´¥: {e}")
    
    # åˆ†æé—®é¢˜
    issues = []
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ‰¹å¤„ç†æ–‡ä»¶ä½†æ²¡æœ‰ç»“æœæ–‡ä»¶
    if batch_files and not result_files:
        issues.append("æœ‰æ‰¹å¤„ç†è¾“å…¥æ–‡ä»¶ä½†æ²¡æœ‰ç»“æœæ–‡ä»¶")
        dir_analysis['has_issues'] = True
    
    # æ£€æŸ¥æ‰¹å¤„ç†æ•°é‡ä¸ç»“æœæ•°é‡çš„åŒ¹é…
    if len(batch_files) > len(result_files):
        missing_count = len(batch_files) - len(result_files)
        issues.append(f"ç¼ºå°‘ {missing_count} ä¸ªç»“æœæ–‡ä»¶")
        dir_analysis['has_issues'] = True
    
    # ä»æ—¥å¿—æ–‡ä»¶ä¸­æå–batch ID
    batch_ids = extract_batch_ids_from_logs(log_files)
    dir_analysis['batch_ids_found'] = batch_ids
    
    # æ£€æŸ¥æ˜¯å¦æœ‰batch IDä½†æ²¡æœ‰å¯¹åº”çš„ç»“æœæ–‡ä»¶
    if batch_ids and not result_files:
        issues.append(f"æ‰¾åˆ° {len(batch_ids)} ä¸ªbatch IDä½†æ²¡æœ‰ç»“æœæ–‡ä»¶")
        dir_analysis['has_issues'] = True
    
    dir_analysis['issues'] = issues
    
    if issues:
        logger.warning(f"âš ï¸  å‘ç°é—®é¢˜: {', '.join(issues)}")
    else:
        logger.info(f"âœ… ç›®å½•æ­£å¸¸")
    
    return dir_analysis

def extract_batch_ids_from_logs(log_files: List[str]) -> List[str]:
    """ä»æ—¥å¿—æ–‡ä»¶ä¸­æå–batch ID"""
    batch_ids = set()
    
    for log_file in log_files:
        try:
            with open(log_file, 'r', encoding='utf-8') as f:
                content = f.read()
                
                # æŸ¥æ‰¾batch IDæ¨¡å¼
                pattern = r'batch_[a-f0-9]{32}'
                found_ids = re.findall(pattern, content)
                batch_ids.update(found_ids)
                
        except Exception as e:
            logger.warning(f"âš ï¸  è¯»å–æ—¥å¿—æ–‡ä»¶å¤±è´¥ {log_file}: {e}")
    
    return list(batch_ids)

def analyze_log_for_errors(log_file: str) -> Dict:
    """åˆ†ææ—¥å¿—æ–‡ä»¶ä¸­çš„é”™è¯¯ä¿¡æ¯"""
    logger.info(f"ğŸ“„ åˆ†ææ—¥å¿—æ–‡ä»¶: {os.path.basename(log_file)}")
    
    analysis = {
        'file': log_file,
        'error_patterns': {},
        'success_indicators': 0,
        'failure_indicators': 0,
        'batch_ids': [],
        'timeline': []
    }
    
    try:
        with open(log_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        error_patterns = {
            'quota': r'quota|é…é¢',
            'rate_limit': r'rate.?limit|é€Ÿç‡é™åˆ¶',
            'api_key': r'api.?key|å¯†é’¥',
            'timeout': r'timeout|è¶…æ—¶',
            'validation': r'validation|éªŒè¯',
            'network': r'network|ç½‘ç»œ|connection',
            'permission': r'permission|æƒé™'
        }
        
        for line in lines:
            # æ£€æŸ¥é”™è¯¯æ¨¡å¼
            for pattern_name, pattern in error_patterns.items():
                if re.search(pattern, line, re.IGNORECASE):
                    if pattern_name not in analysis['error_patterns']:
                        analysis['error_patterns'][pattern_name] = []
                    analysis['error_patterns'][pattern_name].append(line.strip())
            
            # æ£€æŸ¥æˆåŠŸ/å¤±è´¥æŒ‡ç¤ºå™¨
            if 'æˆåŠŸ' in line or 'success' in line.lower():
                analysis['success_indicators'] += 1
            if 'å¤±è´¥' in line or 'failed' in line.lower() or 'error' in line.lower():
                analysis['failure_indicators'] += 1
            
            # æå–batch ID
            batch_ids = re.findall(r'batch_[a-f0-9]{32}', line)
            analysis['batch_ids'].extend(batch_ids)
        
        # å»é‡batch ID
        analysis['batch_ids'] = list(set(analysis['batch_ids']))
        
    except Exception as e:
        logger.error(f"âŒ åˆ†ææ—¥å¿—æ–‡ä»¶å¤±è´¥: {e}")
    
    return analysis

def generate_overall_recommendations(analysis: Dict) -> List[str]:
    """ç”Ÿæˆæ€»ä½“ä¿®å¤å»ºè®®"""
    recommendations = []
    
    problematic_count = len(analysis['problematic_batches'])
    successful_count = len(analysis['successful_batches'])
    
    if problematic_count > 0:
        recommendations.append(f"ğŸš¨ å‘ç° {problematic_count} ä¸ªæœ‰é—®é¢˜çš„æ‰¹å¤„ç†ç›®å½•")
        
        # æ”¶é›†æ‰€æœ‰batch ID
        all_batch_ids = []
        for batch_info in analysis['problematic_batches']:
            all_batch_ids.extend(batch_info['batch_ids_found'])
        
        if all_batch_ids:
            recommendations.append(f"ğŸ” å»ºè®®ä½¿ç”¨è°ƒè¯•å·¥å…·åˆ†æä»¥ä¸‹batch ID:")
            for batch_id in all_batch_ids[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                recommendations.append(f"   python enhanced_batch_debugger.py {batch_id}")
            
            if len(all_batch_ids) > 5:
                recommendations.append(f"   ... è¿˜æœ‰ {len(all_batch_ids) - 5} ä¸ªbatch ID")
        
        recommendations.append(f"ğŸ”§ å»ºè®®ä½¿ç”¨ä¿®å¤å·¥å…·:")
        for batch_info in analysis['problematic_batches']:
            recommendations.append(f"   python fix_missing_batch_results.py {batch_info['directory']}")
    
    if successful_count > 0:
        recommendations.append(f"âœ… {successful_count} ä¸ªæ‰¹å¤„ç†ç›®å½•æ­£å¸¸")
    
    return recommendations

def main():
    parser = argparse.ArgumentParser(description='åˆ†æå½“å‰æ‰¹å¤„ç†é—®é¢˜')
    parser.add_argument('--output-dir', default='output', help='è¾“å‡ºåŸºç¡€ç›®å½•ï¼Œé»˜è®¤ä¸ºoutput')
    parser.add_argument('--specific-dir', help='åˆ†æç‰¹å®šçš„è¾“å‡ºç›®å½•')
    parser.add_argument('--save-report', action='store_true', help='ä¿å­˜åˆ†ææŠ¥å‘Šåˆ°æ–‡ä»¶')
    
    args = parser.parse_args()
    
    if args.specific_dir:
        # åˆ†æç‰¹å®šç›®å½•
        if not os.path.exists(args.specific_dir):
            logger.error(f"âŒ ç›®å½•ä¸å­˜åœ¨: {args.specific_dir}")
            return 1
        
        dir_analysis = analyze_single_output_directory(args.specific_dir)
        
        print("\n" + "="*80)
        print(f"ğŸ“‹ ç›®å½•åˆ†ææŠ¥å‘Š: {dir_analysis['directory_name']}")
        print("="*80)
        
        print(f"ğŸ“ ç›®å½•: {dir_analysis['directory']}")
        print(f"ğŸ“„ æ‰¹å¤„ç†æ–‡ä»¶: {len(dir_analysis['batch_files'])}")
        print(f"ğŸ“„ ç»“æœæ–‡ä»¶: {len(dir_analysis['result_files'])}")
        print(f"ğŸ“„ æ—¥å¿—æ–‡ä»¶: {len(dir_analysis['log_files'])}")
        print(f"ğŸ†” æ‰¾åˆ°çš„batch ID: {len(dir_analysis['batch_ids_found'])}")
        
        if dir_analysis['has_issues']:
            print(f"\nâš ï¸  å‘ç°é—®é¢˜:")
            for issue in dir_analysis['issues']:
                print(f"   - {issue}")
        else:
            print(f"\nâœ… ç›®å½•çŠ¶æ€æ­£å¸¸")
        
        if dir_analysis['batch_ids_found']:
            print(f"\nğŸ†” Batch IDåˆ—è¡¨:")
            for batch_id in dir_analysis['batch_ids_found']:
                print(f"   - {batch_id}")
    
    else:
        # åˆ†ææ‰€æœ‰ç›®å½•
        analysis = analyze_recent_batch_runs(args.output_dir)
        
        print("\n" + "="*80)
        print("ğŸ“‹ æ‰¹å¤„ç†é—®é¢˜åˆ†ææŠ¥å‘Š")
        print("="*80)
        
        print(f"ğŸ“ åˆ†æç›®å½•: {args.output_dir}")
        print(f"ğŸ“‚ æ‰¾åˆ°è¾“å‡ºç›®å½•: {len(analysis['output_directories'])}")
        print(f"âš ï¸  æœ‰é—®é¢˜çš„æ‰¹å¤„ç†: {len(analysis['problematic_batches'])}")
        print(f"âœ… æ­£å¸¸çš„æ‰¹å¤„ç†: {len(analysis['successful_batches'])}")
        
        if analysis['problematic_batches']:
            print(f"\nğŸš¨ æœ‰é—®é¢˜çš„æ‰¹å¤„ç†ç›®å½•:")
            for batch_info in analysis['problematic_batches']:
                print(f"   ğŸ“‚ {batch_info['directory_name']}")
                for issue in batch_info['issues']:
                    print(f"      - {issue}")
                if batch_info['batch_ids_found']:
                    print(f"      ğŸ†” Batch IDs: {', '.join(batch_info['batch_ids_found'][:3])}...")
        
        if analysis['recommendations']:
            print(f"\nğŸ’¡ ä¿®å¤å»ºè®®:")
            for rec in analysis['recommendations']:
                print(rec)
        
        # ä¿å­˜æŠ¥å‘Š
        if args.save_report:
            report_file = f"batch_analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(analysis, f, indent=2, ensure_ascii=False, default=str)
            print(f"\nğŸ“„ åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    
    print("\n" + "="*80)
    return 0

if __name__ == "__main__":
    import sys
    sys.exit(main())
