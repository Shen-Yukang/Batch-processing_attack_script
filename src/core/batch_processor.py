#!/usr/bin/env python3
"""
OpenAI Batch APIå¤„ç†å™¨
å¤„ç†æ‰¹é‡è¯·æ±‚çš„å®Œæ•´å·¥ä½œæµç¨‹ï¼šä¸Šä¼ æ–‡ä»¶ã€åˆ›å»ºæ‰¹å¤„ç†ã€ç›‘æ§çŠ¶æ€ã€ä¸‹è½½ç»“æœ
"""

import os
import time
import json
import argparse
import logging
from typing import Optional
import openai

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class BatchProcessor:
    def __init__(self, api_key: str):
        """
        åˆå§‹åŒ–æ‰¹å¤„ç†å™¨

        Args:
            api_key: OpenAI APIå¯†é’¥
        """
        self.client = openai.OpenAI(api_key=api_key)

    def upload_batch_file(self, file_path: str) -> Optional[str]:
        """
        ä¸Šä¼ æ‰¹å¤„ç†è¾“å…¥æ–‡ä»¶

        Args:
            file_path: JSONLæ–‡ä»¶è·¯å¾„

        Returns:
            æ–‡ä»¶IDï¼Œå¦‚æœå¤±è´¥è¿”å›None
        """
        try:
            with open(file_path, "rb") as file:
                response = self.client.files.create(
                    file=file,
                    purpose="batch"
                )

            file_id = response.id
            logger.info(f"æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼Œæ–‡ä»¶ID: {file_id}")
            return file_id

        except Exception as e:
            logger.error(f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {e}")
            return None

    def create_batch(self, input_file_id: str, completion_window: str = "24h") -> Optional[str]:
        """
        åˆ›å»ºæ‰¹å¤„ç†ä»»åŠ¡

        Args:
            input_file_id: è¾“å…¥æ–‡ä»¶ID
            completion_window: å®Œæˆæ—¶é—´çª—å£ï¼ˆ24hï¼‰

        Returns:
            æ‰¹å¤„ç†IDï¼Œå¦‚æœå¤±è´¥è¿”å›None
        """
        try:
            response = self.client.batches.create(
                input_file_id=input_file_id,
                endpoint="/v1/chat/completions",
                completion_window=completion_window
            )

            batch_id = response.id
            logger.info(f"æ‰¹å¤„ç†åˆ›å»ºæˆåŠŸï¼Œæ‰¹å¤„ç†ID: {batch_id}")
            logger.info(f"çŠ¶æ€: {response.status}")
            return batch_id

        except Exception as e:
            logger.error(f"æ‰¹å¤„ç†åˆ›å»ºå¤±è´¥: {e}")
            return None

    def get_batch_status(self, batch_id: str) -> Optional[dict]:
        """
        è·å–æ‰¹å¤„ç†çŠ¶æ€

        Args:
            batch_id: æ‰¹å¤„ç†ID

        Returns:
            æ‰¹å¤„ç†çŠ¶æ€ä¿¡æ¯ï¼Œå¦‚æœå¤±è´¥è¿”å›None
        """
        try:
            response = self.client.batches.retrieve(batch_id)

            # è¯¦ç»†è®°å½•çŠ¶æ€ä¿¡æ¯
            status_info = {
                'id': response.id,
                'status': response.status,
                'created_at': response.created_at,
                'completed_at': response.completed_at,
                'failed_at': response.failed_at,
                'expires_at': response.expires_at,
                'request_counts': response.request_counts,
                'output_file_id': response.output_file_id,
                'error_file_id': response.error_file_id
            }

            # è®°å½•è¯¦ç»†çš„çŠ¶æ€ä¿¡æ¯
            logger.debug(f"æ‰¹å¤„ç†çŠ¶æ€è¯¦æƒ… {batch_id}:")
            logger.debug(f"  çŠ¶æ€: {status_info['status']}")
            logger.debug(f"  åˆ›å»ºæ—¶é—´: {status_info['created_at']}")
            logger.debug(f"  å®Œæˆæ—¶é—´: {status_info['completed_at']}")
            logger.debug(f"  å¤±è´¥æ—¶é—´: {status_info['failed_at']}")
            logger.debug(f"  è¿‡æœŸæ—¶é—´: {status_info['expires_at']}")
            logger.debug(f"  è¾“å‡ºæ–‡ä»¶ID: {status_info['output_file_id']}")
            logger.debug(f"  é”™è¯¯æ–‡ä»¶ID: {status_info['error_file_id']}")

            if status_info['request_counts']:
                counts = status_info['request_counts']
                logger.debug(f"  è¯·æ±‚ç»Ÿè®¡: æ€»æ•°={getattr(counts, 'total', 0)}, "
                           f"å®Œæˆ={getattr(counts, 'completed', 0)}, "
                           f"å¤±è´¥={getattr(counts, 'failed', 0)}")

            return status_info

        except Exception as e:
            logger.error(f"è·å–æ‰¹å¤„ç†çŠ¶æ€å¤±è´¥: {e}")
            logger.error(f"é”™è¯¯è¯¦æƒ…: {str(e)}")
            return None

        def __init__(self, input_csv: str, batch_size: int = 50, model: str = "gpt-4o-mini", batch_interval: int = 120, output_base_dir: str = "output"):
                error_response = self.client.files.content(error_file_id)
    def wait_for_completion(self, batch_id: str, check_interval: int = 60) -> bool:
        """
        ç­‰å¾…æ‰¹å¤„ç†å®Œæˆ

        Args:
            batch_id: æ‰¹å¤„ç†ID
            check_interval: æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰

        Returns:
            æ˜¯å¦æˆåŠŸå®Œæˆ
        """
        logger.info(f"ç­‰å¾…æ‰¹å¤„ç†å®Œæˆ: {batch_id}")
        start_time = time.time()

        while True:
            status_info = self.get_batch_status(batch_id)
            if not status_info:
                logger.error("æ— æ³•è·å–æ‰¹å¤„ç†çŠ¶æ€")
                return False

            status = status_info['status']
            elapsed_time = time.time() - start_time
            logger.info(f"å½“å‰çŠ¶æ€: {status} (å·²ç­‰å¾… {elapsed_time:.1f} ç§’)")

            # å®˜æ–¹æ‰€æœ‰çŠ¶æ€å¤„ç†
            if status == 'validating':
                logger.info("ğŸ” æ‰¹å¤„ç†æ­£åœ¨éªŒè¯è¾“å…¥æ–‡ä»¶...")
            elif status == 'failed':
                logger.error("âŒ æ‰¹å¤„ç†å¤±è´¥ï¼ˆè¾“å…¥æ–‡ä»¶æœªé€šè¿‡éªŒè¯æˆ–è¿è¡Œå¤±è´¥ï¼‰")
                self._robust_log_failure_details(status_info)
                return False
            elif status == 'in_progress':
                logger.info("ğŸš€ æ‰¹å¤„ç†æ­£åœ¨è¿è¡Œ...")
            elif status == 'finalizing':
                logger.info("ğŸ“¦ æ‰¹å¤„ç†å·²å®Œæˆï¼Œæ­£åœ¨å‡†å¤‡ç»“æœ...")
            elif status == 'completed':
                logger.info("âœ… æ‰¹å¤„ç†å·²å®Œæˆï¼Œç»“æœå·²å‡†å¤‡å¥½")
                if status_info.get('request_counts'):
                    counts = status_info['request_counts']
                    total = getattr(counts, 'total', 0)
                    completed = getattr(counts, 'completed', 0)
                    failed = getattr(counts, 'failed', 0)
                    logger.info(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡: {completed}/{total} å®Œæˆ, {failed} å¤±è´¥")
                return True
            elif status == 'expired':
                logger.error("â° æ‰¹å¤„ç†å·²è¿‡æœŸï¼ˆ24å°æ—¶æœªå®Œæˆï¼‰")
                logger.error(f"è¿‡æœŸæ—¶é—´: {status_info.get('expires_at')}")
                return False
            elif status == 'cancelling':
                logger.warning("âš ï¸ æ‰¹å¤„ç†æ­£åœ¨å–æ¶ˆï¼ˆå¯èƒ½éœ€è¦10åˆ†é’Ÿï¼‰")
            elif status == 'cancelled':
                logger.error("ğŸš« æ‰¹å¤„ç†å·²è¢«å–æ¶ˆ")
                return False
            else:
                logger.warning(f"âš ï¸ æœªçŸ¥æ‰¹å¤„ç†çŠ¶æ€: {status}")

            # æ˜¾ç¤ºè¿›åº¦ä¿¡æ¯
            if status_info.get('request_counts'):
                counts = status_info['request_counts']
                total = getattr(counts, 'total', 0)
                completed = getattr(counts, 'completed', 0)
                failed = getattr(counts, 'failed', 0)
                logger.info(f"ğŸ“ˆ è¿›åº¦: {completed}/{total} å®Œæˆ, {failed} å¤±è´¥")

            logger.info(f"â³ ç­‰å¾… {check_interval} ç§’åå†æ¬¡æ£€æŸ¥...")
            time.sleep(check_interval)

    def _log_failure_details(self, status_info: dict):
        """è®°å½•å¤±è´¥çš„è¯¦ç»†ä¿¡æ¯"""
        logger.error("ğŸ’¥ æ‰¹å¤„ç†å¤±è´¥è¯¦æƒ…:")
        logger.error(f"   å¤±è´¥æ—¶é—´: {status_info.get('failed_at')}")

        # å¦‚æœæœ‰é”™è¯¯æ–‡ä»¶ï¼Œå°è¯•ä¸‹è½½å¹¶è®°å½•å†…å®¹
        error_file_id = status_info.get('error_file_id')
        if error_file_id:
            logger.error(f"   é”™è¯¯æ–‡ä»¶ID: {error_file_id}")
            try:
                error_response = self.client.files.content(error_file_id)
                error_content = error_response.content.decode('utf-8')
                logger.error(f"   é”™è¯¯å†…å®¹é¢„è§ˆ: {error_content[:500]}...")
            except Exception as e:
                logger.error(f"   æ— æ³•è¯»å–é”™è¯¯æ–‡ä»¶: {e}")
        else:
            logger.error("   æ²¡æœ‰é”™è¯¯æ–‡ä»¶ID")

        # è®°å½•è¯·æ±‚ç»Ÿè®¡
        if status_info.get('request_counts'):
            counts = status_info['request_counts']
            total = getattr(counts, 'total', 0)
            completed = getattr(counts, 'completed', 0)
            failed = getattr(counts, 'failed', 0)
            logger.error(f"   è¯·æ±‚ç»Ÿè®¡: æ€»æ•°={total}, å®Œæˆ={completed}, å¤±è´¥={failed}")

    def download_results(self, batch_id: str, output_dir: str = ".") -> Optional[str]:
        """
        ä¸‹è½½æ‰¹å¤„ç†ç»“æœ

        Args:
            batch_id: æ‰¹å¤„ç†ID
            output_dir: è¾“å‡ºç›®å½•

        Returns:
            ç»“æœæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœå¤±è´¥è¿”å›None
        """
        try:
            logger.info(f"ğŸ”½ å¼€å§‹ä¸‹è½½æ‰¹å¤„ç†ç»“æœ: {batch_id}")

            # è·å–æ‰¹å¤„ç†çŠ¶æ€
            status_info = self.get_batch_status(batch_id)
            if not status_info:
                logger.error("âŒ æ— æ³•è·å–æ‰¹å¤„ç†çŠ¶æ€")
                return None

            if status_info['status'] != 'completed':
                logger.error(f"âŒ æ‰¹å¤„ç†æœªå®Œæˆï¼Œå½“å‰çŠ¶æ€: {status_info['status']}")
                return None

            output_file_id = status_info.get('output_file_id')
            if not output_file_id:
                logger.error("âŒ æ²¡æœ‰è¾“å‡ºæ–‡ä»¶ID")
                logger.error("ğŸ’¡ è¿™å¯èƒ½æ„å‘³ç€æ‰¹å¤„ç†è™½ç„¶æ ‡è®°ä¸ºå®Œæˆï¼Œä½†æ²¡æœ‰ç”Ÿæˆç»“æœæ–‡ä»¶")
                return None

            logger.info(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶ID: {output_file_id}")

            # ä¸‹è½½ç»“æœæ–‡ä»¶
            logger.info("ğŸ”½ æ­£åœ¨ä¸‹è½½ç»“æœæ–‡ä»¶...")
            response = self.client.files.content(output_file_id)

            # ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶
            output_path = os.path.join(output_dir, f"batch_results_{batch_id}.jsonl")
            with open(output_path, 'wb') as f:
                f.write(response.content)

            # éªŒè¯æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(output_path)
            logger.info(f"âœ… ç»“æœæ–‡ä»¶å·²ä¸‹è½½: {output_path} (å¤§å°: {file_size} å­—èŠ‚)")

            # ä¸‹è½½é”™è¯¯æ–‡ä»¶ï¼ˆå¦‚æœæœ‰ï¼‰
            error_file_id = status_info.get('error_file_id')
            if error_file_id:
                logger.warning(f"âš ï¸  å‘ç°é”™è¯¯æ–‡ä»¶ID: {error_file_id}")
                try:
                    error_response = self.client.files.content(error_file_id)
                    error_path = os.path.join(output_dir, f"batch_errors_{batch_id}.jsonl")
                    with open(error_path, 'wb') as f:
                        f.write(error_response.content)

                    error_size = os.path.getsize(error_path)
                    logger.warning(f"âš ï¸  é”™è¯¯æ–‡ä»¶å·²ä¸‹è½½: {error_path} (å¤§å°: {error_size} å­—èŠ‚)")

                    # è¯»å–å¹¶è®°å½•é”™è¯¯å†…å®¹çš„å‰å‡ è¡Œ
                    try:
                        with open(error_path, 'r', encoding='utf-8') as f:
                            error_preview = f.read(1000)
                            logger.warning(f"âš ï¸  é”™è¯¯å†…å®¹é¢„è§ˆ: {error_preview}")
                    except Exception as e:
                        logger.warning(f"âš ï¸  æ— æ³•è¯»å–é”™è¯¯æ–‡ä»¶å†…å®¹: {e}")

                except Exception as e:
                    logger.warning(f"âš ï¸  ä¸‹è½½é”™è¯¯æ–‡ä»¶å¤±è´¥: {e}")
            else:
                logger.info("âœ… æ²¡æœ‰é”™è¯¯æ–‡ä»¶")

            return output_path

        except Exception as e:
            logger.error(f"âŒ ä¸‹è½½ç»“æœå¤±è´¥: {e}")
            logger.error(f"ğŸ” é”™è¯¯è¯¦æƒ…: {str(e)}")
            return None

    def process_batch(self, input_file: str, output_dir: str = ".",
                     completion_window: str = "24h", check_interval: int = 60) -> Optional[str]:
        """
        å¤„ç†å®Œæ•´çš„æ‰¹å¤„ç†å·¥ä½œæµç¨‹

        Args:
            input_file: è¾“å…¥JSONLæ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            completion_window: å®Œæˆæ—¶é—´çª—å£
            check_interval: çŠ¶æ€æ£€æŸ¥é—´éš”

        Returns:
            ç»“æœæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœå¤±è´¥è¿”å›None
        """
        print("ğŸš€ å¼€å§‹æ‰¹å¤„ç†å·¥ä½œæµç¨‹")  # ç¡®ä¿è¾“å‡ºåˆ°stdout
        logger.info("ğŸš€ å¼€å§‹æ‰¹å¤„ç†å·¥ä½œæµç¨‹")
        logger.info(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {input_file}")
        logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        logger.info(f"â° å®Œæˆæ—¶é—´çª—å£: {completion_window}")
        logger.info(f"ğŸ”„ æ£€æŸ¥é—´éš”: {check_interval}ç§’")

        # éªŒè¯è¾“å…¥æ–‡ä»¶
        if not os.path.exists(input_file):
            logger.error(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
            print("âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨")
            return None

        file_size = os.path.getsize(input_file)
        logger.info(f"ğŸ“„ è¾“å…¥æ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚")

        # 1. ä¸Šä¼ æ–‡ä»¶
        print("ğŸ“¤ æ­¥éª¤1: ä¸Šä¼ æ–‡ä»¶...")
        logger.info("ğŸ“¤ æ­¥éª¤1: å¼€å§‹ä¸Šä¼ æ–‡ä»¶")
        file_id = self.upload_batch_file(input_file)
        if not file_id:
            logger.error("âŒ æ–‡ä»¶ä¸Šä¼ å¤±è´¥")
            print("âŒ æ–‡ä»¶ä¸Šä¼ å¤±è´¥")
            return None
        logger.info(f"âœ… æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼ŒID: {file_id}")

        # 2. åˆ›å»ºæ‰¹å¤„ç†
        print("ğŸ”¨ æ­¥éª¤2: åˆ›å»ºæ‰¹å¤„ç†...")
        logger.info("ğŸ”¨ æ­¥éª¤2: å¼€å§‹åˆ›å»ºæ‰¹å¤„ç†")
        batch_id = self.create_batch(file_id, completion_window)
        if not batch_id:
            logger.error("âŒ æ‰¹å¤„ç†åˆ›å»ºå¤±è´¥")
            print("âŒ æ‰¹å¤„ç†åˆ›å»ºå¤±è´¥")
            return None
        logger.info(f"âœ… æ‰¹å¤„ç†åˆ›å»ºæˆåŠŸï¼ŒID: {batch_id}")

        # 3. ç­‰å¾…å®Œæˆ
        print("â³ æ­¥éª¤3: ç­‰å¾…æ‰¹å¤„ç†å®Œæˆ...")
        logger.info("â³ æ­¥éª¤3: å¼€å§‹ç­‰å¾…æ‰¹å¤„ç†å®Œæˆ")
        completion_success = self.wait_for_completion(batch_id, check_interval)
        if not completion_success:
            logger.error("âŒ æ‰¹å¤„ç†æ‰§è¡Œå¤±è´¥æˆ–è¶…æ—¶")
            print("âŒ æ‰¹å¤„ç†æ‰§è¡Œå¤±è´¥")
            return None
        logger.info("âœ… æ‰¹å¤„ç†æ‰§è¡Œå®Œæˆ")

        # 4. ä¸‹è½½ç»“æœ
        print("ğŸ“¥ æ­¥éª¤4: ä¸‹è½½ç»“æœ...")
        logger.info("ğŸ“¥ æ­¥éª¤4: å¼€å§‹ä¸‹è½½ç»“æœ")
        result_file = self.download_results(batch_id, output_dir)

        if result_file:
            print(f"âœ… æ‰¹å¤„ç†å·¥ä½œæµç¨‹å®Œæˆï¼Œç»“æœæ–‡ä»¶: {result_file}")
            logger.info(f"âœ… æ‰¹å¤„ç†å·¥ä½œæµç¨‹å®Œæˆï¼Œç»“æœæ–‡ä»¶: {result_file}")
        else:
            logger.error("âŒ ç»“æœä¸‹è½½å¤±è´¥")
            print("âŒ ç»“æœä¸‹è½½å¤±è´¥")

        return result_file

def main():
    parser = argparse.ArgumentParser(description='OpenAI Batch APIå¤„ç†å™¨')
    parser.add_argument('input_file', help='è¾“å…¥JSONLæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--api-key', help='OpenAI APIå¯†é’¥ï¼ˆä¹Ÿå¯é€šè¿‡ç¯å¢ƒå˜é‡OPENAI_API_KEYè®¾ç½®ï¼‰')
    parser.add_argument('--output-dir', default='.', help='è¾“å‡ºç›®å½•ï¼Œé»˜è®¤å½“å‰ç›®å½•')
    parser.add_argument('--completion-window', default='24h', help='å®Œæˆæ—¶é—´çª—å£ï¼Œé»˜è®¤24h')
    parser.add_argument('--check-interval', type=int, default=60, help='çŠ¶æ€æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤60')

    args = parser.parse_args()

    # è·å–APIå¯†é’¥
    api_key = args.api_key or os.getenv('OPENAI_API_KEY')
    if not api_key:
        logger.error("è¯·æä¾›OpenAI APIå¯†é’¥ï¼Œé€šè¿‡--api-keyå‚æ•°æˆ–OPENAI_API_KEYç¯å¢ƒå˜é‡")
        return

    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.input_file):
        logger.error(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input_file}")
        return

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)

    # å¤„ç†æ‰¹å¤„ç†
    processor = BatchProcessor(api_key)
    result_file = processor.process_batch(
        input_file=args.input_file,
        output_dir=args.output_dir,
        completion_window=args.completion_window,
        check_interval=args.check_interval
    )

    if result_file:
        print(f"æ‰¹å¤„ç†æˆåŠŸå®Œæˆï¼ç»“æœæ–‡ä»¶: {result_file}")  # ç¡®ä¿è¾“å‡ºåˆ°stdout
        logger.info(f"æ‰¹å¤„ç†æˆåŠŸå®Œæˆï¼ç»“æœæ–‡ä»¶: {result_file}")
        return 0
    else:
        print("æ‰¹å¤„ç†å¤±è´¥")  # ç¡®ä¿è¾“å‡ºåˆ°stdout
        logger.error("æ‰¹å¤„ç†å¤±è´¥")
        return 1

if __name__ == "__main__":
    import sys
    sys.exit(main())
