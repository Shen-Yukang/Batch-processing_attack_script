#!/usr/bin/env python3
"""
å¥å£®çš„æ‰¹å¤„ç†å™¨ - å…·æœ‰å®Œæ•´çš„å®¹é”™å’Œé‡è¯•æœºåˆ¶
"""

import os
import time
import json
import logging
import subprocess
from datetime import datetime
from typing import List, Dict, Optional
from cost_tracker import CostTracker

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class BatchJob:
    """æ‰¹å¤„ç†ä»»åŠ¡ç±»"""
    def __init__(self, name: str, start_row: int, end_row: int, status: str = "pending"):
        self.name = name
        self.start_row = start_row
        self.end_row = end_row
        self.status = status  # pending, running, completed, failed, timeout
        self.attempts = 0
        self.max_attempts = 3
        self.error_message = ""
        self.batch_id = ""
        self.created_at = datetime.now()
        self.completed_at = None

    def to_dict(self):
        return {
            "name": self.name,
            "start_row": self.start_row,
            "end_row": self.end_row,
            "status": self.status,
            "attempts": self.attempts,
            "max_attempts": self.max_attempts,
            "error_message": self.error_message,
            "batch_id": self.batch_id,
            "created_at": self.created_at.isoformat(),
            "completed_at": self.completed_at.isoformat() if self.completed_at else None
        }

class RobustBatchProcessor:
    """å¥å£®çš„æ‰¹å¤„ç†å™¨"""

    def __init__(self, output_dir: str = "batch_results", batch_size: int = 20):
        self.output_dir = output_dir
        self.batch_size = batch_size
        self.jobs: List[BatchJob] = []
        self.status_file = os.path.join(output_dir, "batch_status.json")

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)

        # åˆå§‹åŒ–æˆæœ¬è·Ÿè¸ªå™¨
        self.cost_tracker = CostTracker(os.path.join(output_dir, "batch_costs.json"))

        # åŠ è½½ä¹‹å‰çš„çŠ¶æ€ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        self.load_status()

    def save_status(self):
        """ä¿å­˜æ‰¹å¤„ç†çŠ¶æ€"""
        status_data = {
            "last_updated": datetime.now().isoformat(),
            "total_jobs": len(self.jobs),
            "jobs": [job.to_dict() for job in self.jobs]
        }

        with open(self.status_file, 'w', encoding='utf-8') as f:
            json.dump(status_data, f, indent=2, ensure_ascii=False)

    def load_status(self):
        """åŠ è½½ä¹‹å‰çš„æ‰¹å¤„ç†çŠ¶æ€"""
        if os.path.exists(self.status_file):
            try:
                with open(self.status_file, 'r', encoding='utf-8') as f:
                    status_data = json.load(f)

                self.jobs = []
                for job_data in status_data.get("jobs", []):
                    job = BatchJob(
                        job_data["name"],
                        job_data["start_row"],
                        job_data["end_row"],
                        job_data["status"]
                    )
                    job.attempts = job_data.get("attempts", 0)
                    job.error_message = job_data.get("error_message", "")
                    job.batch_id = job_data.get("batch_id", "")

                    if job_data.get("created_at"):
                        job.created_at = datetime.fromisoformat(job_data["created_at"])
                    if job_data.get("completed_at"):
                        job.completed_at = datetime.fromisoformat(job_data["completed_at"])

                    self.jobs.append(job)

                logger.info(f"åŠ è½½äº† {len(self.jobs)} ä¸ªæ‰¹å¤„ç†ä»»åŠ¡çš„çŠ¶æ€")

            except Exception as e:
                logger.warning(f"åŠ è½½çŠ¶æ€æ–‡ä»¶å¤±è´¥: {e}")

    def create_jobs(self, start_from: int, end_at: int):
        """åˆ›å»ºæ‰¹å¤„ç†ä»»åŠ¡"""
        current = start_from
        job_num = 1

        while current < end_at:
            end = min(current + self.batch_size, end_at)
            job_name = f"batch_{job_num:03d}"

            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨è¯¥ä»»åŠ¡
            existing_job = next((job for job in self.jobs if job.name == job_name), None)
            if not existing_job:
                job = BatchJob(job_name, current, end)
                self.jobs.append(job)

            current = end
            job_num += 1

        self.save_status()
        logger.info(f"åˆ›å»ºäº† {len(self.jobs)} ä¸ªæ‰¹å¤„ç†ä»»åŠ¡")

    def run_command_with_timeout(self, command: str, timeout: int = 600) -> tuple[bool, str]:
        """è¿è¡Œå‘½ä»¤å¹¶è®¾ç½®è¶…æ—¶"""
        try:
            result = subprocess.run(
                command,
                shell=True,
                capture_output=True,
                text=True,
                timeout=timeout
            )

            if result.returncode == 0:
                return True, result.stdout
            else:
                return False, result.stderr

        except subprocess.TimeoutExpired:
            return False, f"å‘½ä»¤è¶…æ—¶ ({timeout}ç§’)"
        except Exception as e:
            return False, f"å‘½ä»¤æ‰§è¡Œå¼‚å¸¸: {str(e)}"

    def process_single_job(self, job: BatchJob) -> bool:
        """å¤„ç†å•ä¸ªæ‰¹å¤„ç†ä»»åŠ¡"""
        job.attempts += 1
        job.status = "running"
        self.save_status()

        logger.info(f"ğŸ“¦ å¤„ç†ä»»åŠ¡ {job.name} (ç¬¬{job.attempts}æ¬¡å°è¯•)")
        logger.info(f"   è¡ŒèŒƒå›´: {job.start_row+1}-{job.end_row}")

        # æ­¥éª¤1: åˆ›å»ºè¾“å…¥æ–‡ä»¶
        jsonl_file = os.path.join(self.output_dir, f"{job.name}.jsonl")
        create_cmd = f"python create_safe_batch_input.py new_csv/content_CogAgent.csv {jsonl_file} --model gpt-4o-mini --start-row {job.start_row} --end-row {job.end_row}"

        success, output = self.run_command_with_timeout(create_cmd, 120)
        if not success:
            job.status = "failed"
            job.error_message = f"åˆ›å»ºè¾“å…¥æ–‡ä»¶å¤±è´¥: {output}"
            self.save_status()
            logger.error(f"âŒ {job.name} åˆ›å»ºè¾“å…¥æ–‡ä»¶å¤±è´¥: {output}")
            return False

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åˆ›å»º
        if not os.path.exists(jsonl_file):
            job.status = "failed"
            job.error_message = "è¾“å…¥æ–‡ä»¶æœªåˆ›å»º"
            self.save_status()
            logger.error(f"âŒ {job.name} è¾“å…¥æ–‡ä»¶æœªåˆ›å»º")
            return False

        # æ­¥éª¤2: æäº¤æ‰¹å¤„ç†
        process_cmd = f"python batch_processor.py {jsonl_file} --output-dir {self.output_dir} --check-interval 30"

        success, output = self.run_command_with_timeout(process_cmd, 600)
        if success:
            job.status = "completed"
            job.completed_at = datetime.now()
            job.error_message = ""

            # è®°å½•æˆæœ¬ï¼ˆä¼°ç®—ï¼‰
            num_requests = job.end_row - job.start_row
            cost_estimate = self.cost_tracker.estimate_batch_cost(num_requests)
            cost_estimate["actual_completed"] = num_requests
            self.cost_tracker.record_batch_cost(job.name, job.batch_id, cost_estimate)

            self.save_status()
            logger.info(f"âœ… {job.name} å®Œæˆ (ä¼°ç®—æˆæœ¬: ${cost_estimate['batch_cost']:.4f})")
            return True
        else:
            if "è¶…æ—¶" in output:
                job.status = "timeout"
                job.error_message = f"å¤„ç†è¶…æ—¶: {output}"
            else:
                job.status = "failed"
                job.error_message = f"å¤„ç†å¤±è´¥: {output}"

            self.save_status()
            logger.error(f"âŒ {job.name} å¤±è´¥: {output}")
            return False

    def process_all_jobs(self, retry_failed: bool = True):
        """å¤„ç†æ‰€æœ‰æ‰¹å¤„ç†ä»»åŠ¡"""
        logger.info("=" * 80)
        logger.info(f"å¼€å§‹å¤„ç† {len(self.jobs)} ä¸ªæ‰¹å¤„ç†ä»»åŠ¡")
        logger.info("=" * 80)

        for i, job in enumerate(self.jobs, 1):
            # è·³è¿‡å·²å®Œæˆçš„ä»»åŠ¡
            if job.status == "completed":
                logger.info(f"â­ï¸  è·³è¿‡å·²å®Œæˆçš„ä»»åŠ¡ {job.name}")
                continue

            # æ£€æŸ¥é‡è¯•æ¬¡æ•°
            if job.attempts >= job.max_attempts:
                logger.warning(f"âš ï¸  ä»»åŠ¡ {job.name} å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
                continue

            logger.info(f"\nå¤„ç†ä»»åŠ¡ {i}/{len(self.jobs)}: {job.name}")

            success = self.process_single_job(job)

            # åœ¨ä»»åŠ¡ä¹‹é—´æ·»åŠ å»¶è¿Ÿ
            if i < len(self.jobs):
                delay = 30 if success else 60  # å¤±è´¥åç­‰å¾…æ›´é•¿æ—¶é—´
                logger.info(f"ç­‰å¾… {delay} ç§’åå¤„ç†ä¸‹ä¸€ä¸ªä»»åŠ¡...")
                time.sleep(delay)

        self.print_summary()

    def retry_failed_jobs(self):
        """é‡è¯•å¤±è´¥çš„ä»»åŠ¡"""
        failed_jobs = [job for job in self.jobs if job.status in ["failed", "timeout"] and job.attempts < job.max_attempts]

        if not failed_jobs:
            logger.info("æ²¡æœ‰éœ€è¦é‡è¯•çš„å¤±è´¥ä»»åŠ¡")
            return

        logger.info(f"ğŸ”„ é‡è¯• {len(failed_jobs)} ä¸ªå¤±è´¥çš„ä»»åŠ¡")

        for job in failed_jobs:
            logger.info(f"\né‡è¯•ä»»åŠ¡: {job.name}")
            self.process_single_job(job)
            time.sleep(60)  # é‡è¯•é—´éš”æ›´é•¿

        self.print_summary()

    def print_summary(self):
        """æ‰“å°å¤„ç†æ€»ç»“"""
        completed = [job for job in self.jobs if job.status == "completed"]
        failed = [job for job in self.jobs if job.status in ["failed", "timeout"]]
        pending = [job for job in self.jobs if job.status == "pending"]

        logger.info("=" * 80)
        logger.info("æ‰¹å¤„ç†æ€»ç»“")
        logger.info("=" * 80)
        logger.info(f"æ€»ä»»åŠ¡æ•°: {len(self.jobs)}")
        logger.info(f"å·²å®Œæˆ: {len(completed)}")
        logger.info(f"å¤±è´¥/è¶…æ—¶: {len(failed)}")
        logger.info(f"å¾…å¤„ç†: {len(pending)}")
        logger.info(f"å®Œæˆç‡: {len(completed)/len(self.jobs)*100:.1f}%")

        if failed:
            logger.warning("å¤±è´¥çš„ä»»åŠ¡:")
            for job in failed:
                logger.warning(f"  {job.name} (ç¬¬{job.start_row+1}-{job.end_row}è¡Œ): {job.error_message}")

        if completed:
            logger.info("å¯ä»¥åˆå¹¶ç»“æœ:")
            logger.info(f"python merge_all_results.py {self.output_dir} new_csv/content_CogAgent.csv final_output.csv")

            # æ˜¾ç¤ºæˆæœ¬æ€»ç»“
            logger.info("\n" + "=" * 80)
            self.cost_tracker.print_cost_report()

def main():
    import sys

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    start_from = 0  # é»˜è®¤ä»130å¼€å§‹
    end_at = 20      # é»˜è®¤åˆ°172ç»“æŸ
    batch_size = 20   # é»˜è®¤æ¯æ‰¹20è¡Œ

    if len(sys.argv) > 1:
        start_from = int(sys.argv[1])
    if len(sys.argv) > 2:
        end_at = int(sys.argv[2])
    if len(sys.argv) > 3:
        batch_size = int(sys.argv[3])

    # åˆ›å»ºå¤„ç†å™¨
    processor = RobustBatchProcessor("batch_results_20250524_224700", batch_size)

    # åˆ›å»ºä»»åŠ¡ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
    if not processor.jobs:
        processor.create_jobs(start_from, end_at)

    # å¤„ç†æ‰€æœ‰ä»»åŠ¡
    processor.process_all_jobs()

    # è¯¢é—®æ˜¯å¦é‡è¯•å¤±è´¥çš„ä»»åŠ¡
    failed_jobs = [job for job in processor.jobs if job.status in ["failed", "timeout"] and job.attempts < job.max_attempts]
    if failed_jobs:
        response = input(f"\nå‘ç° {len(failed_jobs)} ä¸ªå¤±è´¥ä»»åŠ¡ï¼Œæ˜¯å¦é‡è¯•ï¼Ÿ(y/N): ")
        if response.lower() == 'y':
            processor.retry_failed_jobs()

if __name__ == "__main__":
    main()
