#!/usr/bin/env python3
"""
å¥å£®çš„æ‰¹å¤„ç†å™¨ - å…·æœ‰å®Œæ•´çš„å®¹é”™å’Œé‡è¯•æœºåˆ¶
"""

import os
import time
import json
import logging
import subprocess
import argparse
from datetime import datetime
from typing import List, Dict, Optional
from cost_tracker import CostTracker

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def get_csv_line_count(csv_path):
    """è·å–CSVæ–‡ä»¶çš„è¡Œæ•°"""
    import csv
    try:
        with open(csv_path, 'r', encoding='utf-8') as f:
            reader = csv.reader(f)
            # å‡å»1æ˜¯å› ä¸ºé€šå¸¸ç¬¬ä¸€è¡Œæ˜¯æ ‡é¢˜
            return sum(1 for _ in reader) - 1
    except Exception as e:
        logger.error(f"è¯»å–CSVæ–‡ä»¶å¤±è´¥: {e}")
        return 0

class BatchJob:
    """æ‰¹å¤„ç†ä»»åŠ¡ç±»"""
    def __init__(self, name: str, start_row: int, end_row: int, status: str = "pending"):
        self.name = name
        self.start_row = start_row
        self.end_row = end_row
        self.status = status  # pending, running, completed, failed, timeout
        self.attempts = 0
        self.max_attempts = 3
        self.error_message = ""
        self.batch_id = ""
        self.created_at = datetime.now()
        self.completed_at = None

    def to_dict(self):
        return {
            "name": self.name,
            "start_row": self.start_row,
            "end_row": self.end_row,
            "status": self.status,
            "attempts": self.attempts,
            "max_attempts": self.max_attempts,
            "error_message": self.error_message,
            "batch_id": self.batch_id,
            "created_at": self.created_at.isoformat(),
            "completed_at": self.completed_at.isoformat() if self.completed_at else None
        }

class RobustBatchProcessor:
    """å¥å£®çš„æ‰¹å¤„ç†å™¨"""

    def __init__(self, output_dir: str = "batch_results", batch_size: int = 20, input_csv: str = "_csvs/content_CogAgent.csv", model: str = "gpt-4o-mini"):
        self.output_dir = output_dir
        self.batch_size = batch_size
        self.input_csv = input_csv
        self.model = model
        self.jobs: List[BatchJob] = []
        self.status_file = os.path.join(output_dir, "batch_status.json")

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)

        # åˆå§‹åŒ–æˆæœ¬è·Ÿè¸ªå™¨
        self.cost_tracker = CostTracker(os.path.join(output_dir, "batch_costs.json"))

        # åŠ è½½ä¹‹å‰çš„çŠ¶æ€ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        self.load_status()

    def save_status(self):
        """ä¿å­˜æ‰¹å¤„ç†çŠ¶æ€"""
        status_data = {
            "last_updated": datetime.now().isoformat(),
            "total_jobs": len(self.jobs),
            "jobs": [job.to_dict() for job in self.jobs]
        }

        with open(self.status_file, 'w', encoding='utf-8') as f:
            json.dump(status_data, f, indent=2, ensure_ascii=False)

    def load_status(self):
        """åŠ è½½ä¹‹å‰çš„æ‰¹å¤„ç†çŠ¶æ€"""
        if os.path.exists(self.status_file):
            try:
                with open(self.status_file, 'r', encoding='utf-8') as f:
                    status_data = json.load(f)

                self.jobs = []
                for job_data in status_data.get("jobs", []):
                    job = BatchJob(
                        job_data["name"],
                        job_data["start_row"],
                        job_data["end_row"],
                        job_data["status"]
                    )
                    job.attempts = job_data.get("attempts", 0)
                    job.error_message = job_data.get("error_message", "")
                    job.batch_id = job_data.get("batch_id", "")

                    if job_data.get("created_at"):
                        job.created_at = datetime.fromisoformat(job_data["created_at"])
                    if job_data.get("completed_at"):
                        job.completed_at = datetime.fromisoformat(job_data["completed_at"])

                    self.jobs.append(job)

                logger.info(f"åŠ è½½äº† {len(self.jobs)} ä¸ªæ‰¹å¤„ç†ä»»åŠ¡çš„çŠ¶æ€")

            except Exception as e:
                logger.warning(f"åŠ è½½çŠ¶æ€æ–‡ä»¶å¤±è´¥: {e}")

    def create_jobs(self, start_from: int, end_at: int):
        """åˆ›å»ºæ‰¹å¤„ç†ä»»åŠ¡"""
        current = start_from
        job_num = 1

        while current < end_at:
            end = min(current + self.batch_size, end_at)
            job_name = f"batch_{job_num:03d}"

            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨è¯¥ä»»åŠ¡
            existing_job = next((job for job in self.jobs if job.name == job_name), None)
            if not existing_job:
                job = BatchJob(job_name, current, end)
                self.jobs.append(job)

            current = end
            job_num += 1

        self.save_status()
        logger.info(f"åˆ›å»ºäº† {len(self.jobs)} ä¸ªæ‰¹å¤„ç†ä»»åŠ¡")

    def run_command_with_timeout(self, command: str, timeout: int = 600) -> tuple[bool, str]:
        """è¿è¡Œå‘½ä»¤å¹¶è®¾ç½®è¶…æ—¶"""
        logger.info(f"ğŸ–¥ï¸  æ‰§è¡Œå‘½ä»¤: {command}")
        logger.info(f"â±ï¸  è¶…æ—¶è®¾ç½®: {timeout}ç§’")

        try:
            start_time = datetime.now()
            logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œå‘½ä»¤...")

            result = subprocess.run(
                command,
                shell=True,
                capture_output=True,
                text=True,
                timeout=timeout
            )

            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            logger.info(f"â±ï¸  å‘½ä»¤æ‰§è¡Œè€—æ—¶: {duration:.1f}ç§’")
            logger.info(f"ğŸ” è¿”å›ç : {result.returncode}")

            # æ£€æŸ¥è¿”å›ç å’Œè¾“å‡ºå†…å®¹
            if result.returncode == 0:
                # å³ä½¿è¿”å›ç ä¸º0ï¼Œä¹Ÿè¦æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯ä¿¡æ¯
                combined_output = result.stdout + result.stderr
                logger.debug(f"ğŸ“¤ æ ‡å‡†è¾“å‡º: {result.stdout[:200]}...")
                logger.debug(f"ğŸ“¤ é”™è¯¯è¾“å‡º: {result.stderr[:200]}...")

                # æ£€æŸ¥å¸¸è§çš„é”™è¯¯æ¨¡å¼
                error_patterns = [
                    "è¯·æä¾›OpenAI APIå¯†é’¥",
                    "APIå¯†é’¥",
                    "authentication",
                    "unauthorized",
                    "invalid api key",
                    "error",
                    "failed",
                    "exception"
                ]

                for pattern in error_patterns:
                    if pattern.lower() in combined_output.lower():
                        logger.error(f"âŒ æ£€æµ‹åˆ°é”™è¯¯æ¨¡å¼: {pattern}")
                        return False, combined_output

                # æ£€æŸ¥æ˜¯å¦æœ‰å®é™…çš„æˆåŠŸè¾“å‡º
                success_patterns = ["æ–‡ä»¶ä¸Šä¼ æˆåŠŸ", "æ‰¹å¤„ç†åˆ›å»ºæˆåŠŸ", "æ‰¹å¤„ç†å·²å®Œæˆ", "æ‰¹å¤„ç†æˆåŠŸå®Œæˆ"]
                has_success_indicator = any(pattern in combined_output for pattern in success_patterns)

                if has_success_indicator:
                    logger.info(f"âœ… æ£€æµ‹åˆ°æˆåŠŸæ ‡è¯†")
                    return True, result.stdout
                elif len(combined_output.strip()) == 0:
                    logger.warning(f"âš ï¸  å‘½ä»¤æ‰§è¡Œæ— è¾“å‡ºï¼Œå¯èƒ½å­˜åœ¨é…ç½®é—®é¢˜")
                    return False, "å‘½ä»¤æ‰§è¡Œæ— è¾“å‡ºï¼Œå¯èƒ½å­˜åœ¨é…ç½®é—®é¢˜"
                else:
                    logger.info(f"âœ… å‘½ä»¤æ‰§è¡Œå®Œæˆï¼Œè¿”å›ç ä¸º0")
                    return True, result.stdout
            else:
                logger.error(f"âŒ å‘½ä»¤æ‰§è¡Œå¤±è´¥ï¼Œè¿”å›ç : {result.returncode}")
                logger.error(f"ğŸ” é”™è¯¯è¾“å‡º: {result.stderr}")
                return False, result.stderr

        except subprocess.TimeoutExpired:
            logger.error(f"â° å‘½ä»¤æ‰§è¡Œè¶…æ—¶ ({timeout}ç§’)")
            return False, f"å‘½ä»¤è¶…æ—¶ ({timeout}ç§’)"
        except Exception as e:
            logger.error(f"ğŸ’¥ å‘½ä»¤æ‰§è¡Œå¼‚å¸¸: {str(e)}")
            return False, f"å‘½ä»¤æ‰§è¡Œå¼‚å¸¸: {str(e)}"

    def process_single_job(self, job: BatchJob) -> bool:
        """å¤„ç†å•ä¸ªæ‰¹å¤„ç†ä»»åŠ¡"""
        job.attempts += 1
        job.status = "running"
        self.save_status()

        logger.info(f"ğŸ“¦ å¤„ç†ä»»åŠ¡ {job.name} (ç¬¬{job.attempts}æ¬¡å°è¯•)")
        logger.info(f"   è¡ŒèŒƒå›´: {job.start_row+1}-{job.end_row}")

        # æ­¥éª¤1: åˆ›å»ºè¾“å…¥æ–‡ä»¶
        jsonl_file = os.path.join(self.output_dir, f"{job.name}.jsonl")
        create_cmd = f"python create_safe_batch_input.py {self.input_csv} {jsonl_file} --model {self.model} --start-row {job.start_row} --end-row {job.end_row}"

        success, output = self.run_command_with_timeout(create_cmd, 120)
        if not success:
            job.status = "failed"
            job.error_message = f"åˆ›å»ºè¾“å…¥æ–‡ä»¶å¤±è´¥: {output}"
            self.save_status()
            logger.error(f"âŒ {job.name} åˆ›å»ºè¾“å…¥æ–‡ä»¶å¤±è´¥: {output}")
            return False

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åˆ›å»º
        if not os.path.exists(jsonl_file):
            job.status = "failed"
            job.error_message = "è¾“å…¥æ–‡ä»¶æœªåˆ›å»º"
            self.save_status()
            logger.error(f"âŒ {job.name} è¾“å…¥æ–‡ä»¶æœªåˆ›å»º")
            return False

        # æ­¥éª¤2: æäº¤æ‰¹å¤„ç†
        process_cmd = f"python batch_processor.py {jsonl_file} --output-dir {self.output_dir} --check-interval 30"

        success, output = self.run_command_with_timeout(process_cmd, 600)
        if success:
            job.status = "completed"
            job.completed_at = datetime.now()
            job.error_message = ""

            # è®°å½•æˆæœ¬ï¼ˆä¼°ç®—ï¼‰
            num_requests = job.end_row - job.start_row
            cost_estimate = self.cost_tracker.estimate_batch_cost(num_requests)
            cost_estimate["actual_completed"] = num_requests
            self.cost_tracker.record_batch_cost(job.name, job.batch_id, cost_estimate)

            self.save_status()
            logger.info(f"âœ… {job.name} å®Œæˆ (ä¼°ç®—æˆæœ¬: ${cost_estimate['batch_cost']:.4f})")
            return True
        else:
            if "è¶…æ—¶" in output:
                job.status = "timeout"
                job.error_message = f"å¤„ç†è¶…æ—¶: {output}"
            else:
                job.status = "failed"
                job.error_message = f"å¤„ç†å¤±è´¥: {output}"

            self.save_status()
            logger.error(f"âŒ {job.name} å¤±è´¥: {output}")
            return False

    def process_all_jobs(self, retry_failed: bool = True):
        """å¤„ç†æ‰€æœ‰æ‰¹å¤„ç†ä»»åŠ¡"""
        logger.info("=" * 80)
        logger.info(f"å¼€å§‹å¤„ç† {len(self.jobs)} ä¸ªæ‰¹å¤„ç†ä»»åŠ¡")
        logger.info("=" * 80)

        for i, job in enumerate(self.jobs, 1):
            # è·³è¿‡å·²å®Œæˆçš„ä»»åŠ¡
            if job.status == "completed":
                logger.info(f"â­ï¸  è·³è¿‡å·²å®Œæˆçš„ä»»åŠ¡ {job.name}")
                continue

            # æ£€æŸ¥é‡è¯•æ¬¡æ•°
            if job.attempts >= job.max_attempts:
                logger.warning(f"âš ï¸  ä»»åŠ¡ {job.name} å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
                continue

            logger.info(f"\nå¤„ç†ä»»åŠ¡ {i}/{len(self.jobs)}: {job.name}")

            success = self.process_single_job(job)

            # åœ¨ä»»åŠ¡ä¹‹é—´æ·»åŠ å»¶è¿Ÿ
            if i < len(self.jobs):
                delay = 30 if success else 60  # å¤±è´¥åç­‰å¾…æ›´é•¿æ—¶é—´
                logger.info(f"ç­‰å¾… {delay} ç§’åå¤„ç†ä¸‹ä¸€ä¸ªä»»åŠ¡...")
                time.sleep(delay)

        self.print_summary()

    def retry_failed_jobs(self):
        """é‡è¯•å¤±è´¥çš„ä»»åŠ¡"""
        failed_jobs = [job for job in self.jobs if job.status in ["failed", "timeout"] and job.attempts < job.max_attempts]

        if not failed_jobs:
            logger.info("æ²¡æœ‰éœ€è¦é‡è¯•çš„å¤±è´¥ä»»åŠ¡")
            return

        logger.info(f"ğŸ”„ é‡è¯• {len(failed_jobs)} ä¸ªå¤±è´¥çš„ä»»åŠ¡")

        for job in failed_jobs:
            logger.info(f"\né‡è¯•ä»»åŠ¡: {job.name}")
            self.process_single_job(job)
            time.sleep(60)  # é‡è¯•é—´éš”æ›´é•¿

        self.print_summary()

    def print_summary(self):
        """æ‰“å°å¤„ç†æ€»ç»“"""
        completed = [job for job in self.jobs if job.status == "completed"]
        failed = [job for job in self.jobs if job.status in ["failed", "timeout"]]
        pending = [job for job in self.jobs if job.status == "pending"]

        logger.info("=" * 80)
        logger.info("æ‰¹å¤„ç†æ€»ç»“")
        logger.info("=" * 80)
        logger.info(f"æ€»ä»»åŠ¡æ•°: {len(self.jobs)}")
        logger.info(f"å·²å®Œæˆ: {len(completed)}")
        logger.info(f"å¤±è´¥/è¶…æ—¶: {len(failed)}")
        logger.info(f"å¾…å¤„ç†: {len(pending)}")
        logger.info(f"å®Œæˆç‡: {len(completed)/len(self.jobs)*100:.1f}%")

        if failed:
            logger.warning("å¤±è´¥çš„ä»»åŠ¡:")
            for job in failed:
                logger.warning(f"  {job.name} (ç¬¬{job.start_row+1}-{job.end_row}è¡Œ): {job.error_message}")

        if completed:
            logger.info("å¯ä»¥åˆå¹¶ç»“æœ:")
            logger.info(f"python merge_all_results.py {self.output_dir} {self.input_csv} final_output.csv")

            # æ˜¾ç¤ºæˆæœ¬æ€»ç»“
            logger.info("\n" + "=" * 80)
            self.cost_tracker.print_cost_report()

def main():
    parser = argparse.ArgumentParser(description="å¥å£®çš„æ‰¹å¤„ç†å™¨")
    parser.add_argument("--input-csv", required=True, help="è¾“å…¥CSVæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--start-row", type=int, default=0, help="èµ·å§‹è¡Œ")
    parser.add_argument("--end-row", type=int, default=None, help="ç»“æŸè¡Œï¼Œä¸æŒ‡å®šåˆ™ä½¿ç”¨æ–‡ä»¶æ€»è¡Œæ•°")
    parser.add_argument("--batch-size", type=int, default=20, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--output-dir", default=None, help="è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä¸ºä»¥æ—¶é—´æˆ³å‘½åçš„ç›®å½•")
    parser.add_argument("--model", default="gpt-4o-mini", help="ä½¿ç”¨çš„æ¨¡å‹")
    args = parser.parse_args()

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.input_csv):
        logger.error(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input_csv}")
        import sys
        sys.exit(1)

    # å¦‚æœæœªæŒ‡å®šç»“æŸè¡Œï¼Œè·å–CSVæ–‡ä»¶çš„è¡Œæ•°
    if args.end_row is None:
        args.end_row = get_csv_line_count(args.input_csv)
        logger.info(f"æ£€æµ‹åˆ°CSVæ–‡ä»¶ {args.input_csv} å…±æœ‰ {args.end_row} è¡Œæ•°æ®")

    # è®¾ç½®è¾“å‡ºç›®å½•
    output_dir = args.output_dir if args.output_dir else f"batch_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

    # åˆ›å»ºå¤„ç†å™¨
    processor = RobustBatchProcessor(output_dir, args.batch_size, args.input_csv, args.model)

    # åˆ›å»ºä»»åŠ¡ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
    if not processor.jobs:
        processor.create_jobs(args.start_row, args.end_row)

    # å¤„ç†æ‰€æœ‰ä»»åŠ¡
    processor.process_all_jobs()

    # è¯¢é—®æ˜¯å¦é‡è¯•å¤±è´¥çš„ä»»åŠ¡
    failed_jobs = [job for job in processor.jobs if job.status in ["failed", "timeout"] and job.attempts < job.max_attempts]
    if failed_jobs:
        response = input(f"\nå‘ç° {len(failed_jobs)} ä¸ªå¤±è´¥ä»»åŠ¡ï¼Œæ˜¯å¦é‡è¯•ï¼Ÿ(y/N): ")
        if response.lower() == 'y':
            processor.retry_failed_jobs()

if __name__ == "__main__":
    main()
