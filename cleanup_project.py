#!/usr/bin/env python3
"""
é¡¹ç›®æ¸…ç†è„šæœ¬
ç”¨äºæ¸…ç†å’Œé‡ç»„ChatGPTæ‰¹å¤„ç†é¡¹ç›®çš„ç›®å½•ç»“æ„
"""

import os
import shutil
import glob
from pathlib import Path
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class ProjectCleaner:
    def __init__(self, project_root="."):
        self.project_root = Path(project_root)
        self.backup_dir = self.project_root / "backup_before_cleanup"
        
    def create_backup(self):
        """åˆ›å»ºå¤‡ä»½"""
        logger.info("ğŸ”„ åˆ›å»ºé¡¹ç›®å¤‡ä»½...")
        
        if self.backup_dir.exists():
            shutil.rmtree(self.backup_dir)
        
        # å¤‡ä»½é‡è¦æ–‡ä»¶
        important_files = [
            "batch_processor.py",
            "robust_batch_processor.py", 
            "cost_tracker.py",
            "create_safe_batch_input.py",
            "process_batch_results.py",
            "batch_workflow.py",
            "resume_batch_processing.py",
            "requirements.txt",
            "README.md"
        ]
        
        self.backup_dir.mkdir(exist_ok=True)
        
        for file in important_files:
            src = self.project_root / file
            if src.exists():
                shutil.copy2(src, self.backup_dir / file)
                logger.info(f"âœ… å¤‡ä»½: {file}")
        
        # å¤‡ä»½é…ç½®æ–‡ä»¶
        config_files = glob.glob(str(self.project_root / "*.conf"))
        for config_file in config_files:
            shutil.copy2(config_file, self.backup_dir)
            logger.info(f"âœ… å¤‡ä»½é…ç½®: {os.path.basename(config_file)}")
        
        logger.info(f"ğŸ“ å¤‡ä»½å®Œæˆï¼Œä¿å­˜åœ¨: {self.backup_dir}")
    
    def clean_cache_files(self):
        """æ¸…ç†ç¼“å­˜æ–‡ä»¶"""
        logger.info("ğŸ§¹ æ¸…ç†ç¼“å­˜æ–‡ä»¶...")
        
        # æ¸…ç†__pycache__ç›®å½•
        pycache_dirs = glob.glob(str(self.project_root / "**/__pycache__"), recursive=True)
        for cache_dir in pycache_dirs:
            shutil.rmtree(cache_dir)
            logger.info(f"ğŸ—‘ï¸  åˆ é™¤ç¼“å­˜: {cache_dir}")
        
        # æ¸…ç†.pycæ–‡ä»¶
        pyc_files = glob.glob(str(self.project_root / "**/*.pyc"), recursive=True)
        for pyc_file in pyc_files:
            os.remove(pyc_file)
            logger.info(f"ğŸ—‘ï¸  åˆ é™¤: {pyc_file}")
        
        # æ¸…ç†.DS_Storeæ–‡ä»¶ (macOS)
        ds_store_files = glob.glob(str(self.project_root / "**/.DS_Store"), recursive=True)
        for ds_file in ds_store_files:
            os.remove(ds_file)
            logger.info(f"ğŸ—‘ï¸  åˆ é™¤: {ds_file}")
    
    def identify_redundant_scripts(self):
        """è¯†åˆ«å†—ä½™è„šæœ¬"""
        logger.info("ğŸ” è¯†åˆ«å†—ä½™è„šæœ¬...")
        
        # å¯ä»¥åˆ é™¤çš„è„šæœ¬æ¨¡å¼
        delete_patterns = [
            "debug_*.py",
            "test_*.py",
            "fix_*.py", 
            "check_*.py",
            "analyze_*.py",
            "diagnose_*.py",
            "enhanced_*.py",
            "intelligent_*.py",
            "smart_*.py",
            "optimize_*.py",
            "monitor_*.py",
            "cleanup_*.py",
            "correct_*.py",
            "finish_*.py",
            "retry_*.py",
            "find_*.py"
        ]
        
        redundant_scripts = []
        for pattern in delete_patterns:
            matches = glob.glob(str(self.project_root / pattern))
            redundant_scripts.extend(matches)
        
        # ç‰¹å®šçš„å†—ä½™è„šæœ¬
        specific_redundant = [
            "simple_test.py",
            "test_script.py", 
            "run_example.py",
            "complete_batch_execution.py",
            "process_all_batches.py",
            "process_remaining_batches.py",
            "merge_all_results.py",
            "auto_batch_processor.sh",
            "test_environment.sh"
        ]
        
        for script in specific_redundant:
            script_path = self.project_root / script
            if script_path.exists():
                redundant_scripts.append(str(script_path))
        
        logger.info(f"ğŸ“‹ å‘ç° {len(redundant_scripts)} ä¸ªå†—ä½™è„šæœ¬:")
        for script in redundant_scripts:
            logger.info(f"   - {os.path.basename(script)}")
        
        return redundant_scripts
    
    def identify_redundant_outputs(self):
        """è¯†åˆ«å†—ä½™è¾“å‡ºæ–‡ä»¶"""
        logger.info("ğŸ” è¯†åˆ«å†—ä½™è¾“å‡ºæ–‡ä»¶...")
        
        redundant_outputs = []
        
        # æ—§çš„æ‰¹å¤„ç†ç»“æœç›®å½•
        old_batch_dirs = glob.glob(str(self.project_root / "batch_results_*"))
        redundant_outputs.extend(old_batch_dirs)
        
        # ä¸´æ—¶CSVæ–‡ä»¶
        temp_csvs = [
            "current_status.csv",
            "new_output.csv", 
            "output.csv",
            "final_output*.csv",
            "complete_final_output.csv",
            "quick_test_result.csv",
            "resume_data_*.csv"
        ]
        
        for pattern in temp_csvs:
            matches = glob.glob(str(self.project_root / pattern))
            redundant_outputs.extend(matches)
        
        # ä¸´æ—¶JSONLæ–‡ä»¶
        temp_jsonls = glob.glob(str(self.project_root / "*.jsonl"))
        redundant_outputs.extend(temp_jsonls)
        
        # ä¸´æ—¶JSONæ–‡ä»¶
        temp_jsons = glob.glob(str(self.project_root / "batch_analysis_*.json"))
        redundant_outputs.extend(temp_jsons)
        
        # ä¸´æ—¶æ–‡æœ¬æ–‡ä»¶
        temp_txts = glob.glob(str(self.project_root / "*missing_rows*.txt"))
        redundant_outputs.extend(temp_txts)
        
        logger.info(f"ğŸ“‹ å‘ç° {len(redundant_outputs)} ä¸ªå†—ä½™è¾“å‡ºæ–‡ä»¶:")
        for output in redundant_outputs[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            logger.info(f"   - {os.path.basename(output)}")
        if len(redundant_outputs) > 10:
            logger.info(f"   ... è¿˜æœ‰ {len(redundant_outputs) - 10} ä¸ªæ–‡ä»¶")
        
        return redundant_outputs
    
    def identify_redundant_docs(self):
        """è¯†åˆ«å†—ä½™æ–‡æ¡£"""
        logger.info("ğŸ” è¯†åˆ«å†—ä½™æ–‡æ¡£...")
        
        redundant_docs = []
        
        # å¤šä½™çš„æŒ‡å—æ–‡æ¡£
        guide_docs = [
            "AUTO_BATCH_GUIDE.md",
            "ENHANCED_PROCESSOR_GUIDE.md", 
            "COMPLETE_EXECUTION_GUIDE.md",
            "missing_rows_guide.md"
        ]
        
        for doc in guide_docs:
            doc_path = self.project_root / doc
            if doc_path.exists():
                redundant_docs.append(str(doc_path))
        
        logger.info(f"ğŸ“‹ å‘ç° {len(redundant_docs)} ä¸ªå†—ä½™æ–‡æ¡£:")
        for doc in redundant_docs:
            logger.info(f"   - {os.path.basename(doc)}")
        
        return redundant_docs
    
    def create_cleanup_summary(self, redundant_scripts, redundant_outputs, redundant_docs):
        """åˆ›å»ºæ¸…ç†æ‘˜è¦"""
        summary_file = self.project_root / "CLEANUP_SUMMARY.md"
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write("# é¡¹ç›®æ¸…ç†æ‘˜è¦\n\n")
            f.write(f"æ¸…ç†æ—¶é—´: {logging.Formatter().formatTime(logging.LogRecord('', 0, '', 0, '', (), None))}\n\n")
            
            f.write("## å¯åˆ é™¤çš„å†—ä½™è„šæœ¬\n\n")
            for script in redundant_scripts:
                f.write(f"- {os.path.basename(script)}\n")
            
            f.write("\n## å¯åˆ é™¤çš„å†—ä½™è¾“å‡ºæ–‡ä»¶\n\n")
            for output in redundant_outputs:
                f.write(f"- {os.path.basename(output)}\n")
            
            f.write("\n## å¯åˆ é™¤çš„å†—ä½™æ–‡æ¡£\n\n")
            for doc in redundant_docs:
                f.write(f"- {os.path.basename(doc)}\n")
            
            f.write("\n## å»ºè®®ä¿ç•™çš„æ ¸å¿ƒæ–‡ä»¶\n\n")
            core_files = [
                "batch_processor.py",
                "robust_batch_processor.py",
                "cost_tracker.py", 
                "create_safe_batch_input.py",
                "process_batch_results.py",
                "batch_workflow.py",
                "resume_batch_processing.py",
                "quick_test.py",
                "view_costs.py",
                "requirements.txt",
                "README.md",
                "BATCH_GUIDE.md"
            ]
            
            for file in core_files:
                f.write(f"- {file}\n")
        
        logger.info(f"ğŸ“„ æ¸…ç†æ‘˜è¦å·²ä¿å­˜: {summary_file}")
    
    def run_analysis(self):
        """è¿è¡Œåˆ†æï¼Œä¸æ‰§è¡Œåˆ é™¤"""
        logger.info("ğŸš€ å¼€å§‹é¡¹ç›®æ¸…ç†åˆ†æ...")
        
        # åˆ›å»ºå¤‡ä»½
        self.create_backup()
        
        # æ¸…ç†ç¼“å­˜æ–‡ä»¶
        self.clean_cache_files()
        
        # è¯†åˆ«å†—ä½™æ–‡ä»¶
        redundant_scripts = self.identify_redundant_scripts()
        redundant_outputs = self.identify_redundant_outputs()
        redundant_docs = self.identify_redundant_docs()
        
        # åˆ›å»ºæ¸…ç†æ‘˜è¦
        self.create_cleanup_summary(redundant_scripts, redundant_outputs, redundant_docs)
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_redundant = len(redundant_scripts) + len(redundant_outputs) + len(redundant_docs)
        logger.info("=" * 60)
        logger.info("ğŸ“Š æ¸…ç†åˆ†æå®Œæˆ")
        logger.info("=" * 60)
        logger.info(f"å†—ä½™è„šæœ¬: {len(redundant_scripts)} ä¸ª")
        logger.info(f"å†—ä½™è¾“å‡º: {len(redundant_outputs)} ä¸ª") 
        logger.info(f"å†—ä½™æ–‡æ¡£: {len(redundant_docs)} ä¸ª")
        logger.info(f"æ€»è®¡å†—ä½™æ–‡ä»¶: {total_redundant} ä¸ª")
        logger.info("=" * 60)
        logger.info("ğŸ“‹ è¯·æŸ¥çœ‹ CLEANUP_SUMMARY.md äº†è§£è¯¦ç»†ä¿¡æ¯")
        logger.info("ğŸ”§ è¯·æŸ¥çœ‹ PROJECT_RESTRUCTURE_PLAN.md äº†è§£é‡æ„è®¡åˆ’")

def main():
    cleaner = ProjectCleaner()
    cleaner.run_analysis()

if __name__ == "__main__":
    main()
