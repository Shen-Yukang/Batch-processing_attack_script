#!/usr/bin/env python3
"""
å¤„ç†å‰©ä½™æ‰¹æ¬¡çš„è„šæœ¬ï¼ˆæ›´å°çš„æ‰¹æ¬¡å¤§å°ï¼‰
"""

import os
import time
import logging
import subprocess
from datetime import datetime

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def run_command_with_timeout(command, description, timeout=600):
    """è¿è¡Œå‘½ä»¤å¹¶è®¾ç½®è¶…æ—¶"""
    logger.info(f"æ‰§è¡Œ: {description}")
    logger.info(f"å‘½ä»¤: {command}")
    
    try:
        result = subprocess.run(command, shell=True, capture_output=True, text=True, timeout=timeout)
        
        if result.returncode == 0:
            logger.info(f"âœ… {description} æˆåŠŸ")
            return True
        else:
            logger.error(f"âŒ {description} å¤±è´¥")
            logger.error(f"é”™è¯¯: {result.stderr.strip()}")
            return False
            
    except subprocess.TimeoutExpired:
        logger.error(f"âŒ {description} è¶…æ—¶ ({timeout}ç§’)")
        return False
    except Exception as e:
        logger.error(f"âŒ {description} å¼‚å¸¸: {e}")
        return False

def create_small_batch_configs(start_from=30):
    """åˆ›å»ºæ›´å°çš„æ‰¹æ¬¡é…ç½®ï¼ˆæ¯æ‰¹20è¡Œï¼‰"""
    batches = []
    current = start_from
    batch_num = 2
    
    while current < 172:
        end = min(current + 20, 172)
        batches.append({
            "name": f"batch{batch_num}",
            "start": current,
            "end": end
        })
        current = end
        batch_num += 1
    
    return batches

def process_remaining_batches(start_from=30):
    """å¤„ç†å‰©ä½™çš„æ‰¹æ¬¡"""
    
    # æ£€æŸ¥APIå¯†é’¥
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        logger.error("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ OPENAI_API_KEY")
        return False
    
    # ä½¿ç”¨ç°æœ‰çš„è¾“å‡ºç›®å½•
    output_dir = "batch_results_20250524_224700"
    if not os.path.exists(output_dir):
        logger.error(f"è¾“å‡ºç›®å½•ä¸å­˜åœ¨: {output_dir}")
        return False
    
    # è·å–æ‰¹æ¬¡é…ç½®
    batches = create_small_batch_configs(start_from)
    
    logger.info("=" * 80)
    logger.info(f"å¤„ç†å‰©ä½™ {len(batches)} ä¸ªæ‰¹æ¬¡ï¼ˆæ¯æ‰¹20è¡Œï¼‰")
    logger.info("=" * 80)
    
    successful_batches = []
    failed_batches = []
    
    for i, batch_config in enumerate(batches, 1):
        batch_name = batch_config["name"]
        start_row = batch_config["start"]
        end_row = batch_config["end"]
        
        logger.info(f"\nğŸ“¦ å¤„ç†æ‰¹æ¬¡ {i}/{len(batches)}: {batch_name}")
        logger.info(f"è¡ŒèŒƒå›´: {start_row+1}-{end_row}")
        
        # æ­¥éª¤1: åˆ›å»ºæ‰¹å¤„ç†è¾“å…¥æ–‡ä»¶
        jsonl_file = os.path.join(output_dir, f"{batch_name}.jsonl")
        create_cmd = f"python create_safe_batch_input.py new_csv/content_CogAgent.csv {jsonl_file} --model gpt-4o-mini --start-row {start_row} --end-row {end_row}"
        
        if not run_command_with_timeout(create_cmd, f"åˆ›å»º {batch_name} è¾“å…¥æ–‡ä»¶", 120):
            failed_batches.append(batch_name)
            continue
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åˆ›å»ºæˆåŠŸ
        if not os.path.exists(jsonl_file):
            logger.error(f"âŒ {batch_name} è¾“å…¥æ–‡ä»¶æœªåˆ›å»º")
            failed_batches.append(batch_name)
            continue
        
        # æ­¥éª¤2: æäº¤æ‰¹å¤„ç†ï¼ˆè®¾ç½®10åˆ†é’Ÿè¶…æ—¶ï¼‰
        process_cmd = f"python batch_processor.py {jsonl_file} --output-dir {output_dir} --check-interval 30"
        
        if run_command_with_timeout(process_cmd, f"å¤„ç† {batch_name} æ‰¹æ¬¡", 600):
            successful_batches.append(batch_name)
            logger.info(f"âœ… {batch_name} æ‰¹æ¬¡å®Œæˆ")
        else:
            failed_batches.append(batch_name)
            logger.error(f"âŒ {batch_name} æ‰¹æ¬¡å¤±è´¥æˆ–è¶…æ—¶")
        
        # åœ¨æ‰¹æ¬¡ä¹‹é—´æ·»åŠ æ›´é•¿çš„å»¶è¿Ÿ
        if i < len(batches):
            logger.info("ç­‰å¾…60ç§’åå¤„ç†ä¸‹ä¸€ä¸ªæ‰¹æ¬¡...")
            time.sleep(60)
    
    # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
    logger.info("=" * 80)
    logger.info("å‰©ä½™æ‰¹æ¬¡å¤„ç†å®Œæˆæ€»ç»“")
    logger.info("=" * 80)
    logger.info(f"æˆåŠŸæ‰¹æ¬¡ ({len(successful_batches)}): {', '.join(successful_batches)}")
    
    if failed_batches:
        logger.warning(f"å¤±è´¥æ‰¹æ¬¡ ({len(failed_batches)}): {', '.join(failed_batches)}")
        logger.info("å¯ä»¥æ‰‹åŠ¨é‡è¯•å¤±è´¥çš„æ‰¹æ¬¡")
    
    return len(failed_batches) == 0

def main():
    import sys
    
    start_from = 30  # ä»ç¬¬30è¡Œå¼€å§‹ï¼ˆbatch2ï¼‰
    if len(sys.argv) > 1:
        try:
            start_from = int(sys.argv[1])
        except ValueError:
            logger.error("è¯·æä¾›æœ‰æ•ˆçš„èµ·å§‹è¡Œå·")
            return
    
    logger.info(f"ğŸš€ ä»ç¬¬{start_from+1}è¡Œå¼€å§‹å¤„ç†å‰©ä½™æ‰¹æ¬¡")
    
    success = process_remaining_batches(start_from)
    
    if success:
        logger.info("ğŸ‰ æ‰€æœ‰å‰©ä½™æ‰¹æ¬¡å¤„ç†æˆåŠŸï¼")
        logger.info("ç°åœ¨å¯ä»¥åˆå¹¶æ‰€æœ‰ç»“æœ:")
        logger.info("python merge_all_results.py batch_results_20250524_224700 new_csv/content_CogAgent.csv final_output.csv")
    else:
        logger.warning("âš ï¸ éƒ¨åˆ†æ‰¹æ¬¡å¤„ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")

if __name__ == "__main__":
    main()
