#!/usr/bin/env python3
"""
å¥å£®çš„æ‰¹å¤„ç†å™¨ - å…·æœ‰å®Œæ•´çš„å®¹é”™å’Œé‡è¯•æœºåˆ¶
å¢å¼ºç‰ˆï¼šæ”¯æŒäº¤äº’å¼CSVé€‰æ‹©ã€æ™ºèƒ½åˆå¹¶ã€å®‰å…¨çš„å‘½ä»¤æ‰§è¡Œ
"""

import os
import time
import json
import logging
import subprocess
import argparse
import glob
import shlex
import pandas as pd
import base64
from datetime import datetime
from typing import List, Dict, Optional, Tuple
from cost_tracker import CostTracker

def setup_enhanced_logging(log_dir: str = "output/logs") -> logging.Logger:
    """è®¾ç½®å¢å¼ºçš„æ—¥å¿—è®°å½•"""
    os.makedirs(log_dir, exist_ok=True)

    # åˆ›å»ºæ—¥å¿—æ–‡ä»¶å
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_file = os.path.join(log_dir, f"batch_processor_{timestamp}.log")

    # é…ç½®æ ¹æ—¥å¿—è®°å½•å™¨
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.DEBUG)

    # æ¸…é™¤ç°æœ‰çš„handlers
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)

    # åˆ›å»ºæ ¼å¼åŒ–å™¨
    detailed_formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s'
    )
    simple_formatter = logging.Formatter(
        '%(asctime)s - %(levelname)s - %(message)s'
    )

    # æ–‡ä»¶handler - è®°å½•æ‰€æœ‰è¯¦ç»†ä¿¡æ¯
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(detailed_formatter)
    root_logger.addHandler(file_handler)

    # æ§åˆ¶å°handler - æ˜¾ç¤ºé‡è¦ä¿¡æ¯
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(simple_formatter)
    root_logger.addHandler(console_handler)

    logger = logging.getLogger(__name__)
    logger.info(f"ğŸ“ è¯¦ç»†æ—¥å¿—ä¿å­˜åˆ°: {log_file}")

    return logger

# åˆå§‹åŒ–æ—¥å¿—
logger = setup_enhanced_logging()

def get_csv_line_count(csv_path):
    """è·å–CSVæ–‡ä»¶çš„è¡Œæ•°"""
    import csv
    try:
        with open(csv_path, 'r', encoding='utf-8') as f:
            reader = csv.reader(f)
            # å‡å»1æ˜¯å› ä¸ºé€šå¸¸ç¬¬ä¸€è¡Œæ˜¯æ ‡é¢˜
            return sum(1 for _ in reader) - 1
    except Exception as e:
        logger.error(f"è¯»å–CSVæ–‡ä»¶å¤±è´¥: {e}")
        return 0

def find_csv_files(search_dirs: List[str] = ["_csvs", ".", "data"]) -> List[str]:
    """æŸ¥æ‰¾æ‰€æœ‰å¯ç”¨çš„CSVæ–‡ä»¶"""
    csv_files = []

    for search_dir in search_dirs:
        if os.path.exists(search_dir):
            pattern = os.path.join(search_dir, "*.csv")
            found_files = glob.glob(pattern)
            csv_files.extend(found_files)

    # å»é‡å¹¶æ’åº
    csv_files = sorted(list(set(csv_files)))
    return csv_files

def interactive_csv_selection() -> Optional[str]:
    """äº¤äº’å¼é€‰æ‹©CSVæ–‡ä»¶"""
    logger.info("ğŸ” æœç´¢å¯ç”¨çš„CSVæ–‡ä»¶...")

    csv_files = find_csv_files()

    if not csv_files:
        logger.error("âŒ æœªæ‰¾åˆ°ä»»ä½•CSVæ–‡ä»¶")
        logger.info("ğŸ’¡ è¯·ç¡®ä¿CSVæ–‡ä»¶ä½äºä»¥ä¸‹ç›®å½•ä¹‹ä¸€: _csvs/, data/, æˆ–å½“å‰ç›®å½•")
        return None

    logger.info(f"ğŸ“Š æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶:")

    # æ˜¾ç¤ºæ–‡ä»¶åˆ—è¡¨
    for i, csv_file in enumerate(csv_files, 1):
        try:
            lines = get_csv_line_count(csv_file)
            file_size = os.path.getsize(csv_file) / 1024 / 1024  # MB
            logger.info(f"  {i:2d}. {csv_file}")
            logger.info(f"      ğŸ“Š {lines} è¡Œæ•°æ®, ğŸ’¾ {file_size:.2f} MB")
        except Exception as e:
            logger.warning(f"  {i:2d}. {csv_file} (æ— æ³•è¯»å–: {e})")

    # ç”¨æˆ·é€‰æ‹©
    while True:
        try:
            choice = input(f"\nè¯·é€‰æ‹©CSVæ–‡ä»¶ (1-{len(csv_files)}) æˆ–è¾“å…¥ 'q' é€€å‡º: ").strip()

            if choice.lower() == 'q':
                logger.info("ğŸ›‘ ç”¨æˆ·å–æ¶ˆé€‰æ‹©")
                return None

            choice_num = int(choice) - 1
            if 0 <= choice_num < len(csv_files):
                selected_file = csv_files[choice_num]
                logger.info(f"âœ… é€‰æ‹©äº†æ–‡ä»¶: {selected_file}")
                return selected_file
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡è¯•")

        except ValueError:
            print("âŒ è¯·è¾“å…¥æ•°å­—")
        except KeyboardInterrupt:
            logger.info("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­é€‰æ‹©")
            return None

def safe_command_execution(command_parts: List[str], timeout: int = 600) -> Tuple[bool, str]:
    """å®‰å…¨çš„å‘½ä»¤æ‰§è¡Œï¼Œé¿å…shellæ³¨å…¥"""
    try:
        logger.debug(f"ğŸ–¥ï¸  æ‰§è¡Œå‘½ä»¤: {' '.join(command_parts)}")

        start_time = datetime.now()
        result = subprocess.run(
            command_parts,  # ç›´æ¥ä¼ é€’åˆ—è¡¨ï¼Œä¸ä½¿ç”¨shell=True
            capture_output=True,
            text=True,
            timeout=timeout
        )

        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        logger.debug(f"â±ï¸  å‘½ä»¤æ‰§è¡Œè€—æ—¶: {duration:.1f}ç§’")

        return result.returncode == 0, result.stdout if result.returncode == 0 else result.stderr

    except subprocess.TimeoutExpired:
        logger.error(f"â° å‘½ä»¤æ‰§è¡Œè¶…æ—¶ ({timeout}ç§’)")
        return False, f"å‘½ä»¤è¶…æ—¶ ({timeout}ç§’)"
    except Exception as e:
        logger.error(f"ğŸ’¥ å‘½ä»¤æ‰§è¡Œå¼‚å¸¸: {str(e)}")
        return False, f"å‘½ä»¤æ‰§è¡Œå¼‚å¸¸: {str(e)}"

class BatchJob:
    """æ‰¹å¤„ç†ä»»åŠ¡ç±»"""
    def __init__(self, name: str, start_row: int, end_row: int, status: str = "pending"):
        self.name = name
        self.start_row = start_row
        self.end_row = end_row
        self.status = status  # pending, running, completed, failed, timeout
        self.attempts = 0
        self.max_attempts = 3
        self.error_message = ""
        self.batch_id = ""
        self.created_at = datetime.now()
        self.completed_at = None

    def to_dict(self):
        return {
            "name": self.name,
            "start_row": self.start_row,
            "end_row": self.end_row,
            "status": self.status,
            "attempts": self.attempts,
            "max_attempts": self.max_attempts,
            "error_message": self.error_message,
            "batch_id": self.batch_id,
            "created_at": self.created_at.isoformat(),
            "completed_at": self.completed_at.isoformat() if self.completed_at else None
        }

class RobustBatchProcessor:
    """å¥å£®çš„æ‰¹å¤„ç†å™¨ - å¢å¼ºç‰ˆ"""

    def __init__(self, input_csv: str, batch_size: int = 50, model: str = "gpt-4o-mini", batch_interval: int = 120, output_base_dir: str = "output"):
        self.input_csv = input_csv
        self.batch_size = batch_size
        self.model = model
        self.batch_interval = batch_interval
        self.output_base_dir = output_base_dir

        # åˆ›å»ºåŸºäºCSVæ–‡ä»¶åå’Œæ—¶é—´æˆ³çš„è¾“å‡ºç›®å½•
        csv_basename = os.path.splitext(os.path.basename(input_csv))[0]
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        self.output_dir = os.path.join(output_base_dir, f"batch_results_{csv_basename}_{timestamp}")

        self.jobs: List[BatchJob] = []
        self.status_file = os.path.join(self.output_dir, "batch_status.json")

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.output_dir, exist_ok=True)
        logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")

        # åˆå§‹åŒ–æˆæœ¬è·Ÿè¸ªå™¨
        self.cost_tracker = CostTracker(os.path.join(self.output_dir, "batch_costs.json"))

        # åŠ è½½ä¹‹å‰çš„çŠ¶æ€ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        self.load_status()

    def save_status(self):
        """ä¿å­˜æ‰¹å¤„ç†çŠ¶æ€"""
        status_data = {
            "last_updated": datetime.now().isoformat(),
            "total_jobs": len(self.jobs),
            "jobs": [job.to_dict() for job in self.jobs]
        }

        with open(self.status_file, 'w', encoding='utf-8') as f:
            json.dump(status_data, f, indent=2, ensure_ascii=False)

    def load_status(self):
        """åŠ è½½ä¹‹å‰çš„æ‰¹å¤„ç†çŠ¶æ€"""
        if os.path.exists(self.status_file):
            try:
                with open(self.status_file, 'r', encoding='utf-8') as f:
                    status_data = json.load(f)

                self.jobs = []
                for job_data in status_data.get("jobs", []):
                    job = BatchJob(
                        job_data["name"],
                        job_data["start_row"],
                        job_data["end_row"],
                        job_data["status"]
                    )
                    job.attempts = job_data.get("attempts", 0)
                    job.error_message = job_data.get("error_message", "")
                    job.batch_id = job_data.get("batch_id", "")

                    if job_data.get("created_at"):
                        job.created_at = datetime.fromisoformat(job_data["created_at"])
                    if job_data.get("completed_at"):
                        job.completed_at = datetime.fromisoformat(job_data["completed_at"])

                    self.jobs.append(job)

                logger.info(f"åŠ è½½äº† {len(self.jobs)} ä¸ªæ‰¹å¤„ç†ä»»åŠ¡çš„çŠ¶æ€")

            except Exception as e:
                logger.warning(f"åŠ è½½çŠ¶æ€æ–‡ä»¶å¤±è´¥: {e}")

    def create_jobs(self, start_from: int, end_at: int):
        """åˆ›å»ºæ‰¹å¤„ç†ä»»åŠ¡"""
        current = start_from
        job_num = 1

        while current < end_at:
            end = min(current + self.batch_size, end_at)
            job_name = f"batch_{job_num:03d}"

            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨è¯¥ä»»åŠ¡
            existing_job = next((job for job in self.jobs if job.name == job_name), None)
            if not existing_job:
                job = BatchJob(job_name, current, end)
                self.jobs.append(job)

            current = end
            job_num += 1

        self.save_status()
        logger.info(f"åˆ›å»ºäº† {len(self.jobs)} ä¸ªæ‰¹å¤„ç†ä»»åŠ¡")

    def run_safe_command(self, command_parts: List[str], timeout: int = 600) -> Tuple[bool, str]:
        """å®‰å…¨åœ°è¿è¡Œå‘½ä»¤å¹¶è®¾ç½®è¶…æ—¶"""
        logger.info(f"ğŸ–¥ï¸  æ‰§è¡Œå‘½ä»¤: {' '.join(command_parts)}")
        logger.info(f"â±ï¸  è¶…æ—¶è®¾ç½®: {timeout}ç§’")

        try:
            start_time = datetime.now()
            logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œå‘½ä»¤...")

            result = subprocess.run(
                command_parts,  # ä½¿ç”¨åˆ—è¡¨è€Œä¸æ˜¯å­—ç¬¦ä¸²ï¼Œé¿å…shellæ³¨å…¥
                capture_output=True,
                text=True,
                timeout=timeout
            )

            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            logger.info(f"â±ï¸  å‘½ä»¤æ‰§è¡Œè€—æ—¶: {duration:.1f}ç§’")
            logger.info(f"ğŸ” è¿”å›ç : {result.returncode}")

            # æ£€æŸ¥è¿”å›ç å’Œè¾“å‡ºå†…å®¹
            if result.returncode == 0:
                # å³ä½¿è¿”å›ç ä¸º0ï¼Œä¹Ÿè¦æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯ä¿¡æ¯
                combined_output = result.stdout + result.stderr
                logger.debug(f"ğŸ“¤ æ ‡å‡†è¾“å‡º: {result.stdout[:200]}...")
                logger.debug(f"ğŸ“¤ é”™è¯¯è¾“å‡º: {result.stderr[:200]}...")

                # æ£€æŸ¥ä¸¥é‡é”™è¯¯æ¨¡å¼ï¼ˆæ›´ç²¾ç¡®çš„åŒ¹é…ï¼‰
                critical_error_patterns = [
                    "è¯·æä¾›OpenAI APIå¯†é’¥",
                    "APIå¯†é’¥æœªè®¾ç½®",
                    "authentication failed",
                    "unauthorized",
                    "invalid api key",
                    "traceback",
                    "exception:",
                    "fatal error",
                    "critical error"
                ]

                # æ£€æŸ¥æ˜¯å¦æœ‰ä¸¥é‡é”™è¯¯
                has_critical_error = False
                for pattern in critical_error_patterns:
                    if pattern.lower() in combined_output.lower():
                        logger.error(f"âŒ æ£€æµ‹åˆ°ä¸¥é‡é”™è¯¯: {pattern}")
                        has_critical_error = True
                        break

                # æ£€æŸ¥ç‰¹å®šçš„å¤±è´¥æ¨¡å¼
                critical_failure_patterns = [
                    "åˆ›å»ºè¾“å…¥æ–‡ä»¶å¤±è´¥",
                    "æ— æ³•è¿æ¥åˆ°API",
                    "æ‰¹å¤„ç†æäº¤å¤±è´¥",
                    "APIå¯†é’¥æ— æ•ˆ"
                ]

                # æ£€æŸ¥å¯è·³è¿‡çš„æƒ…å†µï¼ˆæ•°æ®é—®é¢˜ï¼Œéç³»ç»Ÿé”™è¯¯ï¼‰
                skippable_patterns = [
                    "æ²¡æœ‰æœ‰æ•ˆçš„è¯·æ±‚å¯ä»¥å†™å…¥"
                ]

                has_critical_failure = False
                for pattern in critical_failure_patterns:
                    if pattern in combined_output:
                        logger.error(f"âŒ æ£€æµ‹åˆ°ä¸¥é‡å¤±è´¥: {pattern}")
                        has_critical_failure = True
                        break

                has_skippable_issue = False
                for pattern in skippable_patterns:
                    if pattern in combined_output:
                        logger.warning(f"âš ï¸  æ£€æµ‹åˆ°å¯è·³è¿‡çš„é—®é¢˜: {pattern}")
                        has_skippable_issue = True
                        break

                if has_critical_error or has_critical_failure:
                    return False, combined_output

                # å¯¹äºå¯è·³è¿‡çš„é—®é¢˜ï¼Œå¦‚æœæ²¡æœ‰å…¶ä»–ä¸¥é‡é”™è¯¯ï¼Œåˆ™è®¤ä¸ºæ˜¯æˆåŠŸï¼ˆä½†ä¼šåœ¨æ—¥å¿—ä¸­è®°å½•ï¼‰
                if has_skippable_issue:
                    logger.info(f"âœ… ä»»åŠ¡å®Œæˆï¼Œä½†å­˜åœ¨æ•°æ®é—®é¢˜ï¼ˆå·²è·³è¿‡æ— æ•ˆæ•°æ®ï¼‰")
                    return True, combined_output

                # æ£€æŸ¥æ˜¯å¦æœ‰å®é™…çš„æˆåŠŸè¾“å‡º
                success_patterns = [
                    "æ–‡ä»¶ä¸Šä¼ æˆåŠŸ",
                    "æ‰¹å¤„ç†åˆ›å»ºæˆåŠŸ",
                    "æ‰¹å¤„ç†å·²å®Œæˆ",
                    "æ‰¹å¤„ç†æˆåŠŸå®Œæˆ",
                    "æˆåŠŸåˆ›å»ºæ‰¹å¤„ç†è¾“å…¥æ–‡ä»¶",
                    "æˆåŠŸå†™å…¥"
                ]
                has_success_indicator = any(pattern in combined_output for pattern in success_patterns)

                if has_success_indicator:
                    logger.info(f"âœ… æ£€æµ‹åˆ°æˆåŠŸæ ‡è¯†")
                    return True, result.stdout
                elif len(combined_output.strip()) == 0:
                    # è¾“å‡ºä¸ºç©ºä½†è¿”å›ç ä¸º0ï¼Œè¿™æ˜¯batch_processor.pyçš„æ­£å¸¸è¡Œä¸º
                    # å®ƒä¸å‘stdoutè¾“å‡ºæ—¥å¿—ï¼Œä½†ä¼šç”Ÿæˆç»“æœæ–‡ä»¶
                    logger.info(f"âœ… å‘½ä»¤æ‰§è¡Œå®Œæˆï¼Œæ— è¾“å‡ºä½†è¿”å›ç ä¸º0ï¼ˆbatch_processor.pyæ­£å¸¸è¡Œä¸ºï¼‰")
                    return True, result.stdout
                else:
                    # å¯¹äºè¿”å›ç 0ä½†æ²¡æœ‰æ˜ç¡®æˆåŠŸæ ‡è¯†çš„æƒ…å†µï¼Œæ£€æŸ¥æ˜¯å¦æœ‰WARNINGä½†æ²¡æœ‰ERROR
                    if "WARNING" in combined_output and "ERROR" not in combined_output:
                        logger.info(f"âœ… å‘½ä»¤æ‰§è¡Œå®Œæˆï¼Œæœ‰è­¦å‘Šä½†æ— é”™è¯¯")
                        return True, result.stdout
                    else:
                        logger.info(f"âœ… å‘½ä»¤æ‰§è¡Œå®Œæˆï¼Œè¿”å›ç ä¸º0")
                        return True, result.stdout
            else:
                logger.error(f"âŒ å‘½ä»¤æ‰§è¡Œå¤±è´¥ï¼Œè¿”å›ç : {result.returncode}")
                logger.error(f"ğŸ” é”™è¯¯è¾“å‡º: {result.stderr}")
                return False, result.stderr

        except subprocess.TimeoutExpired:
            logger.error(f"â° å‘½ä»¤æ‰§è¡Œè¶…æ—¶ ({timeout}ç§’)")
            return False, f"å‘½ä»¤è¶…æ—¶ ({timeout}ç§’)"
        except Exception as e:
            logger.error(f"ğŸ’¥ å‘½ä»¤æ‰§è¡Œå¼‚å¸¸: {str(e)}")
            return False, f"å‘½ä»¤æ‰§è¡Œå¼‚å¸¸: {str(e)}"

    def process_single_job(self, job: BatchJob) -> bool:
        """å¤„ç†å•ä¸ªæ‰¹å¤„ç†ä»»åŠ¡"""
        job.attempts += 1
        job.status = "running"
        self.save_status()

        logger.info(f"ğŸ“¦ å¤„ç†ä»»åŠ¡ {job.name} (ç¬¬{job.attempts}æ¬¡å°è¯•)")
        logger.info(f"   è¡ŒèŒƒå›´: {job.start_row+1}-{job.end_row}")

        # æ­¥éª¤1: åˆ›å»ºè¾“å…¥æ–‡ä»¶
        jsonl_file = os.path.join(self.output_dir, f"{job.name}.jsonl")
        create_cmd = [
            "python", "create_safe_batch_input.py",
            self.input_csv, jsonl_file,
            "--model", self.model,
            "--start-row", str(job.start_row),
            "--end-row", str(job.end_row)
        ]

        success, output = self.run_safe_command(create_cmd, 120)
        if not success:
            job.status = "failed"
            job.error_message = f"åˆ›å»ºè¾“å…¥æ–‡ä»¶å¤±è´¥: {output}"
            self.save_status()
            logger.error(f"âŒ {job.name} åˆ›å»ºè¾“å…¥æ–‡ä»¶å¤±è´¥: {output}")
            return False

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åˆ›å»º
        if not os.path.exists(jsonl_file):
            job.status = "failed"
            job.error_message = "è¾“å…¥æ–‡ä»¶æœªåˆ›å»º"
            self.save_status()
            logger.error(f"âŒ {job.name} è¾“å…¥æ–‡ä»¶æœªåˆ›å»º")
            return False

        # æ­¥éª¤2: æäº¤æ‰¹å¤„ç†
        process_cmd = [
            "python", "batch_processor.py", jsonl_file,
            "--output-dir", self.output_dir,
            "--check-interval", "30"
        ]

        success, output = self.run_safe_command(process_cmd, 600)
        if success:
            # è¯¦ç»†è®°å½•batch_processor.pyçš„è¾“å‡º
            logger.info(f"ğŸ“‹ batch_processor.py è¾“å‡ºè¯¦æƒ…:")
            logger.info(f"   è¾“å‡ºé•¿åº¦: {len(output)} å­—ç¬¦")

            # æå–batch ID
            import re
            batch_ids = re.findall(r'batch_[a-f0-9]{32}', output)
            current_batch_id = batch_ids[0] if batch_ids else None

            if "æ‰¹å¤„ç†æˆåŠŸå®Œæˆ" in output:
                logger.info(f"   âœ… æ£€æµ‹åˆ°æˆåŠŸå®Œæˆæ ‡è¯†")
                if current_batch_id:
                    logger.info(f"   ğŸ†” æ‰¹å¤„ç†ID: {current_batch_id}")
                    job.batch_id = current_batch_id
            elif "æ‰¹å¤„ç†å¤±è´¥" in output:
                logger.warning(f"   âŒ æ£€æµ‹åˆ°å¤±è´¥æ ‡è¯†")
                if current_batch_id:
                    logger.warning(f"   ğŸ†” å¤±è´¥çš„æ‰¹å¤„ç†ID: {current_batch_id}")
                    job.batch_id = current_batch_id
                    # è‡ªåŠ¨è¿è¡Œè°ƒè¯•åˆ†æ
                    self._run_batch_debug_analysis(current_batch_id)
            elif current_batch_id:
                logger.info(f"   ğŸ†” æ£€æµ‹åˆ°æ‰¹å¤„ç†ID: {current_batch_id}")
                job.batch_id = current_batch_id

            # éªŒè¯ç»“æœæ–‡ä»¶æ˜¯å¦çœŸçš„å­˜åœ¨
            result_files = glob.glob(os.path.join(self.output_dir, f"batch_results_*{job.name}*.jsonl"))
            if not result_files:
                # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•æ–°çš„ç»“æœæ–‡ä»¶
                all_result_files = glob.glob(os.path.join(self.output_dir, "batch_results_*.jsonl"))
                logger.warning(f"âš ï¸  {job.name} å‘½ä»¤æˆåŠŸä½†æœªæ‰¾åˆ°ä¸“ç”¨ç»“æœæ–‡ä»¶")
                logger.info(f"ğŸ“ å½“å‰ç»“æœæ–‡ä»¶æ•°é‡: {len(all_result_files)}")

                # åˆ†æå¯èƒ½çš„åŸå› 
                logger.info(f"ğŸ” åˆ†æå¯èƒ½çš„åŸå› :")
                if "æ‰¹å¤„ç†å¤±è´¥" in output:
                    logger.info(f"   - batch_processor.py æŠ¥å‘Šæ‰¹å¤„ç†å¤±è´¥")
                elif "batch_" not in output:
                    logger.info(f"   - è¾“å‡ºä¸­æœªæ‰¾åˆ°æ‰¹å¤„ç†IDï¼Œå¯èƒ½æäº¤å¤±è´¥")
                elif len(output) < 100:
                    logger.info(f"   - è¾“å‡ºè¿‡çŸ­ï¼Œå¯èƒ½æ‰§è¡Œå¼‚å¸¸")
                else:
                    logger.info(f"   - å¯èƒ½æ˜¯OpenAI APIç«¯é—®é¢˜æˆ–æ–‡ä»¶å‘½åé—®é¢˜")

                # æ£€æŸ¥æ˜¯å¦æœ‰æ–°ç”Ÿæˆçš„ç»“æœæ–‡ä»¶
                if all_result_files:
                    # æ£€æŸ¥æœ€æ–°çš„ç»“æœæ–‡ä»¶æ˜¯å¦åŒ…å«å½“å‰æ‰¹æ¬¡çš„æ•°æ®
                    latest_file = max(all_result_files, key=os.path.getmtime)
                    logger.info(f"ğŸ“„ æ£€æŸ¥æœ€æ–°ç»“æœæ–‡ä»¶: {os.path.basename(latest_file)}")

                    # ç®€å•æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åŒ…å«å½“å‰æ‰¹æ¬¡çš„è¡Œå·
                    try:
                        with open(latest_file, 'r') as f:
                            content = f.read(1000)  # è¯»å–å‰1000å­—ç¬¦
                            expected_rows = [f"row_{i}" for i in range(job.start_row, min(job.start_row + 5, job.end_row))]
                            found_rows = [row for row in expected_rows if row in content]

                            if found_rows:
                                logger.info(f"âœ… åœ¨æœ€æ–°æ–‡ä»¶ä¸­æ‰¾åˆ°å½“å‰æ‰¹æ¬¡æ•°æ®: {found_rows}")
                                job.status = "completed"
                                job.completed_at = datetime.now()
                                job.error_message = ""
                            else:
                                logger.error(f"âŒ æœ€æ–°æ–‡ä»¶ä¸­æœªæ‰¾åˆ°å½“å‰æ‰¹æ¬¡æ•°æ® (æœŸæœ›: {expected_rows[:3]}...)")
                                job.status = "failed"
                                job.error_message = "ç»“æœæ–‡ä»¶ä¸­æœªæ‰¾åˆ°å¯¹åº”æ•°æ®"
                                self.save_status()
                                return False
                    except Exception as e:
                        logger.error(f"âŒ æ£€æŸ¥ç»“æœæ–‡ä»¶å¤±è´¥: {e}")
                        job.status = "failed"
                        job.error_message = f"æ— æ³•éªŒè¯ç»“æœæ–‡ä»¶: {e}"
                        self.save_status()
                        return False
                else:
                    logger.error(f"âŒ {job.name} æœªç”Ÿæˆä»»ä½•ç»“æœæ–‡ä»¶")
                    job.status = "failed"
                    job.error_message = "æœªç”Ÿæˆç»“æœæ–‡ä»¶"
                    self.save_status()
                    return False
            else:
                logger.info(f"âœ… æ‰¾åˆ°ç»“æœæ–‡ä»¶: {[os.path.basename(f) for f in result_files]}")
                job.status = "completed"
                job.completed_at = datetime.now()
                job.error_message = ""

            # è®°å½•æˆæœ¬ï¼ˆä¼°ç®—ï¼‰
            num_requests = job.end_row - job.start_row
            cost_estimate = self.cost_tracker.estimate_batch_cost(num_requests)
            cost_estimate["actual_completed"] = num_requests
            self.cost_tracker.record_batch_cost(job.name, job.batch_id, cost_estimate)

            self.save_status()
            logger.info(f"âœ… {job.name} å®Œæˆ (ä¼°ç®—æˆæœ¬: ${cost_estimate['batch_cost']:.4f})")
            return True
        else:
            if "è¶…æ—¶" in output:
                job.status = "timeout"
                job.error_message = f"å¤„ç†è¶…æ—¶: {output}"
            else:
                job.status = "failed"
                # è¿è¡Œè¯¦ç»†çš„å¤±è´¥åˆ†æ
                self._analyze_batch_failure(job, output)

            self.save_status()
            logger.error(f"âŒ {job.name} å¤±è´¥: {output}")
            return False

    def process_all_jobs(self, retry_failed: bool = True):
        """å¤„ç†æ‰€æœ‰æ‰¹å¤„ç†ä»»åŠ¡"""
        logger.info("=" * 80)
        logger.info(f"å¼€å§‹å¤„ç† {len(self.jobs)} ä¸ªæ‰¹å¤„ç†ä»»åŠ¡")
        logger.info("=" * 80)

        for i, job in enumerate(self.jobs, 1):
            # è·³è¿‡å·²å®Œæˆçš„ä»»åŠ¡
            if job.status == "completed":
                logger.info(f"â­ï¸  è·³è¿‡å·²å®Œæˆçš„ä»»åŠ¡ {job.name}")
                continue

            # æ£€æŸ¥é‡è¯•æ¬¡æ•°
            if job.attempts >= job.max_attempts:
                logger.warning(f"âš ï¸  ä»»åŠ¡ {job.name} å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
                continue

            logger.info(f"\nå¤„ç†ä»»åŠ¡ {i}/{len(self.jobs)}: {job.name}")

            success = self.process_single_job(job)

            # åœ¨ä»»åŠ¡ä¹‹é—´æ·»åŠ å»¶è¿Ÿ
            if i < len(self.jobs):
                delay = self.batch_interval if success else self.batch_interval * 2  # å¤±è´¥åç­‰å¾…æ›´é•¿æ—¶é—´
                logger.info(f"ç­‰å¾… {delay} ç§’åå¤„ç†ä¸‹ä¸€ä¸ªä»»åŠ¡...")
                time.sleep(delay)

        self.print_summary()

    def retry_failed_jobs(self):
        """é‡è¯•å¤±è´¥çš„ä»»åŠ¡"""
        failed_jobs = [job for job in self.jobs if job.status in ["failed", "timeout"] and job.attempts < job.max_attempts]

        if not failed_jobs:
            logger.info("æ²¡æœ‰éœ€è¦é‡è¯•çš„å¤±è´¥ä»»åŠ¡")
            return

        logger.info(f"ğŸ”„ é‡è¯• {len(failed_jobs)} ä¸ªå¤±è´¥çš„ä»»åŠ¡")

        for job in failed_jobs:
            logger.info(f"\né‡è¯•ä»»åŠ¡: {job.name}")
            self.process_single_job(job)
            time.sleep(60)  # é‡è¯•é—´éš”æ›´é•¿

        self.print_summary()

    def print_summary(self):
        """æ‰“å°å¤„ç†æ€»ç»“"""
        completed = [job for job in self.jobs if job.status == "completed"]
        failed = [job for job in self.jobs if job.status in ["failed", "timeout"]]
        pending = [job for job in self.jobs if job.status == "pending"]

        logger.info("=" * 80)
        logger.info("æ‰¹å¤„ç†æ€»ç»“")
        logger.info("=" * 80)
        logger.info(f"æ€»ä»»åŠ¡æ•°: {len(self.jobs)}")
        logger.info(f"å·²å®Œæˆ: {len(completed)}")
        logger.info(f"å¤±è´¥/è¶…æ—¶: {len(failed)}")
        logger.info(f"å¾…å¤„ç†: {len(pending)}")
        logger.info(f"å®Œæˆç‡: {len(completed)/len(self.jobs)*100:.1f}%")

        if failed:
            logger.warning("å¤±è´¥çš„ä»»åŠ¡:")
            for job in failed:
                logger.warning(f"  {job.name} (ç¬¬{job.start_row+1}-{job.end_row}è¡Œ): {job.error_message}")

        if completed:
            logger.info("âœ… æœ‰å·²å®Œæˆçš„ä»»åŠ¡ï¼Œå¯ä»¥è¿›è¡Œç»“æœåˆå¹¶")

            # è¯¢é—®æ˜¯å¦è‡ªåŠ¨åˆå¹¶
            try:
                response = input("\nğŸ¤” æ˜¯å¦è‡ªåŠ¨åˆå¹¶ç»“æœï¼Ÿ(Y/n): ").strip().lower()
                if response in ['', 'y', 'yes']:
                    self.smart_merge_results()
                else:
                    logger.info("ğŸ’¡ æ‰‹åŠ¨åˆå¹¶å‘½ä»¤:")
                    logger.info(f"python merge_all_results.py {self.output_dir} {self.input_csv} final_output.csv")
            except KeyboardInterrupt:
                logger.info("\nğŸ’¡ æ‰‹åŠ¨åˆå¹¶å‘½ä»¤:")
                logger.info(f"python merge_all_results.py {self.output_dir} {self.input_csv} final_output.csv")

            # æ˜¾ç¤ºæˆæœ¬æ€»ç»“
            logger.info("\n" + "=" * 80)
            self.cost_tracker.print_cost_report()

    def smart_merge_results(self) -> bool:
        """æ™ºèƒ½åˆå¹¶æ‰¹å¤„ç†ç»“æœ"""
        logger.info("ğŸ”— å¼€å§‹æ™ºèƒ½åˆå¹¶æ‰¹å¤„ç†ç»“æœ...")

        try:
            # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å
            csv_basename = os.path.splitext(os.path.basename(self.input_csv))[0]
            output_file = os.path.join(self.output_base_dir, f"final_output_{csv_basename}.csv")

            # æŸ¥æ‰¾æ‰€æœ‰ç»“æœæ–‡ä»¶
            result_files = self._find_result_files()
            if not result_files:
                logger.error("âŒ æœªæ‰¾åˆ°ä»»ä½•ç»“æœæ–‡ä»¶")
                return False

            logger.info(f"ğŸ“„ æ‰¾åˆ° {len(result_files)} ä¸ªç»“æœæ–‡ä»¶")

            # è¯»å–åŸå§‹CSV
            logger.info(f"ğŸ“– è¯»å–åŸå§‹CSV: {self.input_csv}")
            df_original = pd.read_csv(self.input_csv)
            total_rows = len(df_original)

            # è§£ææ‰€æœ‰ç»“æœ
            all_results = []
            for file_path in result_files:
                file_results = self._parse_result_file(file_path)
                all_results.extend(file_results)

            logger.info(f"ğŸ“Š æ€»å…±è§£æå‡º {len(all_results)} æ¡ç»“æœ")

            # åˆ›å»ºè¡Œå·åˆ°ç»“æœçš„æ˜ å°„
            row_to_result = {}
            duplicate_count = 0

            for result in all_results:
                row_num = result['row_number']
                if row_num is not None:
                    if row_num in row_to_result:
                        duplicate_count += 1
                        logger.debug(f"ğŸ”„ å‘ç°é‡å¤è¡Œ {row_num}ï¼Œä¿ç•™æœ€æ–°ç»“æœ")
                    row_to_result[row_num] = result

            logger.info(f"ğŸ“Š å»é‡åæœ‰æ•ˆç»“æœ: {len(row_to_result)} æ¡")
            if duplicate_count > 0:
                logger.info(f"ğŸ”„ å»é™¤é‡å¤ç»“æœ: {duplicate_count} æ¡")

            # åˆ›å»ºæœ€ç»ˆè¾“å‡º
            df_output = df_original.copy()
            df_output['AI_Response'] = ''
            df_output['Processing_Status'] = 'Missing'
            df_output['Source_File'] = ''

            # å¡«å…¥AIå“åº”
            matched_count = 0
            for row_num, result in row_to_result.items():
                # è½¬æ¢ä¸º0-indexed
                idx = row_num - 1

                if 0 <= idx < len(df_output):
                    df_output.loc[idx, 'AI_Response'] = result['response_content']
                    df_output.loc[idx, 'Processing_Status'] = 'Completed'
                    df_output.loc[idx, 'Source_File'] = result['file_source']
                    matched_count += 1
                else:
                    logger.warning(f"âš ï¸  è¡Œå· {row_num} è¶…å‡ºCSVèŒƒå›´")

            # ç»Ÿè®¡å’Œä¿å­˜
            completed_rows = len(df_output[df_output['Processing_Status'] == 'Completed'])
            missing_rows = len(df_output[df_output['Processing_Status'] == 'Missing'])
            completion_rate = (completed_rows / total_rows) * 100

            logger.info(f"âœ… æˆåŠŸåŒ¹é…: {matched_count} è¡Œ")
            logger.info(f"ğŸ“ˆ å®Œæˆç»Ÿè®¡:")
            logger.info(f"   - æ€»è¡Œæ•°: {total_rows}")
            logger.info(f"   - å·²å®Œæˆ: {completed_rows}")
            logger.info(f"   - ç¼ºå¤±: {missing_rows}")
            logger.info(f"   - å®Œæˆç‡: {completion_rate:.1f}%")

            # ä¿å­˜ç»“æœ
            df_output.to_csv(output_file, index=False, encoding='utf-8')
            file_size = os.path.getsize(output_file) / 1024 / 1024  # MB
            logger.info(f"ğŸ’¾ ç»“æœå·²ä¿å­˜: {output_file}")
            logger.info(f"ğŸ“„ æ–‡ä»¶å¤§å°: {file_size:.2f} MB")

            # ç”Ÿæˆç¼ºå¤±è¡ŒæŠ¥å‘Š
            if missing_rows > 0:
                missing_file = output_file.replace('.csv', '_missing_rows.txt')
                missing_indices = df_output[df_output['Processing_Status'] == 'Missing'].index + 1

                with open(missing_file, 'w') as f:
                    for idx in missing_indices:
                        f.write(f"{idx}\n")

                logger.warning(f"âš ï¸  ç¼ºå¤±è¡Œåˆ—è¡¨å·²ä¿å­˜: {missing_file}")

            return True

        except Exception as e:
            logger.error(f"âŒ åˆå¹¶è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            logger.error(f"ğŸ” é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return False

    def _run_batch_debug_analysis(self, batch_id: str):
        """è¿è¡Œæ‰¹å¤„ç†è°ƒè¯•åˆ†æ"""
        try:
            logger.info(f"ğŸ” å¼€å§‹è°ƒè¯•åˆ†ææ‰¹å¤„ç†: {batch_id}")

            debug_cmd = [
                "python", "enhanced_batch_debugger.py", batch_id,
                "--output-dir", self.output_dir
            ]

            success, output = self.run_safe_command(debug_cmd, 120)
            if success:
                logger.info(f"âœ… è°ƒè¯•åˆ†æå®Œæˆ")
                logger.info(f"ğŸ“‹ è°ƒè¯•è¾“å‡º: {output}")
            else:
                logger.warning(f"âš ï¸  è°ƒè¯•åˆ†æå¤±è´¥: {output}")

        except Exception as e:
            logger.warning(f"âš ï¸  è°ƒè¯•åˆ†æå¼‚å¸¸: {e}")

    def _analyze_batch_failure(self, job: BatchJob, output: str):
        """åˆ†ææ‰¹å¤„ç†å¤±è´¥çš„åŸå› """
        logger.error(f"ğŸ” åˆ†æ {job.name} å¤±è´¥åŸå› :")

        # åˆ†æè¾“å‡ºä¸­çš„é”™è¯¯ä¿¡æ¯
        if "quota" in output.lower():
            logger.error(f"   ğŸ’° å¯èƒ½åŸå› : APIé…é¢ä¸è¶³")
            job.error_message = "APIé…é¢ä¸è¶³"
        elif "rate" in output.lower() and "limit" in output.lower():
            logger.error(f"   ğŸš¦ å¯èƒ½åŸå› : é€Ÿç‡é™åˆ¶")
            job.error_message = "é€Ÿç‡é™åˆ¶"
        elif "api" in output.lower() and "key" in output.lower():
            logger.error(f"   ğŸ”‘ å¯èƒ½åŸå› : APIå¯†é’¥é—®é¢˜")
            job.error_message = "APIå¯†é’¥é—®é¢˜"
        elif "timeout" in output.lower():
            logger.error(f"   â° å¯èƒ½åŸå› : è¯·æ±‚è¶…æ—¶")
            job.error_message = "è¯·æ±‚è¶…æ—¶"
        elif "validation" in output.lower():
            logger.error(f"   ğŸ“ å¯èƒ½åŸå› : è¾“å…¥éªŒè¯å¤±è´¥")
            job.error_message = "è¾“å…¥éªŒè¯å¤±è´¥"
        else:
            logger.error(f"   â“ æœªçŸ¥é”™è¯¯ï¼Œéœ€è¦è¯¦ç»†åˆ†æ")
            job.error_message = f"æœªçŸ¥é”™è¯¯: {output[:200]}"

        # å¦‚æœæœ‰batch IDï¼Œè¿è¡Œè¯¦ç»†åˆ†æ
        if hasattr(job, 'batch_id') and job.batch_id:
            self._run_batch_debug_analysis(job.batch_id)

    def _find_result_files(self) -> List[str]:
        """æŸ¥æ‰¾æ‰€æœ‰æ‰¹å¤„ç†ç»“æœæ–‡ä»¶"""
        result_pattern = os.path.join(self.output_dir, "batch_results_*.jsonl")
        result_files = glob.glob(result_pattern)
        return sorted(result_files)

    def _parse_result_file(self, file_path: str) -> List[Dict]:
        """è§£æå•ä¸ªç»“æœæ–‡ä»¶"""
        results = []

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if not line:
                        continue

                    try:
                        data = json.loads(line)

                        # æå–å…³é”®ä¿¡æ¯
                        custom_id = data.get('custom_id', '')
                        response = data.get('response', {})

                        if response and response.get('body'):
                            choices = response['body'].get('choices', [])
                            if choices:
                                content = choices[0].get('message', {}).get('content', '')

                                # æå–è¡Œå·
                                row_num = None
                                if custom_id.startswith('request-'):
                                    try:
                                        row_num = int(custom_id.split('-')[1])
                                    except:
                                        pass

                                results.append({
                                    'row_number': row_num,
                                    'custom_id': custom_id,
                                    'response_content': content,
                                    'file_source': os.path.basename(file_path)
                                })

                    except json.JSONDecodeError as e:
                        logger.warning(f"âš ï¸  {os.path.basename(file_path)} ç¬¬{line_num}è¡ŒJSONè§£æå¤±è´¥: {e}")
                        continue

        except Exception as e:
            logger.error(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return []

        logger.debug(f"ğŸ“Š {os.path.basename(file_path)}: è§£æå‡º {len(results)} æ¡ç»“æœ")
        return results

def main():
    parser = argparse.ArgumentParser(description="å¥å£®çš„æ‰¹å¤„ç†å™¨ - å¢å¼ºç‰ˆ")
    parser.add_argument("--input-csv", help="è¾“å…¥CSVæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œæ”¯æŒäº¤äº’å¼é€‰æ‹©ï¼‰")
    parser.add_argument("--start-row", type=int, default=0, help="èµ·å§‹è¡Œ")
    parser.add_argument("--end-row", type=int, default=None, help="ç»“æŸè¡Œï¼Œä¸æŒ‡å®šåˆ™ä½¿ç”¨æ–‡ä»¶æ€»è¡Œæ•°")
    parser.add_argument("--batch-size", type=int, default=50, help="æ‰¹æ¬¡å¤§å°ï¼ˆæ¨è50-100ä»¥å‡å°‘APIè°ƒç”¨é¢‘ç‡ï¼‰")
    parser.add_argument("--model", default="gpt-4o-mini", help="ä½¿ç”¨çš„æ¨¡å‹ï¼ˆæ¨ègpt-4o-miniç”¨äºæ‰¹å¤„ç†ï¼‰")
    parser.add_argument("--batch-interval", type=int, default=120, help="æ‰¹æ¬¡é—´ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤120ç§’")
    parser.add_argument("--interactive", action="store_true", help="å¼ºåˆ¶ä½¿ç”¨äº¤äº’å¼é€‰æ‹©")
    args = parser.parse_args()

    logger.info("ğŸš€ å¥å£®çš„æ‰¹å¤„ç†å™¨ - å¢å¼ºç‰ˆ")
    logger.info("="*80)

    # é€‰æ‹©CSVæ–‡ä»¶
    input_csv = None

    if args.interactive or not args.input_csv:
        # äº¤äº’å¼é€‰æ‹©
        input_csv = interactive_csv_selection()
        if not input_csv:
            logger.error("âŒ æœªé€‰æ‹©CSVæ–‡ä»¶ï¼Œé€€å‡º")
            return 1
    else:
        # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°
        input_csv = args.input_csv
        if not os.path.exists(input_csv):
            logger.error(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_csv}")
            logger.info("ğŸ’¡ ä½¿ç”¨ --interactive å‚æ•°è¿›è¡Œäº¤äº’å¼é€‰æ‹©")
            return 1

    # æ˜¾ç¤ºé€‰æ‹©çš„æ–‡ä»¶ä¿¡æ¯
    logger.info(f"ğŸ“Š é€‰æ‹©çš„CSVæ–‡ä»¶: {input_csv}")
    total_lines = get_csv_line_count(input_csv)
    file_size = os.path.getsize(input_csv) / 1024 / 1024  # MB
    logger.info(f"ğŸ“ˆ æ–‡ä»¶ç»Ÿè®¡: {total_lines} è¡Œæ•°æ®, {file_size:.2f} MB")

    # å¦‚æœæœªæŒ‡å®šç»“æŸè¡Œï¼Œä½¿ç”¨æ–‡ä»¶æ€»è¡Œæ•°
    if args.end_row is None:
        args.end_row = total_lines
        logger.info(f"ğŸ“Š å°†å¤„ç†å…¨éƒ¨ {args.end_row} è¡Œæ•°æ®")
    else:
        logger.info(f"ğŸ“Š å°†å¤„ç†ç¬¬ {args.start_row+1} åˆ° {args.end_row} è¡Œ")

    # æ˜¾ç¤ºå¤„ç†è®¡åˆ’
    total_batches = (args.end_row - args.start_row + args.batch_size - 1) // args.batch_size
    estimated_cost = total_batches * args.batch_size * 0.003  # ç²—ç•¥ä¼°ç®—

    logger.info("ğŸ“‹ å¤„ç†è®¡åˆ’:")
    logger.info(f"   - æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    logger.info(f"   - é¢„è®¡æ‰¹æ¬¡æ•°: {total_batches}")
    logger.info(f"   - ä½¿ç”¨æ¨¡å‹: {args.model}")
    logger.info(f"   - ä¼°ç®—æˆæœ¬: ${estimated_cost:.2f}")

    # ç¡®è®¤æ‰§è¡Œ
    try:
        response = input("\nğŸ¤” ç¡®è®¤å¼€å§‹å¤„ç†ï¼Ÿ(Y/n): ").strip().lower()
        if response not in ['', 'y', 'yes']:
            logger.info("ğŸ›‘ ç”¨æˆ·å–æ¶ˆå¤„ç†")
            return 0
    except KeyboardInterrupt:
        logger.info("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­")
        return 0

    # åˆ›å»ºå¤„ç†å™¨
    processor = RobustBatchProcessor(input_csv, args.batch_size, args.model, args.batch_interval)

    # åˆ›å»ºä»»åŠ¡ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
    if not processor.jobs:
        processor.create_jobs(args.start_row, args.end_row)

    # å¤„ç†æ‰€æœ‰ä»»åŠ¡
    processor.process_all_jobs()

    # è¯¢é—®æ˜¯å¦é‡è¯•å¤±è´¥çš„ä»»åŠ¡
    failed_jobs = [job for job in processor.jobs if job.status in ["failed", "timeout"] and job.attempts < job.max_attempts]
    if failed_jobs:
        try:
            response = input(f"\nğŸ”„ å‘ç° {len(failed_jobs)} ä¸ªå¤±è´¥ä»»åŠ¡ï¼Œæ˜¯å¦é‡è¯•ï¼Ÿ(y/N): ")
            if response.lower() == 'y':
                processor.retry_failed_jobs()
        except KeyboardInterrupt:
            logger.info("\nğŸ›‘ è·³è¿‡é‡è¯•")

    logger.info("ğŸ‰ æ‰¹å¤„ç†å™¨æ‰§è¡Œå®Œæˆ")
    return 0

if __name__ == "__main__":
    import sys
    sys.exit(main())
