#!/usr/bin/env python3
"""
æ™ºèƒ½ç»­ä¼ æ‰¹å¤„ç†è„šæœ¬
è·³è¿‡å·²å®Œæˆçš„è¡Œï¼Œåªå¤„ç†å‰©ä½™æœªå®Œæˆçš„è¡Œ
"""

import os
import json
import pandas as pd
import argparse
import logging
from datetime import datetime
from typing import List, Set
import subprocess

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class BatchResumer:
    def __init__(self, csv_file: str, completed_output_file: str = None, missing_rows_file: str = None):
        """
        åˆå§‹åŒ–ç»­ä¼ å¤„ç†å™¨
        
        Args:
            csv_file: åŸå§‹CSVæ–‡ä»¶è·¯å¾„
            completed_output_file: å·²å®Œæˆçš„è¾“å‡ºæ–‡ä»¶è·¯å¾„
            missing_rows_file: ç¼ºå¤±è¡Œåˆ—è¡¨æ–‡ä»¶è·¯å¾„
        """
        self.csv_file = csv_file
        self.completed_output_file = completed_output_file
        self.missing_rows_file = missing_rows_file
        
    def analyze_completion_status(self) -> dict:
        """åˆ†æå®ŒæˆçŠ¶æ€"""
        logger.info("ğŸ” åˆ†æå½“å‰å®ŒæˆçŠ¶æ€...")
        
        # è¯»å–åŸå§‹CSV
        df_original = pd.read_csv(self.csv_file)
        total_rows = len(df_original)
        
        completed_rows = set()
        missing_rows = set()
        
        # æ–¹æ³•1: ä»å·²å®Œæˆçš„è¾“å‡ºæ–‡ä»¶ä¸­è¯»å–
        if self.completed_output_file and os.path.exists(self.completed_output_file):
            logger.info(f"ğŸ“„ ä»è¾“å‡ºæ–‡ä»¶åˆ†æ: {self.completed_output_file}")
            df_completed = pd.read_csv(self.completed_output_file)
            
            # æ‰¾å‡ºå·²å®Œæˆçš„è¡Œï¼ˆæœ‰AI_Responseä¸”ä¸ä¸ºç©ºï¼‰
            completed_mask = (df_completed['Processing_Status'] == 'Completed') & \
                           (df_completed['AI_Response'].notna()) & \
                           (df_completed['AI_Response'] != '')
            
            completed_indices = df_completed[completed_mask].index.tolist()
            completed_rows = set(idx + 1 for idx in completed_indices)  # è½¬æ¢ä¸º1-based
            
            missing_indices = df_completed[~completed_mask].index.tolist()
            missing_rows = set(idx + 1 for idx in missing_indices)  # è½¬æ¢ä¸º1-based
        
        # æ–¹æ³•2: ä»ç¼ºå¤±è¡Œæ–‡ä»¶ä¸­è¯»å–
        elif self.missing_rows_file and os.path.exists(self.missing_rows_file):
            logger.info(f"ğŸ“„ ä»ç¼ºå¤±è¡Œæ–‡ä»¶åˆ†æ: {self.missing_rows_file}")
            with open(self.missing_rows_file, 'r') as f:
                missing_rows = set(int(line.strip()) for line in f if line.strip())
            
            completed_rows = set(range(1, total_rows + 1)) - missing_rows
        
        else:
            logger.warning("âš ï¸  æœªæ‰¾åˆ°å®ŒæˆçŠ¶æ€æ–‡ä»¶ï¼Œå°†å¤„ç†æ‰€æœ‰è¡Œ")
            missing_rows = set(range(1, total_rows + 1))
            completed_rows = set()
        
        analysis = {
            'total_rows': total_rows,
            'completed_rows': len(completed_rows),
            'missing_rows': len(missing_rows),
            'completion_rate': len(completed_rows) / total_rows * 100,
            'completed_row_numbers': sorted(completed_rows),
            'missing_row_numbers': sorted(missing_rows)
        }
        
        logger.info(f"ğŸ“Š å®ŒæˆçŠ¶æ€åˆ†æ:")
        logger.info(f"   æ€»è¡Œæ•°: {analysis['total_rows']}")
        logger.info(f"   å·²å®Œæˆ: {analysis['completed_rows']}")
        logger.info(f"   å¾…å¤„ç†: {analysis['missing_rows']}")
        logger.info(f"   å®Œæˆç‡: {analysis['completion_rate']:.1f}%")
        
        return analysis
    
    def create_resume_csv(self, missing_row_numbers: List[int], output_file: str) -> str:
        """åˆ›å»ºç»­ä¼ ç”¨çš„CSVæ–‡ä»¶ï¼ŒåªåŒ…å«æœªå®Œæˆçš„è¡Œ"""
        logger.info(f"ğŸ“ åˆ›å»ºç»­ä¼ CSVæ–‡ä»¶: {output_file}")
        
        # è¯»å–åŸå§‹CSV
        df_original = pd.read_csv(self.csv_file)
        
        # æå–æœªå®Œæˆçš„è¡Œï¼ˆè½¬æ¢ä¸º0-basedç´¢å¼•ï¼‰
        missing_indices = [row_num - 1 for row_num in missing_row_numbers]
        df_resume = df_original.iloc[missing_indices].copy()
        
        # é‡ç½®ç´¢å¼•ä½†ä¿ç•™åŸå§‹è¡Œå·ä¿¡æ¯
        df_resume['Original_Row_Number'] = missing_row_numbers
        df_resume.reset_index(drop=True, inplace=True)
        
        # ä¿å­˜ç»­ä¼ CSV
        df_resume.to_csv(output_file, index=False)
        
        logger.info(f"âœ… ç»­ä¼ CSVå·²åˆ›å»º: {output_file}")
        logger.info(f"   åŒ…å« {len(df_resume)} è¡Œå¾…å¤„ç†æ•°æ®")
        
        return output_file
    
    def calculate_batch_plan(self, total_missing: int, batch_size: int = 20) -> List[dict]:
        """è®¡ç®—æ‰¹å¤„ç†è®¡åˆ’"""
        logger.info(f"ğŸ“‹ è®¡ç®—æ‰¹å¤„ç†è®¡åˆ’ (æ¯æ‰¹ {batch_size} è¡Œ)...")
        
        batches = []
        for i in range(0, total_missing, batch_size):
            start_row = i
            end_row = min(i + batch_size - 1, total_missing - 1)
            
            batch_info = {
                'batch_name': f"resume_batch_{i//batch_size + 1:03d}",
                'start_row': start_row,
                'end_row': end_row,
                'row_count': end_row - start_row + 1
            }
            batches.append(batch_info)
        
        logger.info(f"ğŸ“Š æ‰¹å¤„ç†è®¡åˆ’:")
        logger.info(f"   æ€»æ‰¹æ¬¡æ•°: {len(batches)}")
        logger.info(f"   æ¯æ‰¹å¤§å°: {batch_size}")
        logger.info(f"   æœ€åä¸€æ‰¹: {batches[-1]['row_count']} è¡Œ")
        
        return batches
    
    def start_resume_processing(self, model: str = "gpt-4o-mini", batch_size: int = 20, 
                              delay_between_batches: int = 120) -> str:
        """å¼€å§‹ç»­ä¼ å¤„ç†"""
        logger.info("ğŸš€ å¼€å§‹æ™ºèƒ½ç»­ä¼ æ‰¹å¤„ç†...")
        
        # 1. åˆ†æå®ŒæˆçŠ¶æ€
        analysis = self.analyze_completion_status()
        
        if analysis['missing_rows'] == 0:
            logger.info("ğŸ‰ æ‰€æœ‰è¡Œéƒ½å·²å®Œæˆï¼Œæ— éœ€ç»­ä¼ ï¼")
            return None
        
        # 2. åˆ›å»ºç»­ä¼ CSV
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        resume_csv = f"resume_data_{timestamp}.csv"
        self.create_resume_csv(analysis['missing_row_numbers'], resume_csv)
        
        # 3. åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = f"output/resume_batch_results_{timestamp}"
        os.makedirs(output_dir, exist_ok=True)
        
        # 4. è®¡ç®—æ‰¹å¤„ç†è®¡åˆ’
        batches = self.calculate_batch_plan(analysis['missing_rows'], batch_size)
        
        # 5. å¯åŠ¨æ‰¹å¤„ç†
        logger.info(f"ğŸ¯ å¼€å§‹å¤„ç† {analysis['missing_rows']} è¡Œæ•°æ®...")
        
        cmd = [
            "python", "robust_batch_processor.py", 
            resume_csv,
            "--model", model,
            "--batch-size", str(batch_size),
            "--delay", str(delay_between_batches),
            "--output-dir", output_dir
        ]
        
        logger.info(f"ğŸ–¥ï¸  æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
        
        try:
            # å¯åŠ¨æ‰¹å¤„ç†ï¼ˆéé˜»å¡ï¼‰
            process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
            logger.info(f"âœ… ç»­ä¼ æ‰¹å¤„ç†å·²å¯åŠ¨ (PID: {process.pid})")
            logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
            logger.info(f"ğŸ“„ ç»­ä¼ CSV: {resume_csv}")
            
            return output_dir
            
        except Exception as e:
            logger.error(f"âŒ å¯åŠ¨ç»­ä¼ å¤„ç†å¤±è´¥: {e}")
            return None

def main():
    parser = argparse.ArgumentParser(description="æ™ºèƒ½ç»­ä¼ æ‰¹å¤„ç†è„šæœ¬")
    parser.add_argument("csv_file", help="åŸå§‹CSVæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--completed-output", help="å·²å®Œæˆçš„è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--missing-rows-file", help="ç¼ºå¤±è¡Œåˆ—è¡¨æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--model", default="gpt-4o-mini", help="ä½¿ç”¨çš„æ¨¡å‹ï¼Œé»˜è®¤gpt-4o-mini")
    parser.add_argument("--batch-size", type=int, default=20, help="æ¯æ‰¹å¤„ç†çš„è¡Œæ•°ï¼Œé»˜è®¤20")
    parser.add_argument("--delay", type=int, default=120, help="æ‰¹æ¬¡é—´å»¶è¿Ÿç§’æ•°ï¼Œé»˜è®¤120")
    parser.add_argument("--analyze-only", action="store_true", help="ä»…åˆ†æå®ŒæˆçŠ¶æ€ï¼Œä¸å¯åŠ¨å¤„ç†")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.csv_file):
        logger.error(f"âŒ CSVæ–‡ä»¶ä¸å­˜åœ¨: {args.csv_file}")
        return 1
    
    # åˆ›å»ºç»­ä¼ å¤„ç†å™¨
    resumer = BatchResumer(
        csv_file=args.csv_file,
        completed_output_file=args.completed_output,
        missing_rows_file=args.missing_rows_file
    )
    
    if args.analyze_only:
        # ä»…åˆ†æ
        analysis = resumer.analyze_completion_status()
        
        print("\n" + "="*80)
        print("ğŸ“Š å®ŒæˆçŠ¶æ€åˆ†ææŠ¥å‘Š")
        print("="*80)
        print(f"ğŸ“„ åŸå§‹æ–‡ä»¶: {args.csv_file}")
        print(f"ğŸ“Š æ€»è¡Œæ•°: {analysis['total_rows']}")
        print(f"âœ… å·²å®Œæˆ: {analysis['completed_rows']} ({analysis['completion_rate']:.1f}%)")
        print(f"â³ å¾…å¤„ç†: {analysis['missing_rows']}")
        
        if analysis['missing_rows'] > 0:
            print(f"\nğŸ’¡ ç»­ä¼ å»ºè®®:")
            print(f"   python resume_batch_processing.py {args.csv_file} --missing-rows-file your_missing_file.txt")
        
        print("="*80)
    else:
        # å¯åŠ¨ç»­ä¼ å¤„ç†
        output_dir = resumer.start_resume_processing(
            model=args.model,
            batch_size=args.batch_size,
            delay_between_batches=args.delay
        )
        
        if output_dir:
            print("\n" + "="*80)
            print("ğŸš€ ç»­ä¼ æ‰¹å¤„ç†å·²å¯åŠ¨")
            print("="*80)
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
            print(f"ğŸ” ç›‘æ§è¿›åº¦: tail -f {output_dir}/logs/*.log")
            print(f"ğŸ“Š æŸ¥çœ‹çŠ¶æ€: cat {output_dir}/batch_status.json")
            print("="*80)
        else:
            logger.error("âŒ ç»­ä¼ å¤„ç†å¯åŠ¨å¤±è´¥")
            return 1
    
    return 0

if __name__ == "__main__":
    import sys
    sys.exit(main())
