#!/usr/bin/env python3
"""
é‡è¯•å¤±è´¥æ‰¹æ¬¡çš„ä¸“ç”¨è„šæœ¬
"""

import os
import json
import logging
from robust_batch_processor import RobustBatchProcessor

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def show_failed_batches(processor: RobustBatchProcessor):
    """æ˜¾ç¤ºå¤±è´¥çš„æ‰¹æ¬¡"""
    failed_jobs = [job for job in processor.jobs if job.status in ["failed", "timeout"]]
    
    if not failed_jobs:
        logger.info("âœ… æ²¡æœ‰å¤±è´¥çš„æ‰¹æ¬¡ï¼")
        return []
    
    logger.info("=" * 80)
    logger.info("å¤±è´¥çš„æ‰¹æ¬¡è¯¦æƒ…")
    logger.info("=" * 80)
    
    for i, job in enumerate(failed_jobs, 1):
        logger.info(f"{i}. ä»»åŠ¡: {job.name}")
        logger.info(f"   è¡ŒèŒƒå›´: {job.start_row+1}-{job.end_row}")
        logger.info(f"   çŠ¶æ€: {job.status}")
        logger.info(f"   å°è¯•æ¬¡æ•°: {job.attempts}/{job.max_attempts}")
        logger.info(f"   é”™è¯¯ä¿¡æ¯: {job.error_message}")
        logger.info(f"   åˆ›å»ºæ—¶é—´: {job.created_at}")
        logger.info("-" * 40)
    
    return failed_jobs

def retry_specific_batches(processor: RobustBatchProcessor, batch_names: list):
    """é‡è¯•æŒ‡å®šçš„æ‰¹æ¬¡"""
    for batch_name in batch_names:
        job = next((job for job in processor.jobs if job.name == batch_name), None)
        if not job:
            logger.error(f"æœªæ‰¾åˆ°æ‰¹æ¬¡: {batch_name}")
            continue
        
        if job.status == "completed":
            logger.info(f"æ‰¹æ¬¡ {batch_name} å·²å®Œæˆï¼Œè·³è¿‡")
            continue
        
        if job.attempts >= job.max_attempts:
            logger.warning(f"æ‰¹æ¬¡ {batch_name} å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
            continue
        
        logger.info(f"ğŸ”„ é‡è¯•æ‰¹æ¬¡: {batch_name}")
        processor.process_single_job(job)

def main():
    import sys
    
    # æ£€æŸ¥çŠ¶æ€æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    output_dir = "batch_results_20250524_224700"
    status_file = os.path.join(output_dir, "batch_status.json")
    
    if not os.path.exists(status_file):
        logger.error(f"çŠ¶æ€æ–‡ä»¶ä¸å­˜åœ¨: {status_file}")
        logger.info("è¯·å…ˆè¿è¡Œ robust_batch_processor.py")
        return
    
    # åˆ›å»ºå¤„ç†å™¨å¹¶åŠ è½½çŠ¶æ€
    processor = RobustBatchProcessor(output_dir)
    
    if len(sys.argv) == 1:
        # æ˜¾ç¤ºå¤±è´¥çš„æ‰¹æ¬¡
        failed_jobs = show_failed_batches(processor)
        
        if not failed_jobs:
            return
        
        print("\né€‰é¡¹:")
        print("1. é‡è¯•æ‰€æœ‰å¤±è´¥çš„æ‰¹æ¬¡")
        print("2. é‡è¯•ç‰¹å®šçš„æ‰¹æ¬¡")
        print("3. æŸ¥çœ‹è¯¦ç»†çŠ¶æ€")
        print("4. é€€å‡º")
        
        choice = input("\nè¯·é€‰æ‹© (1-4): ").strip()
        
        if choice == "1":
            processor.retry_failed_jobs()
        elif choice == "2":
            print("\nå¯é‡è¯•çš„æ‰¹æ¬¡:")
            retryable_jobs = [job for job in failed_jobs if job.attempts < job.max_attempts]
            for i, job in enumerate(retryable_jobs, 1):
                print(f"  {i}. {job.name} (ç¬¬{job.start_row+1}-{job.end_row}è¡Œ)")
            
            if retryable_jobs:
                indices = input("è¯·è¾“å…¥è¦é‡è¯•çš„æ‰¹æ¬¡ç¼–å· (ç”¨é€—å·åˆ†éš”ï¼Œå¦‚: 1,3,5): ").strip()
                try:
                    selected_indices = [int(x.strip()) - 1 for x in indices.split(",")]
                    selected_batches = [retryable_jobs[i].name for i in selected_indices if 0 <= i < len(retryable_jobs)]
                    
                    if selected_batches:
                        retry_specific_batches(processor, selected_batches)
                    else:
                        logger.error("æ— æ•ˆçš„é€‰æ‹©")
                except ValueError:
                    logger.error("è¾“å…¥æ ¼å¼é”™è¯¯")
        elif choice == "3":
            processor.print_summary()
        
    else:
        # å‘½ä»¤è¡ŒæŒ‡å®šæ‰¹æ¬¡åç§°
        batch_names = sys.argv[1:]
        retry_specific_batches(processor, batch_names)

if __name__ == "__main__":
    main()
