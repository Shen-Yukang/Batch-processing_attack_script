#!/usr/bin/env python3
"""
è¯Šæ–­æ‰¹å¤„ç†ç»“æœæ–‡ä»¶ç¼ºå¤±é—®é¢˜
"""

import os
import json
import glob
import logging
from datetime import datetime

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def analyze_batch_directory(batch_dir):
    """åˆ†ææ‰¹å¤„ç†ç›®å½•"""
    logger.info(f"ğŸ” åˆ†ææ‰¹å¤„ç†ç›®å½•: {batch_dir}")
    
    if not os.path.exists(batch_dir):
        logger.error(f"âŒ ç›®å½•ä¸å­˜åœ¨: {batch_dir}")
        return
    
    # è¯»å–çŠ¶æ€æ–‡ä»¶
    status_file = os.path.join(batch_dir, "batch_status.json")
    if not os.path.exists(status_file):
        logger.error(f"âŒ çŠ¶æ€æ–‡ä»¶ä¸å­˜åœ¨: {status_file}")
        return
    
    with open(status_file, 'r') as f:
        status_data = json.load(f)
    
    total_jobs = status_data.get('total_jobs', 0)
    jobs = status_data.get('jobs', [])
    
    logger.info(f"ğŸ“Š æ€»ä»»åŠ¡æ•°: {total_jobs}")
    
    # ç»Ÿè®¡å„çŠ¶æ€çš„ä»»åŠ¡
    status_counts = {}
    completed_jobs = []
    failed_jobs = []
    
    for job in jobs:
        status = job.get('status', 'unknown')
        status_counts[status] = status_counts.get(status, 0) + 1
        
        if status == 'completed':
            completed_jobs.append(job)
        elif status in ['failed', 'timeout']:
            failed_jobs.append(job)
    
    logger.info("ğŸ“ˆ ä»»åŠ¡çŠ¶æ€ç»Ÿè®¡:")
    for status, count in status_counts.items():
        logger.info(f"  {status}: {count}")
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    input_files = glob.glob(os.path.join(batch_dir, "batch_*.jsonl"))
    input_files = [f for f in input_files if not os.path.basename(f).startswith("batch_results_")]
    
    logger.info(f"ğŸ“ è¾“å…¥æ–‡ä»¶æ•°é‡: {len(input_files)}")
    
    # æ£€æŸ¥ç»“æœæ–‡ä»¶
    result_files = glob.glob(os.path.join(batch_dir, "batch_results_*.jsonl"))
    logger.info(f"ğŸ“„ ç»“æœæ–‡ä»¶æ•°é‡: {len(result_files)}")
    
    if result_files:
        logger.info("ğŸ“„ ç°æœ‰ç»“æœæ–‡ä»¶:")
        for result_file in result_files:
            file_size = os.path.getsize(result_file) / 1024  # KB
            mtime = datetime.fromtimestamp(os.path.getmtime(result_file))
            logger.info(f"  - {os.path.basename(result_file)} ({file_size:.1f} KB, {mtime.strftime('%H:%M:%S')})")
            
            # æ£€æŸ¥æ–‡ä»¶å†…å®¹çš„è¡Œå·èŒƒå›´
            try:
                with open(result_file, 'r') as f:
                    first_line = f.readline()
                    if 'custom_id' in first_line and 'row_' in first_line:
                        import re
                        match = re.search(r'row_(\d+)', first_line)
                        if match:
                            start_row = int(match.group(1))
                            
                            # è¯»å–æœ€åå‡ è¡Œæ¥æ‰¾ç»“æŸè¡Œ
                            f.seek(0, 2)  # ç§»åˆ°æ–‡ä»¶æœ«å°¾
                            file_size = f.tell()
                            f.seek(max(0, file_size - 1000))  # è¯»å–æœ€å1000å­—ç¬¦
                            lines = f.readlines()
                            
                            end_row = start_row
                            for line in reversed(lines):
                                if 'row_' in line:
                                    match = re.search(r'row_(\d+)', line)
                                    if match:
                                        end_row = int(match.group(1))
                                        break
                            
                            logger.info(f"    ğŸ“Š åŒ…å«è¡Œå·: {start_row} - {end_row}")
            except Exception as e:
                logger.warning(f"    âš ï¸  æ— æ³•åˆ†ææ–‡ä»¶å†…å®¹: {e}")
    
    # åˆ†æç¼ºå¤±çš„ç»“æœ
    logger.info("\nğŸ” åˆ†æç¼ºå¤±çš„ç»“æœæ–‡ä»¶:")
    
    missing_results = []
    for job in completed_jobs:
        job_name = job['name']
        # æŸ¥æ‰¾å¯¹åº”çš„ç»“æœæ–‡ä»¶
        job_result_files = [f for f in result_files if job_name in os.path.basename(f)]
        
        if not job_result_files:
            missing_results.append(job)
            logger.warning(f"âŒ {job_name} (è¡Œ {job['start_row']}-{job['end_row']}) æ ‡è®°ä¸ºå®Œæˆä½†ç¼ºå°‘ç»“æœæ–‡ä»¶")
    
    if missing_results:
        logger.error(f"ğŸ’¥ å‘ç° {len(missing_results)} ä¸ªç¼ºå¤±ç»“æœçš„ä»»åŠ¡")
        
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ç»“æœéƒ½åœ¨ä¸€ä¸ªæ–‡ä»¶ä¸­
        if len(result_files) == 1:
            logger.info("ğŸ” æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ç»“æœéƒ½åˆå¹¶åœ¨ä¸€ä¸ªæ–‡ä»¶ä¸­...")
            result_file = result_files[0]
            
            try:
                with open(result_file, 'r') as f:
                    content = f.read()
                
                found_jobs = []
                for job in missing_results:
                    expected_rows = [f"row_{i}" for i in range(job['start_row'], min(job['start_row'] + 3, job['end_row']))]
                    if any(row in content for row in expected_rows):
                        found_jobs.append(job['name'])
                
                if found_jobs:
                    logger.info(f"âœ… åœ¨ç»“æœæ–‡ä»¶ä¸­æ‰¾åˆ°äº† {len(found_jobs)} ä¸ªä»»åŠ¡çš„æ•°æ®: {found_jobs}")
                else:
                    logger.warning("âš ï¸  ç»“æœæ–‡ä»¶ä¸­æœªæ‰¾åˆ°ç¼ºå¤±ä»»åŠ¡çš„æ•°æ®")
                    
            except Exception as e:
                logger.error(f"âŒ æ£€æŸ¥ç»“æœæ–‡ä»¶å¤±è´¥: {e}")
    else:
        logger.info("âœ… æ‰€æœ‰å·²å®Œæˆçš„ä»»åŠ¡éƒ½æœ‰å¯¹åº”çš„ç»“æœæ–‡ä»¶")
    
    # å»ºè®®ä¿®å¤æ–¹æ¡ˆ
    logger.info("\nğŸ’¡ å»ºè®®ä¿®å¤æ–¹æ¡ˆ:")
    if missing_results:
        logger.info("1. é‡æ–°è¿è¡Œç¼ºå¤±ç»“æœçš„æ‰¹æ¬¡")
        logger.info("2. æ£€æŸ¥OpenAI APIçŠ¶æ€å’Œé…é¢")
        logger.info("3. éªŒè¯batch_processor.pyçš„è¾“å‡ºç›®å½•å‚æ•°")
        logger.info("4. æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œæ–‡ä»¶æƒé™")
    else:
        logger.info("1. å½“å‰çŠ¶æ€æ­£å¸¸ï¼Œç»§ç»­ç­‰å¾…å‰©ä½™ä»»åŠ¡å®Œæˆ")
        logger.info("2. å®Œæˆåå¯ä»¥è¿›è¡Œç»“æœåˆå¹¶")

def main():
    logger.info("ğŸ”§ æ‰¹å¤„ç†ç»“æœè¯Šæ–­å·¥å…·")
    logger.info("="*60)
    
    # æŸ¥æ‰¾æœ€æ–°çš„æ‰¹å¤„ç†ç›®å½•
    batch_dirs = glob.glob("output/batch_results_*")
    if not batch_dirs:
        logger.error("âŒ æœªæ‰¾åˆ°ä»»ä½•æ‰¹å¤„ç†ç›®å½•")
        return
    
    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œæœ€æ–°çš„åœ¨å‰
    batch_dirs.sort(key=lambda x: os.path.getmtime(x), reverse=True)
    
    logger.info(f"ğŸ“ æ‰¾åˆ° {len(batch_dirs)} ä¸ªæ‰¹å¤„ç†ç›®å½•")
    
    # åˆ†ææœ€æ–°çš„ç›®å½•
    latest_dir = batch_dirs[0]
    logger.info(f"ğŸ¯ åˆ†ææœ€æ–°ç›®å½•: {latest_dir}")
    
    analyze_batch_directory(latest_dir)
    
    # å¦‚æœæœ‰å¤šä¸ªç›®å½•ï¼Œç®€å•åˆ—å‡ºå…¶ä»–çš„
    if len(batch_dirs) > 1:
        logger.info(f"\nğŸ“‚ å…¶ä»–æ‰¹å¤„ç†ç›®å½•:")
        for i, batch_dir in enumerate(batch_dirs[1:], 2):
            mtime = datetime.fromtimestamp(os.path.getmtime(batch_dir))
            logger.info(f"  {i}. {batch_dir} ({mtime.strftime('%Y-%m-%d %H:%M:%S')})")

if __name__ == "__main__":
    main()
