#!/usr/bin/env python3
"""
ä¿®å¤å¤±è´¥çš„æ‰¹å¤„ç†ä»»åŠ¡ - ä½¿ç”¨æ­£ç¡®çš„æ¨¡å‹é‡æ–°å¤„ç†
"""

import os
import json
import logging
import subprocess
from datetime import datetime

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def fix_batch_status(batch_dir):
    """ä¿®å¤æ‰¹å¤„ç†çŠ¶æ€ï¼Œå°†é”™è¯¯æ ‡è®°ä¸ºå®Œæˆçš„ä»»åŠ¡æ”¹ä¸ºå¤±è´¥"""
    status_file = os.path.join(batch_dir, "batch_status.json")
    
    if not os.path.exists(status_file):
        logger.error(f"âŒ çŠ¶æ€æ–‡ä»¶ä¸å­˜åœ¨: {status_file}")
        return False
    
    try:
        with open(status_file, 'r') as f:
            status_data = json.load(f)
        
        fixed_count = 0
        for job in status_data.get('jobs', []):
            if job.get('status') == 'completed':
                # æ£€æŸ¥æ˜¯å¦çœŸçš„æœ‰ç»“æœæ–‡ä»¶
                job_name = job.get('name', '')
                result_files = [f for f in os.listdir(batch_dir) 
                              if f.startswith('batch_results_') and f.endswith('.jsonl')]
                
                # æ£€æŸ¥ç»“æœæ–‡ä»¶ä¸­æ˜¯å¦åŒ…å«æ­¤ä»»åŠ¡çš„æ•°æ®
                has_results = False
                start_row = job.get('start_row', 0)
                end_row = job.get('end_row', 0)
                expected_rows = [f"row_{i}" for i in range(start_row, min(start_row + 3, end_row))]
                
                for result_file in result_files:
                    try:
                        with open(os.path.join(batch_dir, result_file), 'r') as f:
                            content = f.read(1000)  # è¯»å–å‰1000å­—ç¬¦
                            if any(row in content for row in expected_rows):
                                has_results = True
                                break
                    except:
                        continue
                
                if not has_results:
                    # æ²¡æœ‰æ‰¾åˆ°ç»“æœï¼Œæ ‡è®°ä¸ºå¤±è´¥
                    job['status'] = 'failed'
                    job['error_message'] = 'æ¨¡å‹ä¸æ”¯æŒæ‰¹å¤„ç†APIï¼Œéœ€è¦ä½¿ç”¨gpt-4o-minié‡æ–°å¤„ç†'
                    job['completed_at'] = None
                    fixed_count += 1
                    logger.info(f"ğŸ”§ ä¿®å¤ä»»åŠ¡çŠ¶æ€: {job_name} -> failed")
        
        if fixed_count > 0:
            # ä¿å­˜ä¿®å¤åçš„çŠ¶æ€
            with open(status_file, 'w') as f:
                json.dump(status_data, f, indent=2, ensure_ascii=False)
            logger.info(f"âœ… ä¿®å¤äº† {fixed_count} ä¸ªä»»åŠ¡çš„çŠ¶æ€")
        else:
            logger.info("âœ… æ‰€æœ‰ä»»åŠ¡çŠ¶æ€æ­£ç¡®ï¼Œæ— éœ€ä¿®å¤")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ä¿®å¤çŠ¶æ€å¤±è´¥: {e}")
        return False

def recreate_failed_batches(batch_dir, csv_file):
    """é‡æ–°åˆ›å»ºå¤±è´¥çš„æ‰¹æ¬¡ï¼Œä½¿ç”¨æ­£ç¡®çš„æ¨¡å‹"""
    status_file = os.path.join(batch_dir, "batch_status.json")
    
    if not os.path.exists(status_file):
        logger.error(f"âŒ çŠ¶æ€æ–‡ä»¶ä¸å­˜åœ¨: {status_file}")
        return False
    
    try:
        with open(status_file, 'r') as f:
            status_data = json.load(f)
        
        failed_jobs = [job for job in status_data.get('jobs', []) 
                      if job.get('status') in ['failed', 'timeout']]
        
        if not failed_jobs:
            logger.info("âœ… æ²¡æœ‰å¤±è´¥çš„ä»»åŠ¡éœ€è¦é‡æ–°å¤„ç†")
            return True
        
        logger.info(f"ğŸ”„ å‘ç° {len(failed_jobs)} ä¸ªå¤±è´¥çš„ä»»åŠ¡éœ€è¦é‡æ–°å¤„ç†")
        
        success_count = 0
        for i, job in enumerate(failed_jobs, 1):
            job_name = job.get('name', '')
            start_row = job.get('start_row', 0)
            end_row = job.get('end_row', 0)
            
            logger.info(f"ğŸ”„ é‡æ–°å¤„ç†ä»»åŠ¡ {i}/{len(failed_jobs)}: {job_name} (è¡Œ {start_row}-{end_row})")
            
            # é‡æ–°åˆ›å»ºè¾“å…¥æ–‡ä»¶ï¼Œä½¿ç”¨gpt-4o-miniæ¨¡å‹
            jsonl_file = os.path.join(batch_dir, f"{job_name}.jsonl")
            
            create_cmd = [
                "python", "create_safe_batch_input.py",
                csv_file, jsonl_file,
                "--model", "gpt-4o-mini",
                "--start-row", str(start_row),
                "--end-row", str(end_row)
            ]
            
            logger.info(f"ğŸ“ é‡æ–°åˆ›å»ºè¾“å…¥æ–‡ä»¶...")
            try:
                result = subprocess.run(create_cmd, capture_output=True, text=True, timeout=120)
                if result.returncode != 0:
                    logger.error(f"âŒ åˆ›å»ºè¾“å…¥æ–‡ä»¶å¤±è´¥: {result.stderr}")
                    continue
                logger.info(f"âœ… è¾“å…¥æ–‡ä»¶åˆ›å»ºæˆåŠŸ")
            except Exception as e:
                logger.error(f"âŒ åˆ›å»ºè¾“å…¥æ–‡ä»¶å¼‚å¸¸: {e}")
                continue
            
            # æäº¤æ‰¹å¤„ç†
            process_cmd = [
                "python", "batch_processor.py", jsonl_file,
                "--output-dir", batch_dir,
                "--check-interval", "30"
            ]
            
            logger.info(f"ğŸš€ æäº¤æ‰¹å¤„ç†...")
            try:
                result = subprocess.run(process_cmd, capture_output=True, text=True, timeout=600)
                if result.returncode == 0:
                    logger.info(f"âœ… {job_name} é‡æ–°å¤„ç†æˆåŠŸ")
                    
                    # æ›´æ–°çŠ¶æ€
                    job['status'] = 'completed'
                    job['completed_at'] = datetime.now().isoformat()
                    job['error_message'] = ''
                    job['attempts'] = job.get('attempts', 0) + 1
                    
                    success_count += 1
                else:
                    logger.error(f"âŒ {job_name} é‡æ–°å¤„ç†å¤±è´¥: {result.stderr}")
                    job['error_message'] = f"é‡æ–°å¤„ç†å¤±è´¥: {result.stderr[:200]}"
                    
            except subprocess.TimeoutExpired:
                logger.error(f"â° {job_name} å¤„ç†è¶…æ—¶")
                job['error_message'] = "å¤„ç†è¶…æ—¶"
            except Exception as e:
                logger.error(f"âŒ {job_name} å¤„ç†å¼‚å¸¸: {e}")
                job['error_message'] = f"å¤„ç†å¼‚å¸¸: {str(e)}"
            
            # ä¿å­˜çŠ¶æ€
            with open(status_file, 'w') as f:
                json.dump(status_data, f, indent=2, ensure_ascii=False)
            
            # ä»»åŠ¡é—´å»¶è¿Ÿ
            if i < len(failed_jobs):
                logger.info("â¸ï¸  ç­‰å¾…30ç§’åå¤„ç†ä¸‹ä¸€ä¸ªä»»åŠ¡...")
                import time
                time.sleep(30)
        
        logger.info(f"ğŸ¯ é‡æ–°å¤„ç†å®Œæˆ: æˆåŠŸ {success_count}/{len(failed_jobs)}")
        return success_count > 0
        
    except Exception as e:
        logger.error(f"âŒ é‡æ–°å¤„ç†å¤±è´¥: {e}")
        return False

def main():
    logger.info("ğŸ”§ å¤±è´¥æ‰¹å¤„ç†ä¿®å¤å·¥å…·")
    logger.info("="*60)
    
    # æŸ¥æ‰¾æœ€æ–°çš„æ‰¹å¤„ç†ç›®å½•
    batch_dirs = [d for d in os.listdir('.') if d.startswith('output/batch_results_')]
    if not batch_dirs:
        # æ£€æŸ¥outputç›®å½•
        if os.path.exists('output'):
            batch_dirs = [os.path.join('output', d) for d in os.listdir('output') 
                         if d.startswith('batch_results_')]
    
    if not batch_dirs:
        logger.error("âŒ æœªæ‰¾åˆ°ä»»ä½•æ‰¹å¤„ç†ç›®å½•")
        return 1
    
    # é€‰æ‹©æœ€æ–°çš„ç›®å½•
    latest_dir = max(batch_dirs, key=lambda x: os.path.getmtime(x))
    logger.info(f"ğŸ¯ å¤„ç†ç›®å½•: {latest_dir}")
    
    # ç¡®å®šCSVæ–‡ä»¶
    csv_file = "_csvs/content_Jailbreak28k.csv"  # æ ¹æ®ç›®å½•åæ¨æ–­
    if "FigStep" in latest_dir:
        csv_file = "_csvs/content_FigStep.csv"
    elif "MMSafeBench" in latest_dir:
        csv_file = "_csvs/content_MMSafeBench_cleaned.csv"
    
    if not os.path.exists(csv_file):
        logger.error(f"âŒ CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_file}")
        return 1
    
    logger.info(f"ğŸ“Š ä½¿ç”¨CSVæ–‡ä»¶: {csv_file}")
    
    # æ­¥éª¤1: ä¿®å¤çŠ¶æ€
    logger.info("ğŸ”§ æ­¥éª¤1: ä¿®å¤æ‰¹å¤„ç†çŠ¶æ€...")
    if not fix_batch_status(latest_dir):
        return 1
    
    # æ­¥éª¤2: é‡æ–°å¤„ç†å¤±è´¥çš„æ‰¹æ¬¡
    logger.info("ğŸ”„ æ­¥éª¤2: é‡æ–°å¤„ç†å¤±è´¥çš„æ‰¹æ¬¡...")
    if not recreate_failed_batches(latest_dir, csv_file):
        return 1
    
    logger.info("ğŸ‰ ä¿®å¤å®Œæˆï¼")
    logger.info("ğŸ’¡ å»ºè®®è¿è¡Œè¯Šæ–­è„šæœ¬æ£€æŸ¥ç»“æœ:")
    logger.info("   python diagnose_batch_results.py")
    
    return 0

if __name__ == "__main__":
    import sys
    sys.exit(main())
