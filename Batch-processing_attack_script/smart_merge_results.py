#!/usr/bin/env python3
"""
æ™ºèƒ½åˆå¹¶æ‰¹å¤„ç†ç»“æœè„šæœ¬
è‡ªåŠ¨æ£€æµ‹æ‰€æœ‰ç»“æœæ–‡ä»¶å¹¶åˆå¹¶ä¸ºæœ€ç»ˆè¾“å‡º
"""

import os
import json
import pandas as pd
import logging
from datetime import datetime
import glob
import argparse

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def find_result_files(batch_dir):
    """æŸ¥æ‰¾æ‰€æœ‰æ‰¹å¤„ç†ç»“æœæ–‡ä»¶"""
    logger.info(f"ğŸ” æ‰«ææ‰¹å¤„ç†ç›®å½•: {batch_dir}")
    
    # æŸ¥æ‰¾æ‰€æœ‰ç»“æœæ–‡ä»¶
    result_pattern = os.path.join(batch_dir, "batch_results_*.jsonl")
    result_files = glob.glob(result_pattern)
    
    logger.info(f"ğŸ“„ æ‰¾åˆ° {len(result_files)} ä¸ªç»“æœæ–‡ä»¶")
    
    # æŒ‰æ–‡ä»¶åæ’åº
    result_files.sort()
    
    for i, file in enumerate(result_files, 1):
        file_size = os.path.getsize(file) / 1024  # KB
        logger.debug(f"  {i:2d}. {os.path.basename(file)} ({file_size:.1f} KB)")
    
    return result_files

def parse_result_file(file_path):
    """è§£æå•ä¸ªç»“æœæ–‡ä»¶"""
    results = []
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue
                
                try:
                    data = json.loads(line)
                    
                    # æå–å…³é”®ä¿¡æ¯
                    custom_id = data.get('custom_id', '')
                    response = data.get('response', {})
                    
                    if response and response.get('body'):
                        choices = response['body'].get('choices', [])
                        if choices:
                            content = choices[0].get('message', {}).get('content', '')
                            
                            # æå–è¡Œå·
                            row_num = None
                            if custom_id.startswith('request-'):
                                try:
                                    row_num = int(custom_id.split('-')[1])
                                except:
                                    pass
                            
                            results.append({
                                'row_number': row_num,
                                'custom_id': custom_id,
                                'response_content': content,
                                'file_source': os.path.basename(file_path)
                            })
                
                except json.JSONDecodeError as e:
                    logger.warning(f"âš ï¸  {os.path.basename(file_path)} ç¬¬{line_num}è¡ŒJSONè§£æå¤±è´¥: {e}")
                    continue
    
    except Exception as e:
        logger.error(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        return []
    
    logger.debug(f"ğŸ“Š {os.path.basename(file_path)}: è§£æå‡º {len(results)} æ¡ç»“æœ")
    return results

def merge_all_results(batch_dir, csv_file, output_file):
    """åˆå¹¶æ‰€æœ‰ç»“æœ"""
    logger.info("ğŸš€ å¼€å§‹æ™ºèƒ½åˆå¹¶æ‰¹å¤„ç†ç»“æœ")
    logger.info(f"ğŸ“ æ‰¹å¤„ç†ç›®å½•: {batch_dir}")
    logger.info(f"ğŸ“Š åŸå§‹CSV: {csv_file}")
    logger.info(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶: {output_file}")
    
    # 1. è¯»å–åŸå§‹CSV
    logger.info("ğŸ“– è¯»å–åŸå§‹CSVæ–‡ä»¶...")
    try:
        df_original = pd.read_csv(csv_file)
        total_rows = len(df_original)
        logger.info(f"âœ… åŸå§‹CSVåŒ…å« {total_rows} è¡Œæ•°æ®")
    except Exception as e:
        logger.error(f"âŒ è¯»å–CSVæ–‡ä»¶å¤±è´¥: {e}")
        return False
    
    # 2. æŸ¥æ‰¾æ‰€æœ‰ç»“æœæ–‡ä»¶
    result_files = find_result_files(batch_dir)
    if not result_files:
        logger.error("âŒ æœªæ‰¾åˆ°ä»»ä½•ç»“æœæ–‡ä»¶")
        return False
    
    # 3. è§£ææ‰€æœ‰ç»“æœæ–‡ä»¶
    logger.info("ğŸ”„ è§£ææ‰€æœ‰ç»“æœæ–‡ä»¶...")
    all_results = []
    
    for i, file_path in enumerate(result_files, 1):
        logger.info(f"ğŸ“„ å¤„ç†æ–‡ä»¶ {i}/{len(result_files)}: {os.path.basename(file_path)}")
        file_results = parse_result_file(file_path)
        all_results.extend(file_results)
    
    logger.info(f"ğŸ“Š æ€»å…±è§£æå‡º {len(all_results)} æ¡ç»“æœ")
    
    # 4. æŒ‰è¡Œå·æ’åºå¹¶å»é‡
    logger.info("ğŸ”„ æ•´ç†å’Œå»é‡ç»“æœ...")
    
    # åˆ›å»ºè¡Œå·åˆ°ç»“æœçš„æ˜ å°„
    row_to_result = {}
    duplicate_count = 0
    
    for result in all_results:
        row_num = result['row_number']
        if row_num is not None:
            if row_num in row_to_result:
                duplicate_count += 1
                logger.debug(f"ğŸ”„ å‘ç°é‡å¤è¡Œ {row_num}ï¼Œä¿ç•™æœ€æ–°ç»“æœ")
            row_to_result[row_num] = result
    
    logger.info(f"ğŸ“Š å»é‡åæœ‰æ•ˆç»“æœ: {len(row_to_result)} æ¡")
    if duplicate_count > 0:
        logger.info(f"ğŸ”„ å»é™¤é‡å¤ç»“æœ: {duplicate_count} æ¡")
    
    # 5. åˆ›å»ºæœ€ç»ˆè¾“å‡º
    logger.info("ğŸ”— åˆ›å»ºæœ€ç»ˆè¾“å‡ºæ–‡ä»¶...")
    
    # å¤åˆ¶åŸå§‹æ•°æ®
    df_output = df_original.copy()
    df_output['AI_Response'] = ''
    df_output['Processing_Status'] = 'Missing'
    df_output['Source_File'] = ''
    
    # å¡«å…¥AIå“åº”
    matched_count = 0
    for row_num, result in row_to_result.items():
        # è½¬æ¢ä¸º0-indexed
        idx = row_num - 1
        
        if 0 <= idx < len(df_output):
            df_output.loc[idx, 'AI_Response'] = result['response_content']
            df_output.loc[idx, 'Processing_Status'] = 'Completed'
            df_output.loc[idx, 'Source_File'] = result['file_source']
            matched_count += 1
        else:
            logger.warning(f"âš ï¸  è¡Œå· {row_num} è¶…å‡ºCSVèŒƒå›´")
    
    # 6. ç»Ÿè®¡å’Œä¿å­˜
    logger.info("ğŸ“Š ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯...")
    
    completed_rows = len(df_output[df_output['Processing_Status'] == 'Completed'])
    missing_rows = len(df_output[df_output['Processing_Status'] == 'Missing'])
    completion_rate = (completed_rows / total_rows) * 100
    
    logger.info(f"âœ… æˆåŠŸåŒ¹é…: {matched_count} è¡Œ")
    logger.info(f"ğŸ“ˆ å®Œæˆç»Ÿè®¡:")
    logger.info(f"   - æ€»è¡Œæ•°: {total_rows}")
    logger.info(f"   - å·²å®Œæˆ: {completed_rows}")
    logger.info(f"   - ç¼ºå¤±: {missing_rows}")
    logger.info(f"   - å®Œæˆç‡: {completion_rate:.1f}%")
    
    # ä¿å­˜ç»“æœ
    try:
        df_output.to_csv(output_file, index=False, encoding='utf-8')
        file_size = os.path.getsize(output_file) / 1024 / 1024  # MB
        logger.info(f"ğŸ’¾ ç»“æœå·²ä¿å­˜: {output_file}")
        logger.info(f"ğŸ“„ æ–‡ä»¶å¤§å°: {file_size:.2f} MB")
        
        # ç”Ÿæˆç¼ºå¤±è¡ŒæŠ¥å‘Š
        if missing_rows > 0:
            missing_file = output_file.replace('.csv', '_missing_rows.txt')
            missing_indices = df_output[df_output['Processing_Status'] == 'Missing'].index + 1
            
            with open(missing_file, 'w') as f:
                for idx in missing_indices:
                    f.write(f"{idx}\n")
            
            logger.warning(f"âš ï¸  ç¼ºå¤±è¡Œåˆ—è¡¨å·²ä¿å­˜: {missing_file}")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
        return False

def show_summary(batch_dir, output_file):
    """æ˜¾ç¤ºå¤„ç†æ€»ç»“"""
    logger.info("\n" + "="*80)
    logger.info("ğŸ“Š å¤„ç†æ€»ç»“")
    logger.info("="*80)
    
    # æ£€æŸ¥æˆæœ¬ä¿¡æ¯
    cost_file = os.path.join(batch_dir, "batch_costs.json")
    if os.path.exists(cost_file):
        try:
            with open(cost_file, 'r') as f:
                cost_data = json.load(f)
            
            total_cost = sum(batch.get('batch_cost', 0) for batch in cost_data.get('batches', {}).values())
            logger.info(f"ğŸ’° ä¼°ç®—æ€»æˆæœ¬: ${total_cost:.4f}")
            
        except Exception as e:
            logger.warning(f"âš ï¸  æ— æ³•è¯»å–æˆæœ¬ä¿¡æ¯: {e}")
    
    # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
    if os.path.exists(output_file):
        try:
            df = pd.read_csv(output_file)
            completed = len(df[df['Processing_Status'] == 'Completed'])
            total = len(df)
            
            logger.info(f"ğŸ“„ æœ€ç»ˆè¾“å‡º: {output_file}")
            logger.info(f"ğŸ“Š æ•°æ®ç»Ÿè®¡: {completed}/{total} ({completed/total*100:.1f}%)")
            
        except Exception as e:
            logger.warning(f"âš ï¸  æ— æ³•è¯»å–è¾“å‡ºæ–‡ä»¶ç»Ÿè®¡: {e}")

def main():
    parser = argparse.ArgumentParser(description="æ™ºèƒ½åˆå¹¶æ‰¹å¤„ç†ç»“æœ")
    parser.add_argument("--batch-dir", default="batch_results_20250525_182528", help="æ‰¹å¤„ç†ç›®å½•")
    parser.add_argument("--csv-file", default="_csvs/content_MMSafeBench_cleaned.csv", help="åŸå§‹CSVæ–‡ä»¶")
    parser.add_argument("--output-file", default="final_complete_output.csv", help="è¾“å‡ºæ–‡ä»¶")
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥
    if not os.path.exists(args.batch_dir):
        logger.error(f"âŒ æ‰¹å¤„ç†ç›®å½•ä¸å­˜åœ¨: {args.batch_dir}")
        return 1
    
    if not os.path.exists(args.csv_file):
        logger.error(f"âŒ CSVæ–‡ä»¶ä¸å­˜åœ¨: {args.csv_file}")
        return 1
    
    # æ‰§è¡Œåˆå¹¶
    success = merge_all_results(args.batch_dir, args.csv_file, args.output_file)
    
    if success:
        show_summary(args.batch_dir, args.output_file)
        logger.info("ğŸ‰ åˆå¹¶å®Œæˆï¼")
        return 0
    else:
        logger.error("ğŸ’¥ åˆå¹¶å¤±è´¥")
        return 1

if __name__ == "__main__":
    import sys
    sys.exit(main())
