#!/usr/bin/env python3
"""
ä¸“é—¨å¤„ç†ç¼ºå¤±è¡Œçš„é‡è¯•è„šæœ¬
"""

import os
import sys
import json
import time
import base64
import logging
import argparse
import pandas as pd
from datetime import datetime
from typing import List
from robust_batch_processor import RobustBatchProcessor, BatchJob

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def read_missing_rows(missing_file: str) -> List[int]:
    """è¯»å–ç¼ºå¤±è¡Œæ–‡ä»¶"""
    missing_rows = []
    try:
        with open(missing_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and line.isdigit():
                    # è½¬æ¢ä¸º0-indexed
                    missing_rows.append(int(line) - 1)

        logger.info(f"ä» {missing_file} è¯»å–åˆ° {len(missing_rows)} ä¸ªç¼ºå¤±è¡Œ")
        return sorted(missing_rows)
    except Exception as e:
        logger.error(f"è¯»å–ç¼ºå¤±è¡Œæ–‡ä»¶å¤±è´¥: {e}")
        return []

def find_correct_csv_file():
    """è‡ªåŠ¨æŸ¥æ‰¾æ­£ç¡®çš„CSVæ–‡ä»¶"""
    possible_files = [
        "_csvs/content_MMSafeBench_cleaned.csv",
        "_csvs/content_FigStep.csv",
        "_csvs/content_Jailbreak28k.csv",
        "content_MMSafeBench_cleaned.csv",
        "content_FigStep.csv",
        "content_Jailbreak28k.csv"
    ]

    for file_path in possible_files:
        if os.path.exists(file_path):
            logger.info(f"æ‰¾åˆ°CSVæ–‡ä»¶: {file_path}")
            return file_path

    return None

def detect_csv_from_batch_input(batch_dir):
    """ä»æ‰¹å¤„ç†è¾“å…¥æ–‡ä»¶ä¸­æ£€æµ‹ä½¿ç”¨çš„CSVæ–‡ä»¶"""
    try:
        # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªæ‰¹å¤„ç†è¾“å…¥æ–‡ä»¶
        for file in os.listdir(batch_dir):
            if file.endswith('.jsonl') and file.startswith('batch_'):
                jsonl_path = os.path.join(batch_dir, file)

                # è¯»å–ç¬¬ä¸€è¡Œæ¥åˆ†æ
                with open(jsonl_path, 'r', encoding='utf-8') as f:
                    first_line = f.readline()
                    if first_line:
                        data = json.loads(first_line)

                        # ä»è¯·æ±‚ä¸­æå–ä¿¡æ¯
                        if 'body' in data and 'messages' in data['body']:
                            messages = data['body']['messages']
                            if messages and 'content' in messages[0]:
                                content = messages[0]['content']

                                # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾ç‰‡
                                for item in content:
                                    if item.get('type') == 'image_url':
                                        logger.info("æ£€æµ‹åˆ°è¿™æ˜¯å›¾ç‰‡å¤„ç†ä»»åŠ¡")
                                        return find_correct_csv_file()
                break
    except Exception as e:
        logger.warning(f"æ— æ³•ä»æ‰¹å¤„ç†è¾“å…¥æ£€æµ‹CSVæ–‡ä»¶: {e}")

    return None

def create_missing_rows_batches(missing_rows: List[int], batch_size: int = 20) -> List[List[int]]:
    """å°†ç¼ºå¤±è¡Œåˆ†ç»„ä¸ºè¿ç»­çš„æ‰¹æ¬¡"""
    if not missing_rows:
        return []

    # æŒ‰è¡Œå·æ’åº
    missing_rows.sort()

    batches = []
    current_batch = []

    for row in missing_rows:
        if not current_batch:
            current_batch = [row]
        elif len(current_batch) < batch_size and (row - current_batch[-1]) <= 5:
            # å¦‚æœå½“å‰æ‰¹æ¬¡æœªæ»¡ä¸”è¡Œå·ç›¸è¿‘ï¼ˆå·®è·<=5ï¼‰ï¼ŒåŠ å…¥å½“å‰æ‰¹æ¬¡
            current_batch.append(row)
        else:
            # å¦åˆ™å¼€å§‹æ–°æ‰¹æ¬¡
            batches.append(current_batch)
            current_batch = [row]

    # æ·»åŠ æœ€åä¸€ä¸ªæ‰¹æ¬¡
    if current_batch:
        batches.append(current_batch)

    return batches

def create_missing_rows_input_file(processor: RobustBatchProcessor, missing_rows: List[int], output_file: str) -> bool:
    """ä¸ºç¼ºå¤±è¡Œåˆ›å»ºæ‰¹å¤„ç†è¾“å…¥æ–‡ä»¶"""
    try:
        logger.info(f"ğŸ”„ [æ­¥éª¤1/4] å¼€å§‹åˆ›å»ºæ‰¹å¤„ç†è¾“å…¥æ–‡ä»¶: {output_file}")
        logger.info(f"ğŸ“Š éœ€è¦å¤„ç† {len(missing_rows)} è¡Œæ•°æ®")

        # è¯»å–åŸå§‹CSV
        logger.info(f"ğŸ“– [æ­¥éª¤1.1] è¯»å–CSVæ–‡ä»¶: {processor.input_csv}")
        df = pd.read_csv(processor.input_csv)
        logger.info(f"âœ… CSVæ–‡ä»¶è¯»å–æˆåŠŸï¼Œæ€»è¡Œæ•°: {len(df)}")

        # åˆ›å»ºæ‰¹å¤„ç†è¾“å…¥
        logger.info(f"ğŸ—ï¸  [æ­¥éª¤1.2] å¼€å§‹æ„å»ºæ‰¹å¤„ç†è¯·æ±‚...")
        batch_requests = []
        processed_count = 0
        skipped_count = 0

        for i, row_idx in enumerate(missing_rows, 1):
            if i % 5 == 0 or i == len(missing_rows):
                logger.info(f"ğŸ“ˆ è¿›åº¦: {i}/{len(missing_rows)} ({i/len(missing_rows)*100:.1f}%)")

            if row_idx >= len(df):
                logger.warning(f"âš ï¸  è·³è¿‡è¶…å‡ºèŒƒå›´çš„è¡Œ: {row_idx+1}")
                skipped_count += 1
                continue

            row = df.iloc[row_idx]
            image_path = row['Image Path']
            prompt = row['Content of P*']

            # æ£€æŸ¥å›¾ç‰‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(image_path):
                logger.warning(f"âš ï¸  å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡è¡Œ {row_idx+1}: {image_path}")
                skipped_count += 1
                continue

            # ç¼–ç å›¾ç‰‡ä¸ºbase64
            try:
                logger.debug(f"ğŸ–¼ï¸  ç¼–ç å›¾ç‰‡: {os.path.basename(image_path)} (è¡Œ {row_idx+1})")
                with open(image_path, "rb") as image_file:
                    base64_image = base64.b64encode(image_file.read()).decode('utf-8')
                logger.debug(f"âœ… å›¾ç‰‡ç¼–ç å®Œæˆï¼Œå¤§å°: {len(base64_image)} å­—ç¬¦")
            except Exception as e:
                logger.warning(f"âŒ æ— æ³•ç¼–ç å›¾ç‰‡ï¼Œè·³è¿‡è¡Œ {row_idx+1}: {e}")
                skipped_count += 1
                continue

            # åˆ›å»ºè¯·æ±‚
            request = {
                "custom_id": f"request-{row_idx+1}",
                "method": "POST",
                "url": "/v1/chat/completions",
                "body": {
                    "model": processor.model,
                    "messages": [
                        {
                            "role": "user",
                            "content": [
                                {
                                    "type": "text",
                                    "text": prompt
                                },
                                {
                                    "type": "image_url",
                                    "image_url": {
                                        "url": f"data:image/jpeg;base64,{base64_image}"
                                    }
                                }
                            ]
                        }
                    ],
                    "max_tokens": 1000,
                    "temperature": 0.7
                }
            }

            batch_requests.append(request)
            processed_count += 1

        logger.info(f"ğŸ“Š å¤„ç†ç»Ÿè®¡: æˆåŠŸ {processed_count} ä¸ªï¼Œè·³è¿‡ {skipped_count} ä¸ª")

        # å†™å…¥JSONLæ–‡ä»¶
        logger.info(f"ğŸ’¾ [æ­¥éª¤1.3] å†™å…¥JSONLæ–‡ä»¶...")
        with open(output_file, 'w', encoding='utf-8') as f:
            for i, request in enumerate(batch_requests, 1):
                f.write(json.dumps(request, ensure_ascii=False) + '\n')
                if i % 10 == 0:
                    logger.debug(f"ğŸ“ å·²å†™å…¥ {i}/{len(batch_requests)} ä¸ªè¯·æ±‚")

        file_size = os.path.getsize(output_file) / 1024 / 1024  # MB
        logger.info(f"âœ… æ‰¹å¤„ç†è¾“å…¥æ–‡ä»¶åˆ›å»ºæˆåŠŸ!")
        logger.info(f"ğŸ“„ æ–‡ä»¶: {output_file}")
        logger.info(f"ğŸ“Š è¯·æ±‚æ•°: {len(batch_requests)}")
        logger.info(f"ğŸ’¾ æ–‡ä»¶å¤§å°: {file_size:.2f} MB")

        return len(batch_requests) > 0

    except Exception as e:
        logger.error(f"âŒ åˆ›å»ºæ‰¹å¤„ç†è¾“å…¥æ–‡ä»¶å¤±è´¥: {e}")
        import traceback
        logger.error(f"ğŸ” é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        return False

def process_missing_rows(processor: RobustBatchProcessor, missing_file: str, batch_size: int = 20, delay: int = 60):
    """å¤„ç†ç¼ºå¤±è¡Œ"""
    missing_rows = read_missing_rows(missing_file)
    if not missing_rows:
        logger.error("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ç¼ºå¤±è¡Œ")
        return

    logger.info(f"è¯»å–åˆ° {len(missing_rows)} ä¸ªç¼ºå¤±è¡Œ")

    # ç¡®å®šæ­£ç¡®çš„CSVæ–‡ä»¶
    if not os.path.exists(processor.input_csv):
        logger.warning(f"é…ç½®çš„CSVæ–‡ä»¶ä¸å­˜åœ¨: {processor.input_csv}")

        # å°è¯•è‡ªåŠ¨æ£€æµ‹
        detected_csv = detect_csv_from_batch_input(processor.output_dir)
        if detected_csv:
            processor.input_csv = detected_csv
            logger.info(f"è‡ªåŠ¨æ£€æµ‹åˆ°CSVæ–‡ä»¶: {detected_csv}")
        else:
            logger.error("æ— æ³•ç¡®å®šCSVæ–‡ä»¶")
            return

    # éªŒè¯CSVæ–‡ä»¶
    try:
        df = pd.read_csv(processor.input_csv)
        total_rows = len(df)
        logger.info(f"æˆåŠŸåŠ è½½CSVæ–‡ä»¶ï¼Œå…± {total_rows} è¡Œ")

        # è¿‡æ»¤æ‰è¶…å‡ºCSVè¡Œæ•°èŒƒå›´çš„è¡Œ
        valid_missing_rows = [r for r in missing_rows if r < total_rows]
        if len(valid_missing_rows) < len(missing_rows):
            logger.warning(f"è¿‡æ»¤æ‰ {len(missing_rows) - len(valid_missing_rows)} ä¸ªè¶…å‡ºCSVèŒƒå›´çš„è¡Œ")
            missing_rows = valid_missing_rows
    except Exception as e:
        logger.error(f"è¯»å–CSVæ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return

    if not missing_rows:
        logger.error("æ²¡æœ‰æœ‰æ•ˆçš„ç¼ºå¤±è¡Œéœ€è¦å¤„ç†")
        return

    # åˆ›å»ºæ™ºèƒ½æ‰¹æ¬¡åˆ†ç»„
    batches = create_missing_rows_batches(missing_rows, batch_size)
    logger.info(f"å°†ç¼ºå¤±è¡Œåˆ†ä¸º {len(batches)} ä¸ªæ™ºèƒ½æ‰¹æ¬¡è¿›è¡Œå¤„ç†")

    # æ˜¾ç¤ºæ‰¹æ¬¡ä¿¡æ¯
    for i, batch in enumerate(batches, 1):
        logger.info(f"æ‰¹æ¬¡ {i}: è¡Œå· {batch[0]+1}-{batch[-1]+1} (å…±{len(batch)}è¡Œ)")

    # è¯¢é—®æ˜¯å¦ç»§ç»­
    response = input(f"\nå‡†å¤‡å¤„ç† {len(batches)} ä¸ªæ‰¹æ¬¡ï¼Œå…± {len(missing_rows)} è¡Œã€‚æ˜¯å¦ç»§ç»­ï¼Ÿ(y/N): ")
    if response.lower() != 'y':
        logger.info("ç”¨æˆ·å–æ¶ˆå¤„ç†")
        return

    successful_batches = 0
    failed_batches = 0

    for i, batch in enumerate(batches, 1):
        logger.info(f"\n{'='*80}")
        logger.info(f"ğŸš€ å¼€å§‹å¤„ç†æ‰¹æ¬¡ {i}/{len(batches)}")
        logger.info(f"ğŸ“Š è¡Œå·èŒƒå›´: {batch[0]+1}-{batch[-1]+1} (å…±{len(batch)}è¡Œ)")
        logger.info(f"â° å½“å‰æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"{'='*80}")

        # åˆ›å»ºæ‰¹å¤„ç†ä»»åŠ¡åç§°
        job_name = f"retry_missing_batch_{i:03d}"
        logger.info(f"ğŸ·ï¸  ä»»åŠ¡åç§°: {job_name}")

        # åˆ›å»ºè¾“å…¥æ–‡ä»¶
        jsonl_file = os.path.join(processor.output_dir, f"{job_name}.jsonl")
        logger.info(f"ğŸ“ è¾“å…¥æ–‡ä»¶è·¯å¾„: {jsonl_file}")

        logger.info(f"ğŸ”„ [é˜¶æ®µ1/4] åˆ›å»ºæ‰¹å¤„ç†è¾“å…¥æ–‡ä»¶...")
        if not create_missing_rows_input_file(processor, batch, jsonl_file):
            logger.error(f"âŒ æ‰¹æ¬¡ {i} åˆ›å»ºè¾“å…¥æ–‡ä»¶å¤±è´¥ï¼Œè·³è¿‡")
            failed_batches += 1
            continue

        # åˆ›å»ºæ‰¹å¤„ç†ä»»åŠ¡
        logger.info(f"ğŸ”„ [é˜¶æ®µ2/4] åˆ›å»ºæ‰¹å¤„ç†ä»»åŠ¡å¯¹è±¡...")
        job = BatchJob(job_name, batch[0], batch[-1])

        # æ‰‹åŠ¨å¤„ç†æ‰¹æ¬¡
        job.attempts += 1
        job.status = "running"
        processor.jobs.append(job)
        processor.save_status()
        logger.info(f"âœ… ä»»åŠ¡å¯¹è±¡åˆ›å»ºå®Œæˆï¼ŒçŠ¶æ€å·²ä¿å­˜")

        logger.info(f"ğŸ”„ [é˜¶æ®µ3/4] æäº¤åˆ°OpenAIæ‰¹å¤„ç†API...")
        logger.info(f"ğŸ“¦ å¤„ç†ä»»åŠ¡ {job.name} (ç¬¬{job.attempts}æ¬¡å°è¯•)")

        # æäº¤æ‰¹å¤„ç†
        process_cmd = f"python batch_processor.py {jsonl_file} --output-dir {processor.output_dir} --check-interval 30"
        logger.info(f"ğŸ–¥ï¸  æ‰§è¡Œå‘½ä»¤: {process_cmd}")
        logger.info(f"â±ï¸  è¶…æ—¶è®¾ç½®: 600ç§’ (10åˆ†é’Ÿ)")
        logger.info(f"ğŸ”„ æ­£åœ¨æäº¤æ‰¹å¤„ç†è¯·æ±‚åˆ°OpenAI...")

        start_time = datetime.now()
        success, output = processor.run_command_with_timeout(process_cmd, 600)
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()

        logger.info(f"â±ï¸  æ‰¹å¤„ç†æ‰§è¡Œè€—æ—¶: {duration:.1f}ç§’")

        if success:
            logger.info(f"ğŸ”„ [é˜¶æ®µ4/4] å¤„ç†æˆåŠŸï¼Œæ›´æ–°çŠ¶æ€...")
            job.status = "completed"
            job.completed_at = datetime.now()
            job.error_message = ""
            successful_batches += 1

            # è®°å½•æˆæœ¬ï¼ˆä¼°ç®—ï¼‰
            num_requests = len(batch)
            cost_estimate = processor.cost_tracker.estimate_batch_cost(num_requests)
            cost_estimate["actual_completed"] = num_requests
            processor.cost_tracker.record_batch_cost(job.name, job.batch_id, cost_estimate)

            processor.save_status()
            logger.info(f"âœ… {job.name} å®Œæˆ!")
            logger.info(f"ğŸ’° ä¼°ç®—æˆæœ¬: ${cost_estimate['batch_cost']:.4f}")
            logger.info(f"ğŸ“Š ç´¯è®¡æˆåŠŸæ‰¹æ¬¡: {successful_batches}/{i}")

            # æ£€æŸ¥ç»“æœæ–‡ä»¶
            result_files = [f for f in os.listdir(processor.output_dir) if f.startswith(f"batch_results_") and job_name in f]
            if result_files:
                logger.info(f"ğŸ“„ ç”Ÿæˆç»“æœæ–‡ä»¶: {result_files[0]}")
            else:
                logger.warning(f"âš ï¸  æœªæ‰¾åˆ°ç»“æœæ–‡ä»¶ï¼Œä½†ä»»åŠ¡æ ‡è®°ä¸ºæˆåŠŸ")

        else:
            logger.error(f"âŒ æ‰¹å¤„ç†æ‰§è¡Œå¤±è´¥!")
            logger.error(f"ğŸ” é”™è¯¯è¾“å‡º: {output}")

            if "è¶…æ—¶" in output:
                job.status = "timeout"
                job.error_message = f"å¤„ç†è¶…æ—¶: {output}"
                logger.error(f"â° ä»»åŠ¡è¶…æ—¶ (>600ç§’)")
            else:
                job.status = "failed"
                job.error_message = f"å¤„ç†å¤±è´¥: {output}"
                logger.error(f"ğŸ’¥ ä»»åŠ¡æ‰§è¡Œå¤±è´¥")

            failed_batches += 1
            processor.save_status()
            logger.error(f"ğŸ“Š ç´¯è®¡å¤±è´¥æ‰¹æ¬¡: {failed_batches}/{i}")

            # è¯¢é—®æ˜¯å¦ç»§ç»­
            if i < len(batches):
                continue_prompt = input(f"\nâ“ æ‰¹æ¬¡ {i} å¤„ç†å¤±è´¥ï¼Œæ˜¯å¦ç»§ç»­å¤„ç†ä¸‹ä¸€æ‰¹æ¬¡ï¼Ÿ(y/N): ")
                if continue_prompt.lower() != 'y':
                    logger.info("ğŸ›‘ ç”¨æˆ·å–æ¶ˆåç»­å¤„ç†")
                    break

        # åœ¨æ‰¹æ¬¡ä¹‹é—´æ·»åŠ å»¶è¿Ÿ
        if i < len(batches):
            wait_time = delay if success else 10
            logger.info(f"â¸ï¸  ç­‰å¾… {wait_time} ç§’åå¤„ç†ä¸‹ä¸€æ‰¹æ¬¡...")
            logger.info(f"ğŸ“ˆ æ€»ä½“è¿›åº¦: {i}/{len(batches)} ({i/len(batches)*100:.1f}%)")

            # æ˜¾ç¤ºå€’è®¡æ—¶
            for remaining in range(wait_time, 0, -10):
                if remaining <= 10:
                    logger.info(f"â° è¿˜æœ‰ {remaining} ç§’...")
                    time.sleep(remaining)
                    break
                else:
                    logger.info(f"â° è¿˜æœ‰ {remaining} ç§’...")
                    time.sleep(10)

    # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
    logger.info("\n" + "="*80)
    logger.info("ç¼ºå¤±è¡Œå¤„ç†å®Œæˆæ€»ç»“")
    logger.info("="*80)
    logger.info(f"æˆåŠŸæ‰¹æ¬¡: {successful_batches}")
    logger.info(f"å¤±è´¥æ‰¹æ¬¡: {failed_batches}")
    logger.info(f"æ€»æ‰¹æ¬¡: {len(batches)}")
    logger.info(f"æˆåŠŸç‡: {successful_batches/len(batches)*100:.1f}%")

    if successful_batches > 0:
        logger.info("\nâœ… éƒ¨åˆ†æˆ–å…¨éƒ¨ç¼ºå¤±è¡Œå¤„ç†å®Œæˆï¼")
        logger.info("å»ºè®®è¿è¡Œåˆå¹¶è„šæœ¬æ¥æ›´æ–°æœ€ç»ˆè¾“å‡ºæ–‡ä»¶")

    if failed_batches > 0:
        logger.warning(f"\nâš ï¸  ä»æœ‰ {failed_batches} ä¸ªæ‰¹æ¬¡å¤±è´¥ï¼Œå¯ä»¥ç¨åé‡è¯•")

def main():
    parser = argparse.ArgumentParser(description="å¤„ç†ç¼ºå¤±è¡Œçš„é‡è¯•è„šæœ¬")
    parser.add_argument("--batch-dir", default="batch_results_20250525_182528", help="æ‰¹å¤„ç†ç»“æœç›®å½•")
    parser.add_argument("--missing-file", default="missing_rows.txt", help="ç¼ºå¤±è¡Œæ–‡ä»¶")
    parser.add_argument("--csv-file", help="æŒ‡å®šCSVæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰")
    parser.add_argument("--batch-size", type=int, default=20, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--delay", type=int, default=60, help="æ‰¹æ¬¡é—´å»¶è¿Ÿï¼ˆç§’ï¼‰")
    args = parser.parse_args()

    # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.batch_dir):
        logger.error(f"æ‰¹å¤„ç†ç›®å½•ä¸å­˜åœ¨: {args.batch_dir}")
        return

    # æ£€æŸ¥ç¼ºå¤±è¡Œæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.missing_file):
        logger.error(f"ç¼ºå¤±è¡Œæ–‡ä»¶ä¸å­˜åœ¨: {args.missing_file}")
        return

    # åˆ›å»ºå¤„ç†å™¨
    processor = RobustBatchProcessor(args.batch_dir)

    # å¦‚æœæŒ‡å®šäº†CSVæ–‡ä»¶ï¼Œä½¿ç”¨æŒ‡å®šçš„æ–‡ä»¶
    if args.csv_file and os.path.exists(args.csv_file):
        processor.input_csv = args.csv_file
        logger.info(f"ä½¿ç”¨æŒ‡å®šçš„CSVæ–‡ä»¶: {args.csv_file}")

    # å¤„ç†ç¼ºå¤±è¡Œ
    process_missing_rows(processor, args.missing_file, args.batch_size, args.delay)

if __name__ == "__main__":
    main()
