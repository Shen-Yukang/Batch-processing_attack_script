#!/usr/bin/env python3
"""
é‡è¯•å¤±è´¥æ‰¹æ¬¡å’Œå¤„ç†ç¼ºå¤±è¡Œçš„ä¸“ç”¨è„šæœ¬
"""

import os
import json
import logging
import argparse
import pandas as pd
from robust_batch_processor import RobustBatchProcessor, BatchJob
import time
from typing import List, Optional
import subprocess

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def show_failed_batches(processor: RobustBatchProcessor):
    """æ˜¾ç¤ºå¤±è´¥çš„æ‰¹æ¬¡"""
    failed_jobs = [job for job in processor.jobs if job.status in ["failed", "timeout"]]

    if not failed_jobs:
        logger.info("âœ… æ²¡æœ‰å¤±è´¥çš„æ‰¹æ¬¡ï¼")
        return []

    logger.info("=" * 80)
    logger.info("å¤±è´¥çš„æ‰¹æ¬¡è¯¦æƒ…")
    logger.info("=" * 80)

    for i, job in enumerate(failed_jobs, 1):
        logger.info(f"{i}. ä»»åŠ¡: {job.name}")
        logger.info(f"   è¡ŒèŒƒå›´: {job.start_row+1}-{job.end_row}")
        logger.info(f"   çŠ¶æ€: {job.status}")
        logger.info(f"   å°è¯•æ¬¡æ•°: {job.attempts}/{job.max_attempts}")
        logger.info(f"   é”™è¯¯ä¿¡æ¯: {job.error_message}")
        logger.info(f"   åˆ›å»ºæ—¶é—´: {job.created_at}")
        logger.info("-" * 40)

    return failed_jobs

def retry_specific_batches(processor: RobustBatchProcessor, batch_names: list):
    """é‡è¯•æŒ‡å®šçš„æ‰¹æ¬¡"""
    for batch_name in batch_names:
        job = next((job for job in processor.jobs if job.name == batch_name), None)
        if not job:
            logger.error(f"æœªæ‰¾åˆ°æ‰¹æ¬¡: {batch_name}")
            continue

        if job.status == "completed":
            logger.info(f"æ‰¹æ¬¡ {batch_name} å·²å®Œæˆï¼Œè·³è¿‡")
            continue

        if job.attempts >= job.max_attempts:
            logger.warning(f"æ‰¹æ¬¡ {batch_name} å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
            continue

        logger.info(f"ğŸ”„ é‡è¯•æ‰¹æ¬¡: {batch_name}")
        processor.process_single_job(job)

def read_missing_rows(missing_file: str) -> List[int]:
    """ä»æ–‡ä»¶ä¸­è¯»å–ç¼ºå¤±è¡Œå·åˆ—è¡¨"""
    if not os.path.exists(missing_file):
        logger.error(f"ç¼ºå¤±è¡Œæ–‡ä»¶ä¸å­˜åœ¨: {missing_file}")
        return []

    missing_rows = []
    try:
        with open(missing_file, 'r') as f:
            for line in f:
                line = line.strip()
                if line and line.isdigit():
                    # å°†1-indexedè½¬æ¢ä¸º0-indexed
                    missing_rows.append(int(line) - 1)
    except Exception as e:
        logger.error(f"è¯»å–ç¼ºå¤±è¡Œæ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return []

    return missing_rows

def create_missing_rows_batches(missing_rows: List[int], batch_size: int = 20) -> List[List[int]]:
    """å°†ç¼ºå¤±è¡Œåˆ†ç»„ä¸ºè¿ç»­çš„æ‰¹æ¬¡"""
    if not missing_rows:
        return []

    # æŒ‰è¡Œå·æ’åº
    missing_rows.sort()

    batches = []
    current_batch = []

    for row in missing_rows:
        if not current_batch:
            current_batch = [row]
        elif len(current_batch) < batch_size and (row - current_batch[-1]) <= 5:
            # å¦‚æœå½“å‰æ‰¹æ¬¡æœªæ»¡ä¸”è¡Œå·ç›¸è¿‘ï¼ˆå·®è·<=5ï¼‰ï¼ŒåŠ å…¥å½“å‰æ‰¹æ¬¡
            current_batch.append(row)
        else:
            # å¦åˆ™å¼€å§‹æ–°æ‰¹æ¬¡
            batches.append(current_batch)
            current_batch = [row]

    # æ·»åŠ æœ€åä¸€ä¸ªæ‰¹æ¬¡
    if current_batch:
        batches.append(current_batch)

    return batches

def create_missing_rows_input_file(processor: RobustBatchProcessor, missing_rows: List[int], output_file: str) -> bool:
    """ä¸ºç¼ºå¤±è¡Œåˆ›å»ºæ‰¹å¤„ç†è¾“å…¥æ–‡ä»¶"""
    try:
        import pandas as pd

        # è¯»å–åŸå§‹CSV
        df = pd.read_csv(processor.input_csv)

        # åˆ›å»ºæ‰¹å¤„ç†è¾“å…¥
        batch_requests = []

        for row_idx in missing_rows:
            if row_idx >= len(df):
                logger.warning(f"è·³è¿‡è¶…å‡ºèŒƒå›´çš„è¡Œ: {row_idx+1}")
                continue

            row = df.iloc[row_idx]
            image_path = row['Image Path']
            prompt = row['Content of P*']

            # æ£€æŸ¥å›¾ç‰‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(image_path):
                logger.warning(f"å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡è¡Œ {row_idx+1}: {image_path}")
                continue

            # ç¼–ç å›¾ç‰‡ä¸ºbase64
            import base64
            try:
                with open(image_path, "rb") as image_file:
                    base64_image = base64.b64encode(image_file.read()).decode('utf-8')
            except Exception as e:
                logger.warning(f"æ— æ³•ç¼–ç å›¾ç‰‡ï¼Œè·³è¿‡è¡Œ {row_idx+1}: {e}")
                continue

            # åˆ›å»ºè¯·æ±‚
            request = {
                "custom_id": f"request-{row_idx+1}",
                "method": "POST",
                "url": "/v1/chat/completions",
                "body": {
                    "model": processor.model,
                    "messages": [
                        {
                            "role": "user",
                            "content": [
                                {
                                    "type": "text",
                                    "text": prompt
                                },
                                {
                                    "type": "image_url",
                                    "image_url": {
                                        "url": f"data:image/jpeg;base64,{base64_image}"
                                    }
                                }
                            ]
                        }
                    ],
                    "max_tokens": 1000,
                    "temperature": 0.7
                }
            }

            batch_requests.append(request)

        # å†™å…¥JSONLæ–‡ä»¶
        import json
        with open(output_file, 'w', encoding='utf-8') as f:
            for request in batch_requests:
                f.write(json.dumps(request, ensure_ascii=False) + '\n')

        logger.info(f"æˆåŠŸåˆ›å»ºæ‰¹å¤„ç†è¾“å…¥æ–‡ä»¶: {output_file}ï¼ŒåŒ…å« {len(batch_requests)} ä¸ªè¯·æ±‚")
        return len(batch_requests) > 0

    except Exception as e:
        logger.error(f"åˆ›å»ºæ‰¹å¤„ç†è¾“å…¥æ–‡ä»¶å¤±è´¥: {e}")
        return False

def process_missing_rows(processor: RobustBatchProcessor, missing_file: str, batch_size: int = 20, delay: int = 60):
    """å¤„ç†ç¼ºå¤±è¡Œ"""
    missing_rows = read_missing_rows(missing_file)
    if not missing_rows:
        logger.error("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ç¼ºå¤±è¡Œ")
        return

    logger.info(f"è¯»å–åˆ° {len(missing_rows)} ä¸ªç¼ºå¤±è¡Œ")

    # åŠ è½½åŸå§‹CSVæ•°æ®
    if not os.path.exists(processor.input_csv):
        logger.error(f"è¾“å…¥CSVæ–‡ä»¶ä¸å­˜åœ¨: {processor.input_csv}")
        return

    try:
        df = pd.read_csv(processor.input_csv)
        total_rows = len(df)
        logger.info(f"æˆåŠŸåŠ è½½CSVæ–‡ä»¶ï¼Œå…± {total_rows} è¡Œ")

        # è¿‡æ»¤æ‰è¶…å‡ºCSVè¡Œæ•°èŒƒå›´çš„è¡Œ
        valid_missing_rows = [r for r in missing_rows if r < total_rows]
        if len(valid_missing_rows) < len(missing_rows):
            logger.warning(f"è¿‡æ»¤æ‰ {len(missing_rows) - len(valid_missing_rows)} ä¸ªè¶…å‡ºCSVèŒƒå›´çš„è¡Œ")
            missing_rows = valid_missing_rows
    except Exception as e:
        logger.error(f"è¯»å–CSVæ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return

    if not missing_rows:
        logger.error("æ²¡æœ‰æœ‰æ•ˆçš„ç¼ºå¤±è¡Œéœ€è¦å¤„ç†")
        return

    # æ£€æŸ¥è¾“å…¥CSVæ–‡ä»¶è·¯å¾„
    logger.info(f"å½“å‰ä½¿ç”¨çš„è¾“å…¥CSVæ–‡ä»¶: {processor.input_csv}")

    # ç¡®è®¤ä¸€ä¸‹è¾“å…¥CSVæ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®
    correct_csv = input(f"\nå½“å‰CSVæ–‡ä»¶æ˜¯: {processor.input_csv}\nè¿™æ˜¯æ­£ç¡®çš„æ–‡ä»¶è·¯å¾„å—? (y/N): ")
    if correct_csv.lower() != 'y':
        new_csv = input("è¯·è¾“å…¥æ­£ç¡®çš„CSVæ–‡ä»¶è·¯å¾„: ")
        if os.path.exists(new_csv):
            processor.input_csv = new_csv
            logger.info(f"å·²æ›´æ–°CSVæ–‡ä»¶è·¯å¾„ä¸º: {processor.input_csv}")
        else:
            logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {new_csv}")
            return

    # åˆ›å»ºæ™ºèƒ½æ‰¹æ¬¡åˆ†ç»„
    batches = create_missing_rows_batches(missing_rows, batch_size)
    logger.info(f"å°†ç¼ºå¤±è¡Œåˆ†ä¸º {len(batches)} ä¸ªæ™ºèƒ½æ‰¹æ¬¡è¿›è¡Œå¤„ç†")

    # æ˜¾ç¤ºæ‰¹æ¬¡ä¿¡æ¯
    for i, batch in enumerate(batches, 1):
        logger.info(f"æ‰¹æ¬¡ {i}: è¡Œå· {batch[0]+1}-{batch[-1]+1} (å…±{len(batch)}è¡Œ)")

    # è¯¢é—®æ˜¯å¦ç»§ç»­
    response = input(f"\nå‡†å¤‡å¤„ç† {len(batches)} ä¸ªæ‰¹æ¬¡ï¼Œå…± {len(missing_rows)} è¡Œã€‚æ˜¯å¦ç»§ç»­ï¼Ÿ(y/N): ")
    if response.lower() != 'y':
        logger.info("ç”¨æˆ·å–æ¶ˆå¤„ç†")
        return

    successful_batches = 0
    failed_batches = 0

    for i, batch in enumerate(batches, 1):
        logger.info(f"\n{'='*60}")
        logger.info(f"å¤„ç†æ‰¹æ¬¡ {i}/{len(batches)}")
        logger.info(f"è¡Œå·èŒƒå›´: {batch[0]+1}-{batch[-1]+1} (å…±{len(batch)}è¡Œ)")
        logger.info(f"{'='*60}")

        # åˆ›å»ºæ‰¹å¤„ç†ä»»åŠ¡åç§°
        job_name = f"retry_missing_batch_{i:03d}"

        # åˆ›å»ºè¾“å…¥æ–‡ä»¶
        jsonl_file = os.path.join(processor.output_dir, f"{job_name}.jsonl")

        if not create_missing_rows_input_file(processor, batch, jsonl_file):
            logger.error(f"æ‰¹æ¬¡ {i} åˆ›å»ºè¾“å…¥æ–‡ä»¶å¤±è´¥ï¼Œè·³è¿‡")
            failed_batches += 1
            continue

        # åˆ›å»ºæ‰¹å¤„ç†ä»»åŠ¡
        job = BatchJob(job_name, batch[0], batch[-1])

        # æ‰‹åŠ¨å¤„ç†æ‰¹æ¬¡ï¼ˆä¸ä½¿ç”¨ process_single_jobï¼Œå› ä¸ºå®ƒä¼šé‡æ–°åˆ›å»ºè¾“å…¥æ–‡ä»¶ï¼‰
        job.attempts += 1
        job.status = "running"
        processor.jobs.append(job)
        processor.save_status()

        logger.info(f"ğŸ“¦ å¤„ç†ä»»åŠ¡ {job.name} (ç¬¬{job.attempts}æ¬¡å°è¯•)")

        # æäº¤æ‰¹å¤„ç†
        process_cmd = f"python batch_processor.py {jsonl_file} --output-dir {processor.output_dir} --check-interval 30"

        success, output = processor.run_command_with_timeout(process_cmd, 600)
        if success:
            job.status = "completed"
            job.completed_at = datetime.now()
            job.error_message = ""
            successful_batches += 1

            # è®°å½•æˆæœ¬ï¼ˆä¼°ç®—ï¼‰
            num_requests = len(batch)
            cost_estimate = processor.cost_tracker.estimate_batch_cost(num_requests)
            cost_estimate["actual_completed"] = num_requests
            processor.cost_tracker.record_batch_cost(job.name, job.batch_id, cost_estimate)

            processor.save_status()
            logger.info(f"âœ… {job.name} å®Œæˆ (ä¼°ç®—æˆæœ¬: ${cost_estimate['batch_cost']:.4f})")
        else:
            if "è¶…æ—¶" in output:
                job.status = "timeout"
                job.error_message = f"å¤„ç†è¶…æ—¶: {output}"
            else:
                job.status = "failed"
                job.error_message = f"å¤„ç†å¤±è´¥: {output}"

            failed_batches += 1
            processor.save_status()
            logger.error(f"âŒ {job.name} å¤±è´¥: {output}")

            # è¯¢é—®æ˜¯å¦ç»§ç»­
            if i < len(batches):
                continue_prompt = input(f"\næ‰¹æ¬¡ {i} å¤„ç†å¤±è´¥ï¼Œæ˜¯å¦ç»§ç»­å¤„ç†ä¸‹ä¸€æ‰¹æ¬¡ï¼Ÿ(y/N): ")
                if continue_prompt.lower() != 'y':
                    logger.info("ç”¨æˆ·å–æ¶ˆåç»­å¤„ç†")
                    break

        # åœ¨æ‰¹æ¬¡ä¹‹é—´æ·»åŠ å»¶è¿Ÿ
        if i < len(batches):
            wait_time = delay if success else 10
            logger.info(f"ç­‰å¾… {wait_time} ç§’åå¤„ç†ä¸‹ä¸€æ‰¹æ¬¡...")
            time.sleep(wait_time)

    # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
    logger.info("\n" + "="*80)
    logger.info("ç¼ºå¤±è¡Œå¤„ç†å®Œæˆæ€»ç»“")
    logger.info("="*80)
    logger.info(f"æˆåŠŸæ‰¹æ¬¡: {successful_batches}")
    logger.info(f"å¤±è´¥æ‰¹æ¬¡: {failed_batches}")
    logger.info(f"æ€»æ‰¹æ¬¡: {len(batches)}")
    logger.info(f"æˆåŠŸç‡: {successful_batches/len(batches)*100:.1f}%")

    if successful_batches > 0:
        logger.info("\nâœ… éƒ¨åˆ†æˆ–å…¨éƒ¨ç¼ºå¤±è¡Œå¤„ç†å®Œæˆï¼")
        logger.info("å»ºè®®è¿è¡Œåˆå¹¶è„šæœ¬æ¥æ›´æ–°æœ€ç»ˆè¾“å‡ºæ–‡ä»¶")

    if failed_batches > 0:
        logger.warning(f"\nâš ï¸  ä»æœ‰ {failed_batches} ä¸ªæ‰¹æ¬¡å¤±è´¥ï¼Œå¯ä»¥ç¨åé‡è¯•")

def main():
    # åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨
    parser = argparse.ArgumentParser(description="é‡è¯•å¤±è´¥æ‰¹æ¬¡å’Œå¤„ç†ç¼ºå¤±è¡Œçš„ä¸“ç”¨è„šæœ¬")
    parser.add_argument("--batch-dir", required=False, help="æ‰¹å¤„ç†ç»“æœç›®å½•")
    parser.add_argument("--input-csv", required=False, help="è¾“å…¥CSVæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--model", default="gpt-4o", help="ä½¿ç”¨çš„æ¨¡å‹ï¼Œé»˜è®¤gpt-4o")
    parser.add_argument("--missing-file", default="missing_rows.txt", help="ç¼ºå¤±è¡Œæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--process-missing", action="store_true", help="å¤„ç†ç¼ºå¤±è¡Œ")
    parser.add_argument("--batch-size", type=int, default=20, help="å¤„ç†ç¼ºå¤±è¡Œçš„æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--delay", type=int, default=60, help="æ‰¹æ¬¡é—´çš„å»¶è¿Ÿç§’æ•°")
    parser.add_argument("batches", nargs="*", help="è¦é‡è¯•çš„ç‰¹å®šæ‰¹æ¬¡åç§°")
    args = parser.parse_args()

    # è‡ªåŠ¨æ£€æµ‹æœ€æ–°çš„æ‰¹å¤„ç†ç›®å½•
    if not args.batch_dir:
        import glob
        batch_dirs = glob.glob("batch_results_*")
        if batch_dirs:
            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œè·å–æœ€æ–°çš„
            batch_dirs.sort(key=os.path.getmtime, reverse=True)
            args.batch_dir = batch_dirs[0]
            logger.info(f"è‡ªåŠ¨æ£€æµ‹åˆ°æœ€æ–°æ‰¹å¤„ç†ç›®å½•: {args.batch_dir}")
        else:
            logger.error("æœªæ‰¾åˆ°æ‰¹å¤„ç†ç›®å½•ï¼Œè¯·ä½¿ç”¨--batch-diræŒ‡å®š")
            return

    # æ£€æŸ¥çŠ¶æ€æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    status_file = os.path.join(args.batch_dir, "batch_status.json")
    if not os.path.exists(status_file):
        logger.error(f"çŠ¶æ€æ–‡ä»¶ä¸å­˜åœ¨: {status_file}")
        logger.info(f"è¯·å…ˆè¿è¡Œ robust_batch_processor.py --output-dir {args.batch_dir}")
        return

    # åˆ›å»ºå¤„ç†å™¨å¹¶åŠ è½½çŠ¶æ€
    processor = RobustBatchProcessor(args.batch_dir)

    # å¦‚æœæŒ‡å®šäº†è¾“å…¥CSVå’Œæ¨¡å‹ï¼Œæ›´æ–°å¤„ç†å™¨
    if args.input_csv:
        processor.input_csv = args.input_csv
    if args.model:
        processor.model = args.model

    # å¤„ç†ç¼ºå¤±è¡Œ
    if args.process_missing:
        process_missing_rows(processor, args.missing_file, args.batch_size, args.delay)
        return

    # å¤„ç†å‘½ä»¤è¡Œå‚æ•°
    if args.batches:
        # ç›´æ¥é‡è¯•æŒ‡å®šçš„æ‰¹æ¬¡
        retry_specific_batches(processor, args.batches)
    else:
        # äº¤äº’æ¨¡å¼
        failed_jobs = show_failed_batches(processor)

        if not failed_jobs:
            # æç¤ºæ˜¯å¦å¤„ç†ç¼ºå¤±è¡Œ
            if os.path.exists(args.missing_file):
                print("\né€‰é¡¹:")
                print("1. å¤„ç†ç¼ºå¤±è¡Œ")
                print("2. æŸ¥çœ‹è¯¦ç»†çŠ¶æ€")
                print("3. é€€å‡º")

                choice = input("\nè¯·é€‰æ‹© (1-3): ").strip()

                if choice == "1":
                    process_missing_rows(processor, args.missing_file, args.batch_size, args.delay)
                elif choice == "2":
                    processor.print_summary()
            return

        print("\né€‰é¡¹:")
        print("1. é‡è¯•æ‰€æœ‰å¤±è´¥çš„æ‰¹æ¬¡")
        print("2. é‡è¯•ç‰¹å®šçš„æ‰¹æ¬¡")
        print("3. å¤„ç†ç¼ºå¤±è¡Œ")
        print("4. æŸ¥çœ‹è¯¦ç»†çŠ¶æ€")
        print("5. é€€å‡º")

        choice = input("\nè¯·é€‰æ‹© (1-5): ").strip()

        if choice == "1":
            processor.retry_failed_jobs()
        elif choice == "2":
            print("\nå¯é‡è¯•çš„æ‰¹æ¬¡:")
            retryable_jobs = [job for job in failed_jobs if job.attempts < job.max_attempts]
            for i, job in enumerate(retryable_jobs, 1):
                print(f"  {i}. {job.name} (ç¬¬{job.start_row+1}-{job.end_row}è¡Œ)")

            if retryable_jobs:
                indices = input("è¯·è¾“å…¥è¦é‡è¯•çš„æ‰¹æ¬¡ç¼–å· (ç”¨é€—å·åˆ†éš”ï¼Œå¦‚: 1,3,5): ").strip()
                try:
                    selected_indices = [int(x.strip()) - 1 for x in indices.split(",")]
                    selected_batches = [retryable_jobs[i].name for i in selected_indices if 0 <= i < len(retryable_jobs)]

                    if selected_batches:
                        retry_specific_batches(processor, selected_batches)
                    else:
                        logger.error("æ— æ•ˆçš„é€‰æ‹©")
                except ValueError:
                    logger.error("è¾“å…¥æ ¼å¼é”™è¯¯")
        elif choice == "3":
            process_missing_rows(processor, args.missing_file, args.batch_size, args.delay)
        elif choice == "4":
            processor.print_summary()

if __name__ == "__main__":
    main()
