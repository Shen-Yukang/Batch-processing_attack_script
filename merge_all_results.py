#!/usr/bin/env python3
"""
åˆå¹¶æ‰€æœ‰æ‰¹å¤„ç†ç»“æœçš„è„šæœ¬
"""

import os
import sys
import glob
import pandas as pd
import json
import logging
from typing import Dict, List, Set
from collections import defaultdict

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def parse_batch_results(results_file: str) -> Dict[str, str]:
    """è§£æå•ä¸ªæ‰¹å¤„ç†ç»“æœæ–‡ä»¶"""
    results = {}
    
    try:
        with open(results_file, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                try:
                    result = json.loads(line.strip())
                    
                    custom_id = result.get('custom_id')
                    if not custom_id:
                        continue
                    
                    # æå–å“åº”å†…å®¹
                    response = result.get('response')
                    if not response:
                        results[custom_id] = "é”™è¯¯ï¼šæ— å“åº”æ•°æ®"
                        continue
                    
                    body = response.get('body')
                    if not body:
                        results[custom_id] = "é”™è¯¯ï¼šæ— å“åº”ä½“"
                        continue
                    
                    choices = body.get('choices')
                    if not choices or len(choices) == 0:
                        results[custom_id] = "é”™è¯¯ï¼šæ— é€‰æ‹©é¡¹"
                        continue
                    
                    message = choices[0].get('message')
                    if not message:
                        results[custom_id] = "é”™è¯¯ï¼šæ— æ¶ˆæ¯å†…å®¹"
                        continue
                    
                    content = message.get('content')
                    refusal = message.get('refusal')

                    if content is None and refusal is not None:
                        # ä½¿ç”¨refusalå­—æ®µä½œä¸ºå“åº”ï¼Œæ·»åŠ æ ‡è®°
                        results[custom_id] = f"refusal: {refusal}"
                    elif content is None:
                        # ä¸¤ä¸ªå­—æ®µéƒ½ä¸ºç©º
                        results[custom_id] = "é”™è¯¯ï¼šæ¶ˆæ¯å†…å®¹ä¸ºç©º"
                    else:
                        # æ­£å¸¸ä½¿ç”¨contentå­—æ®µ
                        results[custom_id] = content
                    
                except json.JSONDecodeError as e:
                    logger.warning(f"æ–‡ä»¶ {results_file} ç¬¬{line_num}è¡ŒJSONè§£æå¤±è´¥: {e}")
                    continue
                except Exception as e:
                    logger.warning(f"æ–‡ä»¶ {results_file} ç¬¬{line_num}è¡Œå¤„ç†å¤±è´¥: {e}")
                    continue
        
        logger.info(f"ä» {os.path.basename(results_file)} è§£æäº† {len(results)} ä¸ªç»“æœ")
        return results
        
    except Exception as e:
        logger.error(f"è¯»å–ç»“æœæ–‡ä»¶å¤±è´¥ {results_file}: {e}")
        return {}

def merge_all_results(results_dir: str, original_csv: str, output_csv: str):
    """åˆå¹¶æ‰€æœ‰æ‰¹å¤„ç†ç»“æœ"""
    
    # è¯»å–åŸå§‹CSVæ–‡ä»¶
    try:
        df = pd.read_csv(original_csv)
        logger.info(f"æˆåŠŸè¯»å–åŸå§‹CSVæ–‡ä»¶ï¼Œå…±{len(df)}è¡Œ")
    except Exception as e:
        logger.error(f"è¯»å–åŸå§‹CSVæ–‡ä»¶å¤±è´¥: {e}")
        return False
    
    # æ·»åŠ ChatGPT Responseåˆ—ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    if 'ChatGPT Response' not in df.columns:
        df['ChatGPT Response'] = ''
    
    # æŸ¥æ‰¾æ‰€æœ‰ç»“æœæ–‡ä»¶
    result_files = glob.glob(os.path.join(results_dir, "batch_results_*.jsonl"))
    
    if not result_files:
        logger.error(f"åœ¨ {results_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°ç»“æœæ–‡ä»¶")
        return False
    
    logger.info(f"æ‰¾åˆ° {len(result_files)} ä¸ªç»“æœæ–‡ä»¶:")
    for file in result_files:
        logger.info(f"  - {os.path.basename(file)}")
    
    # åˆå¹¶æ‰€æœ‰ç»“æœ
    all_results = {}
    total_results = 0
    
    # æ‰€æœ‰æ‰¹å¤„ç†ç»“æœä¸­çš„è¡ŒIDé›†åˆ
    all_custom_ids = set()
    
    for result_file in result_files:
        file_results = parse_batch_results(result_file)
        all_results.update(file_results)
        total_results += len(file_results)
        all_custom_ids.update(file_results.keys())
    
    logger.info(f"æ€»å…±è§£æäº† {total_results} ä¸ªç»“æœ")
    
    if not all_results:
        logger.error("æ²¡æœ‰æœ‰æ•ˆçš„æ‰¹å¤„ç†ç»“æœ")
        return False

    # åˆ†ææ‰€æœ‰custom_idçš„åˆ†å¸ƒ
    row_id_pattern = "row_"
    min_row_id = 999999
    max_row_id = -1
    valid_row_ids = []
    
    for custom_id in all_custom_ids:
        if custom_id.startswith(row_id_pattern):
            try:
                row_idx = int(custom_id[len(row_id_pattern):])
                valid_row_ids.append(row_idx)
                min_row_id = min(min_row_id, row_idx)
                max_row_id = max(max_row_id, row_idx)
            except ValueError:
                pass
    
    logger.info(f"ç»“æœä¸­çš„è¡ŒIDèŒƒå›´: {min_row_id} - {max_row_id}")
    logger.info(f"æœ‰æ•ˆçš„è¡ŒIDæ•°é‡: {len(valid_row_ids)}")
    
    # åˆå¹¶ç»“æœåˆ°DataFrame
    merged_count = 0
    missing_count = 0
    missing_rows = []
    
    # æŒ‰ç±»åˆ«ç»Ÿè®¡ç¼ºå¤±æƒ…å†µ
    category_stats = defaultdict(lambda: {"total": 0, "merged": 0, "missing": 0})
    
    for idx in range(len(df)):
        custom_id = f"row_{idx}"
        category = df.at[idx, 'Category'] if 'Category' in df.columns else "æœªåˆ†ç±»"
        
        category_stats[category]["total"] += 1
        
        if custom_id in all_results:
            df.at[idx, 'ChatGPT Response'] = all_results[custom_id]
            merged_count += 1
            category_stats[category]["merged"] += 1
        else:
            missing_count += 1
            missing_rows.append(idx + 1)  # è½¬ä¸º1-indexed
            category_stats[category]["missing"] += 1
    
    # ä¿å­˜ç»“æœ
    try:
        df.to_csv(output_csv, index=False, encoding='utf-8')
        logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {output_csv}")
        
        # ä¿å­˜ç¼ºå¤±è¡Œåˆ—è¡¨åˆ°æ–‡ä»¶
        missing_rows_file = os.path.join(os.path.dirname(output_csv), "missing_rows.txt")
        with open(missing_rows_file, 'w', encoding='utf-8') as f:
            f.write("ç¼ºå¤±è¡Œå·åˆ—è¡¨ (1-indexed):\n")
            for row in missing_rows:
                f.write(f"{row}\n")
        logger.info(f"ç¼ºå¤±è¡Œåˆ—è¡¨å·²ä¿å­˜åˆ°: {missing_rows_file}")
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        logger.info("=" * 60)
        logger.info("åˆå¹¶ç»Ÿè®¡")
        logger.info("=" * 60)
        logger.info(f"æ€»è¡Œæ•°: {len(df)}")
        logger.info(f"æˆåŠŸåˆå¹¶: {merged_count} è¡Œ")
        logger.info(f"ç¼ºå°‘ç»“æœ: {missing_count} è¡Œ")
        logger.info(f"å®Œæˆç‡: {(merged_count/len(df)*100):.1f}%")
        
        # æ˜¾ç¤ºæŒ‰ç±»åˆ«ç»Ÿè®¡
        logger.info("=" * 60)
        logger.info("æŒ‰ç±»åˆ«ç»Ÿè®¡")
        logger.info("=" * 60)
        for category, stats in category_stats.items():
            logger.info(f"ç±»åˆ«: {category}")
            logger.info(f"  æ€»è¡Œæ•°: {stats['total']}")
            logger.info(f"  æˆåŠŸåˆå¹¶: {stats['merged']} ({stats['merged']/stats['total']*100:.1f}%)")
            logger.info(f"  ç¼ºå°‘ç»“æœ: {stats['missing']} ({stats['missing']/stats['total']*100:.1f}%)")
            logger.info("-" * 30)
        
        # æ˜¾ç¤ºç¼ºå°‘ç»“æœçš„è¡Œ
        logger.warning(f"ç¼ºå°‘ç»“æœçš„è¡Œå·: {missing_rows[:10]}")
        if len(missing_rows) > 10:
            logger.warning(f"... è¿˜æœ‰ {len(missing_rows)-10} è¡Œ (è¯¦è§ missing_rows.txt)")
        
        return True
        
    except Exception as e:
        logger.error(f"ä¿å­˜CSVæ–‡ä»¶å¤±è´¥: {e}")
        return False

def analyze_results(output_csv: str):
    """åˆ†æåˆå¹¶åçš„ç»“æœ"""
    try:
        df = pd.read_csv(output_csv)
        
        # ç»Ÿè®¡å“åº”æƒ…å†µ
        total_rows = len(df)
        has_response = df['ChatGPT Response'].notna() & (df['ChatGPT Response'] != '')
        response_count = has_response.sum()
        error_count = df['ChatGPT Response'].str.startswith('é”™è¯¯ï¼š', na=False).sum()
        refusal_count = df['ChatGPT Response'].str.startswith('refusal:', na=False).sum()
        
        logger.info("=" * 60)
        logger.info("ç»“æœåˆ†æ")
        logger.info("=" * 60)
        logger.info(f"æ€»è¡Œæ•°: {total_rows}")
        logger.info(f"æœ‰å“åº”: {response_count}")
        logger.info(f"é”™è¯¯å“åº”: {error_count}")
        logger.info(f"æ‹’ç»å“åº”: {refusal_count}")
        logger.info(f"æ­£å¸¸å†…å®¹å“åº”: {response_count - error_count - refusal_count}")
        logger.info(f"å“åº”ç‡: {(response_count/total_rows*100):.1f}%")
        
        # æŒ‰ç±»åˆ«ç»Ÿè®¡
        if 'Category' in df.columns:
            logger.info("=" * 60)
            logger.info("æŒ‰ç±»åˆ«åˆ†æå“åº”")
            logger.info("=" * 60)
            
            for category in df['Category'].unique():
                category_df = df[df['Category'] == category]
                cat_total = len(category_df)
                cat_has_resp = (category_df['ChatGPT Response'].notna() & (category_df['ChatGPT Response'] != '')).sum()
                cat_error = category_df['ChatGPT Response'].str.startswith('é”™è¯¯ï¼š', na=False).sum()
                cat_refusal = category_df['ChatGPT Response'].str.startswith('refusal:', na=False).sum()
                cat_normal = cat_has_resp - cat_error - cat_refusal
                
                logger.info(f"ç±»åˆ«: {category}")
                logger.info(f"  æ€»è¡Œæ•°: {cat_total}")
                logger.info(f"  æœ‰å“åº”: {cat_has_resp} ({cat_has_resp/cat_total*100:.1f}%)")
                logger.info(f"  é”™è¯¯å“åº”: {cat_error} ({cat_error/cat_total*100:.1f}%)")
                logger.info(f"  æ‹’ç»å“åº”: {cat_refusal} ({cat_refusal/cat_total*100:.1f}%)")
                logger.info(f"  æ­£å¸¸å†…å®¹å“åº”: {cat_normal} ({cat_normal/cat_total*100:.1f}%)")
                logger.info("-" * 30)
        
        # æ˜¾ç¤ºå“åº”é•¿åº¦ç»Ÿè®¡
        valid_responses = df[has_response & ~df['ChatGPT Response'].str.startswith('é”™è¯¯ï¼š', na=False) & ~df['ChatGPT Response'].str.startswith('refusal:', na=False)]
        if len(valid_responses) > 0:
            response_lengths = valid_responses['ChatGPT Response'].str.len()
            logger.info(f"å“åº”é•¿åº¦ç»Ÿè®¡:")
            logger.info(f"  å¹³å‡é•¿åº¦: {response_lengths.mean():.0f} å­—ç¬¦")
            logger.info(f"  æœ€çŸ­å“åº”: {response_lengths.min()} å­—ç¬¦")
            logger.info(f"  æœ€é•¿å“åº”: {response_lengths.max()} å­—ç¬¦")
        
    except Exception as e:
        logger.error(f"åˆ†æç»“æœå¤±è´¥: {e}")

def main():
    if len(sys.argv) != 4:
        print("ç”¨æ³•: python merge_all_results.py RESULTS_DIR ORIGINAL_CSV OUTPUT_CSV")
        print("ä¾‹å¦‚: python merge_all_results.py batch_results_20250524_142000 new_csv/content_CogAgent.csv final_output.csv")
        return
    
    results_dir = sys.argv[1]
    original_csv = sys.argv[2]
    output_csv = sys.argv[3]
    
    # æ£€æŸ¥è¾“å…¥
    if not os.path.exists(results_dir):
        logger.error(f"ç»“æœç›®å½•ä¸å­˜åœ¨: {results_dir}")
        return
    
    if not os.path.exists(original_csv):
        logger.error(f"åŸå§‹CSVæ–‡ä»¶ä¸å­˜åœ¨: {original_csv}")
        return
    
    # åˆå¹¶ç»“æœ
    logger.info("ğŸ”„ å¼€å§‹åˆå¹¶æ‰€æœ‰æ‰¹å¤„ç†ç»“æœ...")
    
    success = merge_all_results(results_dir, original_csv, output_csv)
    
    if success:
        logger.info("âœ… ç»“æœåˆå¹¶æˆåŠŸï¼")
        analyze_results(output_csv)
    else:
        logger.error("âŒ ç»“æœåˆå¹¶å¤±è´¥")

if __name__ == "__main__":
    main()
