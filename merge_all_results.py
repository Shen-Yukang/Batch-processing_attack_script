#!/usr/bin/env python3
"""
åˆå¹¶æ‰€æœ‰æ‰¹å¤„ç†ç»“æœçš„è„šæœ¬
"""

import os
import sys
import glob
import pandas as pd
import json
import logging
from typing import Dict

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def parse_batch_results(results_file: str) -> Dict[str, str]:
    """è§£æå•ä¸ªæ‰¹å¤„ç†ç»“æœæ–‡ä»¶"""
    results = {}
    
    try:
        with open(results_file, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                try:
                    result = json.loads(line.strip())
                    
                    custom_id = result.get('custom_id')
                    if not custom_id:
                        continue
                    
                    # æå–å“åº”å†…å®¹
                    response = result.get('response')
                    if not response:
                        results[custom_id] = "é”™è¯¯ï¼šæ— å“åº”æ•°æ®"
                        continue
                    
                    body = response.get('body')
                    if not body:
                        results[custom_id] = "é”™è¯¯ï¼šæ— å“åº”ä½“"
                        continue
                    
                    choices = body.get('choices')
                    if not choices or len(choices) == 0:
                        results[custom_id] = "é”™è¯¯ï¼šæ— é€‰æ‹©é¡¹"
                        continue
                    
                    message = choices[0].get('message')
                    if not message:
                        results[custom_id] = "é”™è¯¯ï¼šæ— æ¶ˆæ¯å†…å®¹"
                        continue
                    
                    content = message.get('content')
                    if content is None:
                        results[custom_id] = "é”™è¯¯ï¼šæ¶ˆæ¯å†…å®¹ä¸ºç©º"
                        continue
                    
                    results[custom_id] = content
                    
                except json.JSONDecodeError as e:
                    logger.warning(f"æ–‡ä»¶ {results_file} ç¬¬{line_num}è¡ŒJSONè§£æå¤±è´¥: {e}")
                    continue
                except Exception as e:
                    logger.warning(f"æ–‡ä»¶ {results_file} ç¬¬{line_num}è¡Œå¤„ç†å¤±è´¥: {e}")
                    continue
        
        logger.info(f"ä» {os.path.basename(results_file)} è§£æäº† {len(results)} ä¸ªç»“æœ")
        return results
        
    except Exception as e:
        logger.error(f"è¯»å–ç»“æœæ–‡ä»¶å¤±è´¥ {results_file}: {e}")
        return {}

def merge_all_results(results_dir: str, original_csv: str, output_csv: str):
    """åˆå¹¶æ‰€æœ‰æ‰¹å¤„ç†ç»“æœ"""
    
    # è¯»å–åŸå§‹CSVæ–‡ä»¶
    try:
        df = pd.read_csv(original_csv)
        logger.info(f"æˆåŠŸè¯»å–åŸå§‹CSVæ–‡ä»¶ï¼Œå…±{len(df)}è¡Œ")
    except Exception as e:
        logger.error(f"è¯»å–åŸå§‹CSVæ–‡ä»¶å¤±è´¥: {e}")
        return False
    
    # æ·»åŠ ChatGPT Responseåˆ—ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    if 'ChatGPT Response' not in df.columns:
        df['ChatGPT Response'] = ''
    
    # æŸ¥æ‰¾æ‰€æœ‰ç»“æœæ–‡ä»¶
    result_files = glob.glob(os.path.join(results_dir, "batch_results_*.jsonl"))
    
    if not result_files:
        logger.error(f"åœ¨ {results_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°ç»“æœæ–‡ä»¶")
        return False
    
    logger.info(f"æ‰¾åˆ° {len(result_files)} ä¸ªç»“æœæ–‡ä»¶:")
    for file in result_files:
        logger.info(f"  - {os.path.basename(file)}")
    
    # åˆå¹¶æ‰€æœ‰ç»“æœ
    all_results = {}
    total_results = 0
    
    for result_file in result_files:
        file_results = parse_batch_results(result_file)
        all_results.update(file_results)
        total_results += len(file_results)
    
    logger.info(f"æ€»å…±è§£æäº† {total_results} ä¸ªç»“æœ")
    
    if not all_results:
        logger.error("æ²¡æœ‰æœ‰æ•ˆçš„æ‰¹å¤„ç†ç»“æœ")
        return False
    
    # åˆå¹¶ç»“æœåˆ°DataFrame
    merged_count = 0
    missing_count = 0
    
    for idx in range(len(df)):
        custom_id = f"row_{idx}"
        
        if custom_id in all_results:
            df.at[idx, 'ChatGPT Response'] = all_results[custom_id]
            merged_count += 1
        else:
            missing_count += 1
    
    # ä¿å­˜ç»“æœ
    try:
        df.to_csv(output_csv, index=False, encoding='utf-8')
        logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {output_csv}")
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        logger.info("=" * 60)
        logger.info("åˆå¹¶ç»Ÿè®¡")
        logger.info("=" * 60)
        logger.info(f"æ€»è¡Œæ•°: {len(df)}")
        logger.info(f"æˆåŠŸåˆå¹¶: {merged_count} è¡Œ")
        logger.info(f"ç¼ºå°‘ç»“æœ: {missing_count} è¡Œ")
        logger.info(f"å®Œæˆç‡: {(merged_count/len(df)*100):.1f}%")
        
        # æ˜¾ç¤ºç¼ºå°‘ç»“æœçš„è¡Œ
        if missing_count > 0:
            missing_rows = []
            for idx in range(len(df)):
                custom_id = f"row_{idx}"
                if custom_id not in all_results:
                    missing_rows.append(idx + 1)
            
            logger.warning(f"ç¼ºå°‘ç»“æœçš„è¡Œå·: {missing_rows[:10]}")
            if len(missing_rows) > 10:
                logger.warning(f"... è¿˜æœ‰ {len(missing_rows)-10} è¡Œ")
        
        return True
        
    except Exception as e:
        logger.error(f"ä¿å­˜CSVæ–‡ä»¶å¤±è´¥: {e}")
        return False

def analyze_results(output_csv: str):
    """åˆ†æåˆå¹¶åçš„ç»“æœ"""
    try:
        df = pd.read_csv(output_csv)
        
        # ç»Ÿè®¡å“åº”æƒ…å†µ
        total_rows = len(df)
        has_response = df['ChatGPT Response'].notna() & (df['ChatGPT Response'] != '')
        response_count = has_response.sum()
        error_count = df['ChatGPT Response'].str.startswith('é”™è¯¯ï¼š', na=False).sum()
        
        logger.info("=" * 60)
        logger.info("ç»“æœåˆ†æ")
        logger.info("=" * 60)
        logger.info(f"æ€»è¡Œæ•°: {total_rows}")
        logger.info(f"æœ‰å“åº”: {response_count}")
        logger.info(f"é”™è¯¯å“åº”: {error_count}")
        logger.info(f"æˆåŠŸå“åº”: {response_count - error_count}")
        logger.info(f"æˆåŠŸç‡: {((response_count - error_count)/total_rows*100):.1f}%")
        
        # æ˜¾ç¤ºå“åº”é•¿åº¦ç»Ÿè®¡
        valid_responses = df[has_response & ~df['ChatGPT Response'].str.startswith('é”™è¯¯ï¼š', na=False)]
        if len(valid_responses) > 0:
            response_lengths = valid_responses['ChatGPT Response'].str.len()
            logger.info(f"å“åº”é•¿åº¦ç»Ÿè®¡:")
            logger.info(f"  å¹³å‡é•¿åº¦: {response_lengths.mean():.0f} å­—ç¬¦")
            logger.info(f"  æœ€çŸ­å“åº”: {response_lengths.min()} å­—ç¬¦")
            logger.info(f"  æœ€é•¿å“åº”: {response_lengths.max()} å­—ç¬¦")
        
    except Exception as e:
        logger.error(f"åˆ†æç»“æœå¤±è´¥: {e}")

def main():
    if len(sys.argv) != 4:
        print("ç”¨æ³•: python merge_all_results.py RESULTS_DIR ORIGINAL_CSV OUTPUT_CSV")
        print("ä¾‹å¦‚: python merge_all_results.py batch_results_20250524_142000 new_csv/content_CogAgent.csv final_output.csv")
        return
    
    results_dir = sys.argv[1]
    original_csv = sys.argv[2]
    output_csv = sys.argv[3]
    
    # æ£€æŸ¥è¾“å…¥
    if not os.path.exists(results_dir):
        logger.error(f"ç»“æœç›®å½•ä¸å­˜åœ¨: {results_dir}")
        return
    
    if not os.path.exists(original_csv):
        logger.error(f"åŸå§‹CSVæ–‡ä»¶ä¸å­˜åœ¨: {original_csv}")
        return
    
    # åˆå¹¶ç»“æœ
    logger.info("ğŸ”„ å¼€å§‹åˆå¹¶æ‰€æœ‰æ‰¹å¤„ç†ç»“æœ...")
    
    success = merge_all_results(results_dir, original_csv, output_csv)
    
    if success:
        logger.info("âœ… ç»“æœåˆå¹¶æˆåŠŸï¼")
        analyze_results(output_csv)
    else:
        logger.error("âŒ ç»“æœåˆå¹¶å¤±è´¥")

if __name__ == "__main__":
    main()
